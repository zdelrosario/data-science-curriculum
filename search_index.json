[["index.html", "Data Science: Exercise Solutions 1 Data Science: Exercise Solutions 1.1 How to Set Up These Materials 1.2 How to Use This Guide", " Data Science: Exercise Solutions Zachary del Rosario 1 Data Science: Exercise Solutions These are the exercise solutions for this data science curriculum. Note that you should attempt to do each exercise before consulting the solutions! However, if you get stuck, you should make sure to consult these materials. 1.1 How to Set Up These Materials (Setup) Complete this exercise to install RStudio. (Setup) Download and unzip this archive to obtain the curriculum materials. (Setup) Open the folder you unzipped as a project in RStudio (File &gt; Open Project...). (Learn) Work through the exercise files in the exercises_sequenced/ folder at your own pace to learn data science skills. (Learn) Use the challenges in the challenges/ folder to put your new skills to use. 1.2 How to Use This Guide Download and install the materials, as described in the steps above. Work through the exercises; if you get stuck, take a look at this solution guide to get unstuck. "],["reproducibility-setup.html", "2 Reproducibility: Setup 2.1 Setup 2.2 Git 2.3 GitHub 2.4 Discord", " 2 Reproducibility: Setup Purpose: We’re going to need a lot of tools to make it through this class, you and I. This pre-class exercise will get you signed up for all the (free) accounts you’ll need for the class. Reading: (None) Topics: GitHub, Discord Note: If you’re reading this file in RStudio, you can Shift + Click (CMD + Click) to follow a link. 2.1 Setup 2.2 Git Git is free and open-source version control software. One of our desired outcomes from this course is to learn how to use git to rigorously track our data science work. Install Git, following the instructions relevant to your operating system: (Mac OS) If you attempt to run git from the Terminal and don’t have it installed, Mac OS should automatically start the installation. See here for more info. (Windows) Install Git for Windows (Linux) Use your favorite package manager and install git. See here for more info. 2.3 GitHub GitHub is a free service that helps you track and share code, interfacing with Git. We will use this to distribute and submit assignments. You will need an account on GitHub. 2.4 Discord Discord is a free service used for communication. It’s like a combination of Slack (persistent text channels) and Zoom (audio and video chat). We will use this for almost all communication about the class, including technical questions, office hours, and (online) class sessions. You will need an account on Discord. "],["reproducibility-git-and-our-curriculum.html", "3 Reproducibility: Git and Our Curriculum 3.1 Windows-specific Instructions 3.2 Set Up SSH Key 3.3 Downloading Our Exercises", " 3 Reproducibility: Git and Our Curriculum Purpose: Git is a powerful tool to manage our work, but it can be confusing at first. Here we will read some introductory materials about Git, and use the software to download and set up the exercises for the course. Reading: Automated Version Control, complete the steps in Setting Up Git Note: For the steps in the reading, I recommend using the Terminal in RStudio. This should help ensure you have access to Git. Topics: version control, git setup, working with our exercises, ssh keys Note: If you’re reading this file in RStudio, you can Shift + Click (CMD + Click) to follow a link. 3.1 Windows-specific Instructions If you are on a Windows computer, you will need to complete some additional steps. Please follow these instructions before continuing. 3.2 Set Up SSH Key Before you can “clone” (download) the repository of exercises, you’ll need to set up ssh with GitHub. Follow these instructions to add an SSH key to your account. This will allow you to work with GitHub without typing in your password. Note: This can be a bit confusing if you’ve never worked with SSH before; please do ask questions if you are stuck! 3.3 Downloading Our Exercises Open your browser and log into GitHub. Click the New button to create a new repository. Setup Give this a sensible name, like data-science-S2023. Ensure your repository is public, or you will not be able to submit your homework! Setup Once you’ve selected the correct settings, click Create Repository. This creates a repository on GitHub’s servers, but you still need to download it locally. After creating your repository, you’ll see the following link in your browser. Copy the SSH link (note, not the HTTPS link) and open a Terminal. Setup Navigate in your terminal to the location where you’d like to store your work for this class. Type the command git clone and paste the link you copied above. Hit Return to “clone” a local copy of your repository. Git will likely tell you that you cloned an empty repository. That’s OK! We’re going to add the course materials next. Note: If you haven’t used a Terminal before, just open RStudio and find the Terminal tab (towards the bottom). Setup Download the curriculum as a zip file from this link. Unzip this in your blank repository. Unzip the materials and copy them to your (empty) repository. Once you’ve done this, your repository folder should look like this: Setup Once you’ve copied over the materials, run the three following commands from your Terminal while inside your repository: git add * git commit -m \"initial commit\" git push If you do this successfully, you can refresh the repository page in your browser and see the following: Setup Once you’ve gotten to this point, take a moment to look around the repository. The exercises_sequenced folder contains all of the Exercises for the course, while the challenges folder contains all of the challenges you will complete as your homework. You will commit your work to this repository then submit links on Canvas. Aside: Note that Git and GitHub are two different things! Git is a version control tool, while GitHub is a service that uses Git where you can host repositories. For instance, GitLab is another service where you can host Git repositories. "],["setup-rstudio.html", "4 Setup: RStudio", " 4 Setup: RStudio Purpose: We’re going to make extensive use of the R programming language; in particular, the Tidyverse packages. This first exercise will guide you through setting up the necessary software. Reading: (None) Note: If you’re reading this file in RStudio, you can Shift + Click (CMD + Click) to follow a link. 4.0.1 q1 Install Rstudio Download RStudio Desktop and install the R programming language. Both are free! Once installed, you can download the source for this exercise and open open it in RStudio (or you can follow e-rep01-intro-git and clone the repository). This is an R Markdown file, which is a combination of human-readable text and machine-readable code. Think of it as a modern take on a lab notebook. 4.0.2 q2 Install packages Next, run RStudio. When the program opens, you should see a console tab, as in the image below. RStudio console Note that RStudio has multiple tabs, including the Console, Terminal, Jobs, and any files you may have opened. Make sure you are in the Console tab. Type the line install.packages(\"tidyverse\") in your Console and press Enter. This will start the installation of the tidyverse package, which we will use extensively in this class. RStudio package install 4.0.3 q3 Test your install Once your installation has finished, return to your console and use the command library(tidyverse). If your installation is complete, this command should return a list of packages and version numbers, similar to the image below. RStudio package install If you have any issues with installation, please let me know! 4.0.4 q4 Download extras We’ll use a number of extras for this class. To that end, please install the following packages. Note that you can install multiple packages with the syntax install.packages(c(\"curl\", \"mvtnorm\")), extending the arguments inside c() as desired. broom curl fitdistrplus gapminder ggrepel googlesheets4 nycflights13 modelr mvtnorm rsample Rtsne viridis 4.0.5 q5 Download cheatsheets The tidyverse is essentially a language built on top of R. As such, there are a lot of functions to remember. To that end, RStudio has put together a number of cheatsheets to reference when doing data science. Some of the most important ones are: Data visualization Data transformation Data importing Later, we will learn special tools for handling other types of data. The following cheatsheets will be useful for those: String data Dates and times Factors R Markdown "],["reproducibility-tracking-changes-with-git.html", "5 Reproducibility: Tracking Changes with Git 5.1 Configure RStudio to use git 5.2 Set Up Your Repository 5.3 Add a readme 5.4 Commit changes", " 5 Reproducibility: Tracking Changes with Git Purpose: In this class you’ll turn in all your work using GitHub. To do this, you’ll need to be able to track and push changes in your repository. Reading: Follow all the steps in Tracking Changes Topics: Staging, committing, pushing 5.1 Configure RStudio to use git This exercise introduces the graphical interface for git through RStudio. To use this interface, you’ll need to click Tools -&gt; Global Options -&gt; Git/SVN. If Git executable shows (none), click Browse and selec tthe git executable installed on your system. On Mac / Linux you can use which git from your command line On Windows git.exe is probably somewhere in Program Files See these notes for more info. 5.2 Set Up Your Repository 5.3 Add a readme Navigate to your repository (the one you made in e-rep02-create). If you don’t yet have a file called README.md, create it. You can do this through RStudio with File &gt; New File &gt; Markdown File. 5.3.1 q1 Edit your README.md with information about your repository. For instance, you could add your name and a short description. 5.4 Commit changes Once you’ve updated your readme, you can click on the Git tab in RStudio to check changes in your repository. Note: The Git tab will only appear if you’ve opened a project in RStudio. You can do this by clicking the Project button in the top-right of RStudio and opening the data-science-curriculum.Rproj file in your repository. OR you can just use git through whatever interface you like best! New This is an alternative to using Git with the terminal. We’ll work through an entire workflow of commiting and pushing our work to your GitHub repository. 5.4.1 q2 From RStudio, click the Staged button for README.md. Then click Commit to open the commit dialogue; add a commit message then click Commit to finish. New This will open a window confirming your commit; you can safely close this. New Once you’ve finished this, you’ve successfully tracked your changes locally. However, since your repository is linked to GitHub, you can also save your changes by pushing them to the remote server. 5.4.2 q3 Click the push button (green upward arrow, highlighted below) to push your changes to GitHub. Check your repository in GitHub (on the internet); you should be able to see your changes to the readme text. Now you have a backup copy of your work in GitHub, and you can easily share your work by sharing a link to your repository! This is how you will submit Challenges to Canvas: by sharing a link to your work. "],["setup-packages.html", "6 Setup: Packages", " 6 Setup: Packages Purpose: Every time you start an analysis, you will need to load packages. This is a quick warmup to get you in this habit. Reading: (None) Note: If you open this .Rmd file in RStudio you can execute the chunks of code (stuff between triple-ticks) by clicking on the green arrow to the top-right of the chunk, or by placing your cursor within a chunk (between the ticks) and pressing Ctrl + Shift + Enter. See here for a brief introduction to RMarakdown files. Note that in RStudio you can Shift + Click (CMD + Click) to follow a link. 6.0.1 q1 Create a new code chunk and prepare to load the tidyverse. In RStudio use the shortcut Ctrl + Alt + I (Cmd + Option + I on Mac). Type the command library(tidyverse). library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✔ ggplot2 3.4.0 ✔ purrr 1.0.1 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.5.0 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() Make sure to load packages at the beginning of your notebooks! This is a best-practice. 6.0.2 q2 Run the chunk with library(tidyverse). What packages are loaded? In RStudio press Ctrl + Alt + T (Cmd + Option + T on Mac) to run the code chunk at your cursor. ggplot2, tibble, tidyr, readr, purrr, dplyr, stringr, forcats 6.0.3 q3 What are the main differences between install.packages() and library()? How often do you need to call each one? install.packages() downloads and installs packages into R’s library of packages on your computer. After a package is installed, it is available for use, but not loaded to use. library() loads a package from the library for use. The package remains loaded until your session ends, such as when you quit R. You only need to call install.packages() once. You need to call library() for each new session. Packages are frequently updated. You can check for updates and update your packages by using the Update button in the Packages tab of RStudio (lower right pane). "],["setup-function-basics.html", "7 Setup: Function Basics", " 7 Setup: Function Basics Purpose: Functions are our primary tool in carying out data analysis with the tidyverse. It is unreasonable to expect yourself to memorize every function and all its details. To that end, we’ll learn some basic function literacy in R; how to inspect a function, look up its documentation, and find examples on a function’s use. Reading: Programming Basics. Topics: functions, arguments Reading Time: ~ 10 minutes 7.0.1 q1 How do you find help on a function? Get help on the built-in rnorm function. ?rnorm 7.0.2 q2 How do you show the source code for a function? rnorm ## function (n, mean = 0, sd = 1) ## .Call(C_rnorm, n, mean, sd) ## &lt;bytecode: 0x7fa4d942cc40&gt; ## &lt;environment: namespace:stats&gt; 7.0.3 q3 Using either the documentation or the source, determine the arguments for rnorm. The arguments for rnorm are: n: The number of samples to draw mean: The mean of the normal sd: The standard deviation of the normal 7.0.4 q4 Scroll to the bottom of the help for the library() function. How do you list all available packages? Calling library() without arguments lists all the available packages. The examples in the help documentation are often extremely helpful for learning how to use a function (or reminding yourself how its used)! Get used to checking the examples, as they’re a great resource. "],["data-basics.html", "8 Data: Basics 8.1 First Checks 8.2 The Pipe Operator 8.3 Reading Data", " 8 Data: Basics Purpose: When first studying a new dataset, there are very simple checks we should perform first. These are those checks. Additionally, we’ll have our first look at the pipe operator, which will be super useful for writing code that’s readable. Reading: (None) library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✔ ggplot2 3.4.0 ✔ purrr 1.0.1 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.5.0 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() 8.1 First Checks 8.1.1 q0 Run the following chunk: Hint: You can do this either by clicking the green arrow at the top-right of the chunk, or by using the keybaord shortcut Shift + Cmd/Ctrl + Enter. head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa This is a dataset; the fundamental object we’ll study throughout this course. Some nomenclature: The 1, 2, 3, ... on the left enumerate the rows of the dataset The names Sepal.Length, Sepal.Width, ... name the columns of the dataset The column Sepal.Length takes numeric values The column Species takes string values 8.1.2 q1 Load the tidyverse and inspect the diamonds dataset. What do the cut, color, and clarity variables mean? Hint: You can run ?diamonds to get information on a built-in dataset. ?diamonds 8.1.3 q2 Run glimpse(diamonds); what variables does diamonds have? glimpse(diamonds) ## Rows: 53,940 ## Columns: 10 ## $ carat &lt;dbl&gt; 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, 0.23, 0.… ## $ cut &lt;ord&gt; Ideal, Premium, Good, Premium, Good, Very Good, Very Good, Ver… ## $ color &lt;ord&gt; E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J, J, J, I,… ## $ clarity &lt;ord&gt; SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, SI1, VS1, … ## $ depth &lt;dbl&gt; 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, 59.4, 64… ## $ table &lt;dbl&gt; 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54, 62, 58… ## $ price &lt;int&gt; 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339, 340, 34… ## $ x &lt;dbl&gt; 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, 4.00, 4.… ## $ y &lt;dbl&gt; 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, 4.05, 4.… ## $ z &lt;dbl&gt; 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, 2.39, 2.… The diamonds dataset has variables carat, cut, color, clarity, depth, table, price, x, y, z. 8.1.4 q3 Run summary(diamonds); what are the common values for each of the variables? How widely do each of the variables vary? Hint: The Median and Mean are common values, while Min and Max give us a sense of variation. Observations: carat seems to be bounded between 0 and 5 The highest-priced diamond in this set is $18,823! Some of the variables do not have min, max etc. values. These are factors; variables that take one of a finite set of possible values. You should always analyze your dataset in the simplest way possible, build hypotheses, and devise more specific analyses to probe those hypotheses. The glimpse() and summary() functions are two of the simplest tools we have. 8.2 The Pipe Operator Throughout this class we’re going to make heavy use of the pipe operator %&gt;%. This handy little function will help us make our code more readable. Whenever you see %&gt;%, you can translate that into the word “then”. For instance diamonds %&gt;% group_by(cut) %&gt;% summarize(carat_mean = mean(carat)) ## # A tibble: 5 × 2 ## cut carat_mean ## &lt;ord&gt; &lt;dbl&gt; ## 1 Fair 1.05 ## 2 Good 0.849 ## 3 Very Good 0.806 ## 4 Premium 0.892 ## 5 Ideal 0.703 Would translate into the tiny “story” Take the diamonds dataset, then Group it by the variable cut, then summarize it by computing the mean of carat What the pipe actually does. The pipe operator LHS %&gt;% RHS takes its left-hand side (LHS) and inserts it as an the first argument to the function on its right-hand side (RHS). So the pipe will let us take glimpse(diamonds) and turn it into diamonds %&gt;% glimpse(). 8.2.1 q4 Use the pipe operator to re-write summary(diamonds). diamonds %&gt;% summary() ## carat cut color clarity depth ## Min. :0.2000 Fair : 1610 D: 6775 SI1 :13065 Min. :43.00 ## 1st Qu.:0.4000 Good : 4906 E: 9797 VS2 :12258 1st Qu.:61.00 ## Median :0.7000 Very Good:12082 F: 9542 SI2 : 9194 Median :61.80 ## Mean :0.7979 Premium :13791 G:11292 VS1 : 8171 Mean :61.75 ## 3rd Qu.:1.0400 Ideal :21551 H: 8304 VVS2 : 5066 3rd Qu.:62.50 ## Max. :5.0100 I: 5422 VVS1 : 3655 Max. :79.00 ## J: 2808 (Other): 2531 ## table price x y ## Min. :43.00 Min. : 326 Min. : 0.000 Min. : 0.000 ## 1st Qu.:56.00 1st Qu.: 950 1st Qu.: 4.710 1st Qu.: 4.720 ## Median :57.00 Median : 2401 Median : 5.700 Median : 5.710 ## Mean :57.46 Mean : 3933 Mean : 5.731 Mean : 5.735 ## 3rd Qu.:59.00 3rd Qu.: 5324 3rd Qu.: 6.540 3rd Qu.: 6.540 ## Max. :95.00 Max. :18823 Max. :10.740 Max. :58.900 ## ## z ## Min. : 0.000 ## 1st Qu.: 2.910 ## Median : 3.530 ## Mean : 3.539 ## 3rd Qu.: 4.040 ## Max. :31.800 ## 8.3 Reading Data So far we’ve only been looking at built-in datasets. Ultimately, we’ll want to read in our own data. We’ll get to the art of loading and wrangling data later, but for now, know that the readr package provides us tools to read data. Let’s quickly practice loading data below. 8.3.1 q5 Use the function read_csv() to load the file \"./data/tiny.csv\". df_q5 &lt;- read_csv(&quot;./data/tiny.csv&quot;) ## Rows: 2 Columns: 2 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (2): x, y ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. df_q5 ## # A tibble: 2 × 2 ## x y ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 2 ## 2 3 4 "],["setup-documentation.html", "9 Setup: Documentation", " 9 Setup: Documentation Purpose: No programmer memorizes every fact about every function. Expert programmers get used to quickly reading documentation, which allows them to look up the facts they need, when they need them. Just as you had to learn how to read English, you will have to learn how to consult documentation. This exercise will get you started. Reading: Getting help with R (Vignettes and Code Demonstrations) library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✔ ggplot2 3.4.0 ✔ purrr 1.0.1 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.5.0 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() The vignette() function allows us to look up vignettes; short narrative-form tutorials associated with a package, written by its developers. 9.0.1 q1 Use vignette(package = ???) (fill in the ???) to look up vignettes associated with \"dplyr\". What vignettes are available? vignette(package = &quot;dplyr&quot;) The dplyr package has vignettes compatibility, dplyr, programming, two-table, and window-functions. We’ll cover many of these topics in this course! Once we know what vignettes are available, we can use the same function to read a particular vignette. 9.0.2 q2 Use vignette(???, package = \"dplyr\") to read the vignette on dplyr. Read this vignette up to the first note on filter(). Use filter() to select only those rows of the iris dataset where Species == \"setosa\". Note: This should open up your browser. iris %&gt;% as_tibble() %&gt;% filter( # TODO: Filter on Species &quot;setosa&quot; Species == &quot;setosa&quot; ) ## # A tibble: 50 × 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## # … with 40 more rows Vignettes are useful when we only know generally what we’re looking for. Once we know the verbs (functions) we want to use, we need more specific help. 9.0.3 q3 Remember back to e-setup02-functions; how do we look up help for a specific function? We have a few options: We can use ?function to look up the help for a function We can execute function (without parentheses) to show the source code Sometimes we’ll be working with a function, but we won’t quite know how to get it to do what we need. In this case, consulting the function’s documentation can be extremely helpful. 9.0.4 q4 Use your knowledge of documentation lookup to answer the following question: How could we filter the iris dataset to return only those rows with Sepal.Length between 5.1 and 6.4? iris %&gt;% as_tibble() %&gt;% filter( 5.1 &lt;= Sepal.Length, Sepal.Length &lt;= 6.4 ) ## # A tibble: 83 × 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 5.1 3.5 1.4 0.2 setosa ## 2 5.4 3.9 1.7 0.4 setosa ## 3 5.4 3.7 1.5 0.2 setosa ## 4 5.8 4 1.2 0.2 setosa ## 5 5.7 4.4 1.5 0.4 setosa ## 6 5.4 3.9 1.3 0.4 setosa ## 7 5.1 3.5 1.4 0.3 setosa ## 8 5.7 3.8 1.7 0.3 setosa ## 9 5.1 3.8 1.5 0.3 setosa ## 10 5.4 3.4 1.7 0.2 setosa ## # … with 73 more rows We have at least two options: We can use two lines of filter We can use the helper function between() On other occasions we’ll know a function, but would like to know about other, related functions. In this case, it’s useful to be able to trace the function back to its parent package. Then we can read the vignettes on the package to learn more. 9.0.5 q5 Look up the documentation on cut_number; what package does it come from? What about parse_number()? What about row_number()? ggplot2::cut_number() readr::parse_number() dplyr::row_number() "],["communication-code-style.html", "10 Communication: Code Style", " 10 Communication: Code Style Purpose: From the tidyverse style guide “Good coding style is like correct punctuation: you can manage without it, butitsuremakesthingseasiertoread.” We will follow the tidyverse style guide; Google’s internal R style guide is actually based on these guidelines! Reading: tidyverse style guide. Topics: Spacing (subsection only), Pipes (whole section) Reading Time: ~ 10 minutes library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✔ ggplot2 3.4.0 ✔ purrr 1.0.1 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.5.0 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() 10.0.1 q1 Re-write according to the style guide Hint: The pipe operator %&gt;% will help make this code more readable. ## Original code; hard to read summarize(group_by(diamonds, cut), mean_price = mean(price)) ## # A tibble: 5 × 2 ## cut mean_price ## &lt;ord&gt; &lt;dbl&gt; ## 1 Fair 4359. ## 2 Good 3929. ## 3 Very Good 3982. ## 4 Premium 4584. ## 5 Ideal 3458. diamonds %&gt;% group_by(cut) %&gt;% summarize( mean_price = mean(price) ) ## # A tibble: 5 × 2 ## cut mean_price ## &lt;ord&gt; &lt;dbl&gt; ## 1 Fair 4359. ## 2 Good 3929. ## 3 Very Good 3982. ## 4 Premium 4584. ## 5 Ideal 3458. 10.0.2 q2 Re-write according to the style guide Hint: There are particular rules about spacing! ## NOTE: You can copy this code to the chunk below iris %&gt;% mutate(Sepal.Area=Sepal.Length*Sepal.Width) %&gt;% group_by( Species ) %&gt;% summarize_if(is.numeric,mean)%&gt;% ungroup() %&gt;% pivot_longer( names_to=&quot;measure&quot;,values_to=&quot;value&quot;,cols=-Species ) %&gt;% arrange(value ) ## # A tibble: 15 × 3 ## Species measure value ## &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 setosa Petal.Width 0.246 ## 2 versicolor Petal.Width 1.33 ## 3 setosa Petal.Length 1.46 ## 4 virginica Petal.Width 2.03 ## 5 versicolor Sepal.Width 2.77 ## 6 virginica Sepal.Width 2.97 ## 7 setosa Sepal.Width 3.43 ## 8 versicolor Petal.Length 4.26 ## 9 setosa Sepal.Length 5.01 ## 10 virginica Petal.Length 5.55 ## 11 versicolor Sepal.Length 5.94 ## 12 virginica Sepal.Length 6.59 ## 13 versicolor Sepal.Area 16.5 ## 14 setosa Sepal.Area 17.3 ## 15 virginica Sepal.Area 19.7 iris %&gt;% mutate(Sepal.Area = Sepal.Length * Sepal.Width) %&gt;% group_by(Species) %&gt;% summarize_if(is.numeric, mean) %&gt;% ungroup() %&gt;% pivot_longer(names_to = &quot;measure&quot;, values_to = &quot;value&quot;, cols = -Species) %&gt;% arrange(value) ## # A tibble: 15 × 3 ## Species measure value ## &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 setosa Petal.Width 0.246 ## 2 versicolor Petal.Width 1.33 ## 3 setosa Petal.Length 1.46 ## 4 virginica Petal.Width 2.03 ## 5 versicolor Sepal.Width 2.77 ## 6 virginica Sepal.Width 2.97 ## 7 setosa Sepal.Width 3.43 ## 8 versicolor Petal.Length 4.26 ## 9 setosa Sepal.Length 5.01 ## 10 virginica Petal.Length 5.55 ## 11 versicolor Sepal.Length 5.94 ## 12 virginica Sepal.Length 6.59 ## 13 versicolor Sepal.Area 16.5 ## 14 setosa Sepal.Area 17.3 ## 15 virginica Sepal.Area 19.7 10.0.3 q3 Re-write according to the style guide Hint: What do we do about long lines? iris %&gt;% group_by(Species) %&gt;% summarize(Sepal.Length = mean(Sepal.Length), Sepal.Width = mean(Sepal.Width), Petal.Length = mean(Petal.Length), Petal.Width = mean(Petal.Width)) ## # A tibble: 3 × 5 ## Species Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 5.01 3.43 1.46 0.246 ## 2 versicolor 5.94 2.77 4.26 1.33 ## 3 virginica 6.59 2.97 5.55 2.03 iris %&gt;% group_by(Species) %&gt;% summarize( Sepal.Length = mean(Sepal.Length), Sepal.Width = mean(Sepal.Width), Petal.Length = mean(Petal.Length), Petal.Width = mean(Petal.Width) ) ## # A tibble: 3 × 5 ## Species Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 5.01 3.43 1.46 0.246 ## 2 versicolor 5.94 2.77 4.26 1.33 ## 3 virginica 6.59 2.97 5.55 2.03 The following is an addition I’m making to the “effective styleguide” for the class: Rather than doing this: ## NOTE: No need to edit, just an example ggplot(diamonds, aes(carat, price)) + geom_point() Instead, do this: ## NOTE: No need to edit, just an example diamonds %&gt;% ggplot(aes(carat, price)) + geom_point() This may seem like a small difference (it is), but getting in this habit will pay off when we start combining data operations with plotting; for instance: ## NOTE: No need to edit, just an example diamonds %&gt;% filter(1.5 &lt;= carat, carat &lt;= 2.0) %&gt;% ggplot(aes(carat, price)) + geom_point() Getting in the habit of “putting the data first” will make it easier for you to add preprocessing steps. Also, you can easily “disable” the plot to inspect your preprocessing while debugging; that is: ## NOTE: No need to edit, just an example diamonds %&gt;% filter(1.5 &lt;= carat, carat &lt;= 2.0) %&gt;% glimpse() ## Rows: 4,346 ## Columns: 10 ## $ carat &lt;dbl&gt; 1.50, 1.52, 1.52, 1.50, 1.50, 1.50, 1.51, 1.52, 1.52, 1.50, 1.… ## $ cut &lt;ord&gt; Fair, Good, Good, Fair, Good, Premium, Good, Fair, Premium, Pr… ## $ color &lt;ord&gt; H, E, E, H, G, H, G, H, I, H, I, F, F, I, H, D, G, F, H, H, E,… ## $ clarity &lt;ord&gt; I1, I1, I1, I1, I1, I1, I1, I1, I1, I1, I1, I1, I1, I1, I1, I1… ## $ depth &lt;dbl&gt; 65.6, 57.3, 57.3, 69.3, 57.4, 60.1, 64.0, 64.9, 61.2, 61.1, 63… ## $ table &lt;dbl&gt; 54, 58, 58, 61, 62, 57, 59, 58, 58, 59, 61, 59, 56, 58, 61, 60… ## $ price &lt;int&gt; 2964, 3105, 3105, 3175, 3179, 3457, 3497, 3504, 3541, 3599, 36… ## $ x &lt;dbl&gt; 7.26, 7.53, 7.53, 6.99, 7.56, 7.40, 7.29, 7.18, 7.43, 7.37, 7.… ## $ y &lt;dbl&gt; 7.09, 7.42, 7.42, 6.81, 7.39, 7.28, 7.17, 7.13, 7.35, 7.26, 7.… ## $ z &lt;dbl&gt; 4.70, 4.28, 4.28, 4.78, 4.29, 4.42, 4.63, 4.65, 4.52, 4.47, 4.… ## ggplot(aes(carat, price)) + ## geom_point() I’ll enforce this “data first” style, but after this class you are (of course) free to write code however you like! 10.0.4 q4 Re-write according to the style guide Hint: Put the data first! ggplot( iris %&gt;% pivot_longer( names_to = c(&quot;Part&quot;, &quot;.value&quot;), names_sep = &quot;\\\\.&quot;, cols = -Species ), aes(Width, Length, color = Part) ) + geom_point() + facet_wrap(~Species) iris %&gt;% pivot_longer( names_to = c(&quot;Part&quot;, &quot;.value&quot;), names_sep = &quot;\\\\.&quot;, cols = -Species ) %&gt;% ggplot(aes(Width, Length, color = Part)) + geom_point() + facet_wrap(~Species) "],["setup-rstudio-shortcuts.html", "11 Setup: RStudio Shortcuts", " 11 Setup: RStudio Shortcuts Purpose: Your ability to get stuff done is highly dependent on your fluency with your tools. One aspect of fluency is knowing and using keyboard shortcuts. In this exercise, we’ll go over some of the most important ones. Reading: Keyboard shortcuts; Code Chunk Options Note: Use this reading to look up answers to the questions below. Rather than memorizing this information, I recommend you download a cheatsheet, and either print it out or save it in a convenient place on your computer. Get used to referencing your cheatsheets while doing data science—practice makes perfect! 11.0.1 q1 What do the following keyboard shortcuts do? Within the script editor or a chunk Alt + - Insert assignment operator &lt;- * `Shift` + `Cmd/Ctrl` + `M` Insert pipe operator %&gt;% * `Cmd/Ctrl` + `Enter` If no text is highlighted, run complete expression that cursor is on, and advance to the next line. If text is highlighted, run selected text. * `F1` (Note: on a Mac you need to press `fn` + `F1`) If cursor is on a function name, pull up the help page for the function. * `Cmd/Ctrl` + `Shift` + `C` Comment/uncomment the code at the cursor, or highlighted code. Within R Markdown Cmd/Ctrl + Alt + I Insert chunk. Within a chunk Shift + Cmd/Ctrl + Enter Run current chunk. * `Ctrl` + `I` (`Cmd` + `I`) Re-indent selected lines. Try this below! ## This is what it should look like c( &quot;foo&quot;, &quot;bar&quot;, &quot;goo&quot;, &quot;gah&quot; ) ## [1] &quot;foo&quot; &quot;bar&quot; &quot;goo&quot; &quot;gah&quot; 11.0.2 q2 For a chunk, what header option do you use to Run the code, don’t display it, but show its results? echo=FALSE Run the code, but don’t display it or its results? include=FALSE 11.0.3 q3 How do stop the code in a chunk from running once it has started? Click the red square in the top right corner of the chunk. 11.0.4 q4 How do you show the “Document Outline” in RStudio? Hint: Try googling “rstudio document outline” Use the keyboard shortcut Ctrl + Shift + O (Windows), Super + Shift + O (Mac), or click the icon in the top-right of RStudio (it’s between the Publish and Compass icons). "],["setup-vector-basics.html", "12 Setup: Vector Basics", " 12 Setup: Vector Basics Purpose: Vectors are the most important object we’ll work with when doing data science. To that end, let’s learn some basics about vectors. Reading: Programming Basics. Topics: vectors Reading Time: ~10 minutes library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✔ ggplot2 3.4.0 ✔ purrr 1.0.1 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.5.0 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() 12.0.1 q1 What single-letter R function do you use to create vectors with specific entries? Use that function to create a vector with the numbers 1, 2, 3 below. vec_q1 &lt;- c(1, 2, 3) vec_q1 ## [1] 1 2 3 Use the following tests to check your work: ## NOTE: No need to change this assertthat::assert_that(length(vec_q1) == 3) ## [1] TRUE assertthat::assert_that(mean(vec_q1) == 2) ## [1] TRUE print(&quot;Nice!&quot;) ## [1] &quot;Nice!&quot; 12.0.2 q2 Did you know that you can use c() to extend a vector as well? Use this to add the extra entry 4 to vec_q1. vec_q2 &lt;- c(vec_q1, 4) vec_q2 ## [1] 1 2 3 4 Use the following tests to check your work: ## NOTE: No need to change this assertthat::assert_that(length(vec_q2) == 4) ## [1] TRUE assertthat::assert_that(mean(vec_q2) == 2.5) ## [1] TRUE print(&quot;Well done!&quot;) ## [1] &quot;Well done!&quot; "],["setup-types.html", "13 Setup: Types 13.1 Objects vs Strings 13.2 Casting", " 13 Setup: Types Purpose: Vectors can hold data of only one type. While this isn’t a course on computer science, there are some type “gotchas” to look out for when doing data science. This exercise will help us get ahead of those issues. Reading: Types library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✔ ggplot2 3.4.0 ✔ purrr 1.0.1 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.5.0 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() 13.1 Objects vs Strings 13.1.1 q1 Describe what is wrong with the code below. ## TASK: Describe what went wrong here ## Set our airport airport &lt;- &quot;BOS&quot; ## Check our airport value airport == ATL Observations: ATL (without quotes) is trying to refer to an object (variable); we would need to write \"ATL\" (with quotes) to produce a string. 13.2 Casting Sometimes our data will not be in the form we want; in this case we may need to cast the data to another format. as.integer(x) converts to integer as.numeric(x) converts to real (floating point) as.character(x) converts to character (string) as.logical(x) converts to logical (boolean) 13.2.1 q2 Cast the following vector v_string to integers. v_string &lt;- c(&quot;00&quot;, &quot;45&quot;, &quot;90&quot;) v_integer &lt;- as.integer(v_string) Use the following test to check your work. ## NOTE: No need to change this! assertthat::assert_that( assertthat::are_equal( v_integer, c(0L, 45L, 90L) ) ) ## [1] TRUE print(&quot;Great job!&quot;) ## [1] &quot;Great job!&quot; "],["vis-data-visualization-basics.html", "14 Vis: Data Visualization Basics 14.1 A note on aesthetics", " 14 Vis: Data Visualization Basics Purpose: The most powerful way for us to learn about a dataset is to visualize the data. Throughout this class we will make extensive use of the grammar of graphics, a powerful graphical programming grammar that will allow us to create just about any graph you can imagine! Reading: Data Visualization Basics. Note: In RStudio use Ctrl + Click (Mac Command + Click) to follow the link. Topics: Welcome, A code template, Aesthetic mappings. Reading Time: ~ 30 minutes 14.0.1 q1 Inspect the diamonds dataset. What do the cut, color, and clarity variables mean? Hint: We learned how to inspect a dataset in e-data-00-basics! ?diamonds 14.0.2 q2 Use your “standard checks” to determine what variables the dataset has. glimpse(diamonds) ## Rows: 53,940 ## Columns: 10 ## $ carat &lt;dbl&gt; 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, 0.23, 0.… ## $ cut &lt;ord&gt; Ideal, Premium, Good, Premium, Good, Very Good, Very Good, Ver… ## $ color &lt;ord&gt; E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J, J, J, I,… ## $ clarity &lt;ord&gt; SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, SI1, VS1, … ## $ depth &lt;dbl&gt; 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, 59.4, 64… ## $ table &lt;dbl&gt; 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54, 62, 58… ## $ price &lt;int&gt; 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339, 340, 34… ## $ x &lt;dbl&gt; 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, 4.00, 4.… ## $ y &lt;dbl&gt; 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, 4.05, 4.… ## $ z &lt;dbl&gt; 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, 2.39, 2.… The diamonds dataset has variables carat, cut, color, clarity, depth, table, price, x, y, z. Now that we have the list of variables in the dataset, we know what we can visualize! 14.0.3 q3 Using ggplot, visualize price vs carat with points. What trend do you observe? Hint: Usually the language y vs x refers to the vertical axis vs horizontal axis. This is the opposite order from the way we often specify x, y pairs. Language is hard! ## TODO: Complete this code ggplot(diamonds) + geom_point(aes(x = carat, y = price)) Observations: price generally increases with carat The trend is not ‘clean’; there is no single curve in the relationship 14.1 A note on aesthetics The function aes() is short for aesthetics. Aesthetics in ggplot are the mapping of variables in a dataframe to visual elements in the graph. For instance, in the plot above you assigned carat to the x aesthetic, and price to the y aesthetic. But there are many more aesthetics you can set, some of which vary based on the geom_ you are using to visualize. The next question will explore this idea more. 14.1.1 q4 Create a new graph to visualize price, carat, and cut simultaneously. Hint: Remember that you can add additional aesthetic mappings in aes(). Some options include size, color, and shape. ## TODO: Complete this code ggplot(diamonds) + geom_point(aes(x = carat, y = price, color = cut)) Observations: price generally increases with carat The cut helps explain the variation in price; Ideal cut diamonds tend to be more expensive Fair cut diamonds tend to be less expensive "],["communication-active-and-constructive-responding.html", "15 Communication: Active and Constructive Responding 15.1 Review: Active and Constructive Responding (ACR) 15.2 Having Productive Discussions 15.3 Active and Constructive Questions 15.4 Refrences", " 15 Communication: Active and Constructive Responding Purpose: We’re going to spend a lot of time in this class discussing datasets. To help prepare ourselves for productive discussions, we’re going to use some ideas from positive psychology to help make our discussions more pleasant and productive. Reading: GoStrengths: What is Active and Constructive Responding? Reading Time: ~ 10 minutes library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✔ ggplot2 3.4.0 ✔ purrr 1.0.1 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.5.0 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() 15.1 Review: Active and Constructive Responding (ACR) The reading introduces the idea of active and constructive responding (ACR) to good news. The following table summarizes the four primary ways of responding to someone when presented with good news: Active Passive Constructive Good! Bad Destructive Bad Danger zone! We’ll call this the active-passive-constructive-destructive (APCD) framework. For example, if your friend tells you “Hey, I just got an A on my exam!”, you could respond in a number of different ways: “That’s terrific! I remember you were studying really hard. What do you want to do to celebrate?” (Active and Constructive) This is active in the sense that it responds to and builds upon the substance of your friend’s statement. This is constructive in that it conveys positive emotion. (This is the best way you can respond to good news [1]!) “Oh that’s cool, I guess.” (Passive and Constructive) This is constructive in that it conveys positive emotion. However this is passive in that it does not actively engage with your friend’s statement. “But the professor was grading on a curve! Gah, that means I probably didn’t get an A.” (Active and Destructive) This is active in the sense that it responds to and builds upon the substance of your friend’s statement. However this is destructive in that it conveys a negative reaction to your friend’s statement. “Dude whatever. I got an internship!” Passive and Destructive: However this is passive in that it does not actively engage with your friend’s statement. This is also destructive in that it implicitly conveys a negative reaction by ignoring the substance of your friend’s statement. (This is the worst way you can respond to good news!) Active and constructive responding (ACR) has been shown to be associated with more positive interpersonal relationships [1]. If you want want to have better platonic or romantic relationships, you sould practice ACR! 15.2 Having Productive Discussions What does ACR have to do with data science? Imagine you’re in a meeting with some data science colleagues, and the presenter shows the following graph. ## NOTE: No need to edit; just run and inspect diamonds %&gt;% slice_sample(n = 1000) %&gt;% ggplot(aes(carat, price)) + geom_point() + scale_x_log10() + scale_y_log10() + theme_minimal() + labs(x = &quot;&quot;, y = &quot;&quot;) Let’s use the active-passive-constructive-destructive (APCD) response framework above to analyze some discussion scenarios. Imagine the following scenario: Presenter: “Clearly, we can see a positive correlation between price and carat.” Colleague: “Um, what are the axes here?” Presenter: “Obviously the graph shows price vs carat. On the next slide…” Here the bolded response is actively engaging with your colleague’s question, but is doing so in a very destructive way. It’s likely your colleague feels hurt, and is less likely now to speak up in meetings. That’s a bad thing! 15.2.1 q1 Identify the nature of the bolded line below, rating it as either active vs passive and constructive vs destructive. Presenter: “Clearly, we can see a positive correlation between price and carat.” Colleague: “Um, what are the axes here?” Presenter: “Woah good catch there, they’re unlabeled. The vertical shows price, and the horizontal shows carat.” Observations: This response is active as it directly engages with the colleague’s question. This response is constructive as it validates the colleague’s question before providing an answer. 15.2.2 q2 Identify the nature of the bolded line below, rating it as either active vs passive and constructive vs destructive. Presenter: “Clearly, we can see a positive correlation between price and carat.” Colleague: “Um, what are the axes here?” Presenter: “Woah good catch there, they’re unlabeled. Moving on….” Observations: This response is passive as it does not actually engage with the colleague’s question. This response is constructive as it validates the colleague’s question, even though it does not provide an answer. Intermediate Conclusion: We can quite naturally think of discussion responses in the APCD framework. Therefore, we should try to give active and constructive responses in data discussions! 15.3 Active and Constructive Questions With a bit of imagination, we can apply the APCD framework to questions as well. Let’s keep looking at the same graph. ## NOTE: No need to edit; just run and inspect diamonds %&gt;% slice_sample(n = 1000) %&gt;% ggplot(aes(carat, price)) + geom_point() + scale_x_log10() + scale_y_log10() + theme_minimal() + labs(x = &quot;&quot;, y = &quot;&quot;) Imagine the following scenario: Presenter: “Clearly, we can see a positive correlation between price and carat.” Colleague: “Percival, why are you so terrible at making graphs?” Presenter: “…” Here the bolded question is passively engaging with the presenter’s content (it’s not referring to any issues in the graph, but rather making an ad hominem attack of the presenter) and destructively making a value judgment on the presenter. This is a truly horrendous way to ask a question. Let’s look at a far more effective way to ask a question: Presenter: “Clearly, we can see a positive correlation between price and carat.” Colleague: “Percival, this is an interesting finding but I’m confused by this graph—what do the axes represent?” Presenter: “Oh, the labels are missing! The vertical shows price, and the horizontal shows carat.” Here the bolded question is actively engaging with the presenter’s content (the colleague makes it clear what the problem with the graph is) and constructively validates the presenter’s findings. This is far more effective than the question posed above. As a bonus, this question also illustrates another tip: Making “I” statements. To see the difference, let’s look at a subtly-different version of the same question: Presenter: “Clearly, we can see a positive correlation between price and carat.” Colleague: “Percival, this is an interesting finding but your graph is very confusing—what do the axes represent?” Presenter: “Oh, the labels are missing! The vertical shows price, and the horizontal shows carat.” In this second version the colleague is making a value-judgment about the graph itself; in the first version the colleague is making a statement about their own subjective experience. The second version is debatable (the presenter may think the graph is clear), but only a jerk would disagree with a person’s subjective experience. Let’s practice once more! 15.3.1 q3 Identify the nature of the bolded line below, rating it as either active vs passive and constructive vs destructive. Presenter: “Clearly, we can see a positive correlation between price and carat.” Colleague: “What is your opinion on the morality of aesthetic diamonds?” Presenter: “Umm….” Observations: This response is passive as is a clear departure from the topic the presenter is trying to discuss. Honestly I find it difficult to rate this as constructive or destructive. This comment breaks the framework! 15.4 Refrences [1] Gable, S. L., Reis, H. T., Impett, E. A., &amp; Asher, E. R. (2004). What do you do when things go right? The intrapersonal and interpersonal benefits of sharing positive events. Journal of Personality and Social Psychology, 87, 228-245. "],["stats-eda-basics.html", "16 Stats: EDA Basics 16.1 Hypothesis 1 16.2 Hypothesis 2 16.3 Unraveling Hypothesis 2 16.4 Footnotes", " 16 Stats: EDA Basics Purpose: Exploratory Data Analysis (EDA) is a crucial skill for a practicing data scientist. Unfortunately, much like human-centered design EDA is hard to teach. This is because EDA is not a strict procedure, so much as it is a mindset. Also, much like human-centered design, EDA is an iterative, nonlinear process. There are two key principles to keep in mind when doing EDA: Curiosity: Generate lots of ideas and hypotheses about your data. Skepticism: Remain unconvinced of those ideas, unless you can find credible patterns to support them. Since EDA is both crucial and difficult, we will practice doing EDA a lot in this course! Reading: Exploratory Data Analysis Topics: (All topics) Reading Time: ~45 minutes Note: This exercise will consist of interpreting pre-made graphs. You can run the whole notebook to generate all the figures at once. Just make sure to do all the exercises and write your observations! library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✔ ggplot2 3.4.0 ✔ purrr 1.0.1 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.5.0 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() 16.0.1 q0 Remember from e02-data-basics there were simple checks we’re supposed to do? Do those simple checks on the diamonds dataset below. ## TODO: Recall time! Do our first simple checks on the diamonds dataset diamonds %&gt;% glimpse ## Rows: 53,940 ## Columns: 10 ## $ carat &lt;dbl&gt; 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, 0.23, 0.… ## $ cut &lt;ord&gt; Ideal, Premium, Good, Premium, Good, Very Good, Very Good, Ver… ## $ color &lt;ord&gt; E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J, J, J, I,… ## $ clarity &lt;ord&gt; SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, SI1, VS1, … ## $ depth &lt;dbl&gt; 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, 59.4, 64… ## $ table &lt;dbl&gt; 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54, 62, 58… ## $ price &lt;int&gt; 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339, 340, 34… ## $ x &lt;dbl&gt; 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, 4.00, 4.… ## $ y &lt;dbl&gt; 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, 4.05, 4.… ## $ z &lt;dbl&gt; 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, 2.39, 2.… I’m going to walk you through a train of thought I had when studying the diamonds dataset. There are four standard “C’s” of judging a diamond.[1] These are carat, cut, color and clarity, all of which are in the diamonds dataset. 16.1 Hypothesis 1 Here’s a hypothesis: Ideal is the “best” value of cut for a diamond. Since an Ideal cut seems more labor-intensive, I hypothesize that Ideal cut diamonds are less numerous than other cuts. 16.1.1 q1 Run the chunk below, and study the plot. Was hypothesis 1 correct? Why or why not? diamonds %&gt;% ggplot(aes(cut)) + geom_bar() Observations: - The truth is actually the opposite! Ideal cut diamonds are more numerous than all other cuts! Perhaps because cutting a diamond is easier than mining a new one, gemcutters add value to a diamond by striving for an ideal cut. 16.2 Hypothesis 2 Another hypothesis: The Ideal cut diamonds should be the most pricey. 16.2.1 q2.1 Study the following graph; does it support, contradict, or not relate to hypothesis 2? Hint: Is this an effective graph? Why or why not? diamonds %&gt;% ggplot(aes(cut, price)) + geom_point() Observations: - This graph is virtually useless! There is severe overplotting. We cannot tell address Hypothesis 2 with this graph The following is a set of boxplots; the middle bar denotes the median, the boxes denote the quartiles (upper and lower “quarters” of the data), and the lines and dots denote large values and outliers. 16.2.2 q2.2 Study the following graph; does it support or contradict hypothesis 2? diamonds %&gt;% ggplot(aes(cut, price)) + geom_boxplot() Observations: - Surprisingly, Ideal diamonds tend to be the least pricey! This was very surprising to me. 16.3 Unraveling Hypothesis 2 Upon making the graph in q2.2, I was very surprised. So I did some reading on diamond cuts. It turns out that some gemcutters sacrifice cut for carat. Could this effect explain the surprising pattern above? 16.3.1 q3 Study the following graph; does it support a “carat over cut” hypothesis? How might this relate to price? Hint: The article linked above will help you answer these questions! diamonds %&gt;% ggplot(aes(cut, carat)) + geom_boxplot() Observations: - The median of Ideal diamonds is a fair bit lower in carat than other cuts. This provides some evidence that gemcutters trade cut for carat. - The very largest carat diamonds tend to be of Fair cut; this makese sense, as cutting the gemstone will only reduce weight. - It seems that many diamond purchasers are more interested in carat than fine cut. This provides some rationale for why Ideal diamonds are cheaper; they are necessarily lower-carat. 16.4 Footnotes [1] Don’t mistake my focus on diamonds as an endorsement of the diamond industry. In my opinion aesthetic diamonds are a morally dubious scam. "],["data-isolating-data.html", "17 Data: Isolating Data", " 17 Data: Isolating Data Purpose: One of the keys to a successful analysis is the ability to focus on particular topics. When analyzing a dataset, our ability to focus is tied to our facility at isolating data. In this exercise, you will practice isolating columns with select(), picking specific rows with filter(), and sorting your data with arrange() to see what rises to the top. Reading: Isolating Data with dplyr (All topics) library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✔ ggplot2 3.4.0 ✔ purrr 1.0.1 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.5.0 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() library(nycflights13) # For `flights` data We’ll use the nycflights13 dataset for this exercise; upon loading the package, the data are stored in the variable name flights. For instance: flights %&gt;% glimpse() ## Rows: 336,776 ## Columns: 19 ## $ year &lt;int&gt; 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2… ## $ month &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ day &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ dep_time &lt;int&gt; 517, 533, 542, 544, 554, 554, 555, 557, 557, 558, 558, … ## $ sched_dep_time &lt;int&gt; 515, 529, 540, 545, 600, 558, 600, 600, 600, 600, 600, … ## $ dep_delay &lt;dbl&gt; 2, 4, 2, -1, -6, -4, -5, -3, -3, -2, -2, -2, -2, -2, -1… ## $ arr_time &lt;int&gt; 830, 850, 923, 1004, 812, 740, 913, 709, 838, 753, 849,… ## $ sched_arr_time &lt;int&gt; 819, 830, 850, 1022, 837, 728, 854, 723, 846, 745, 851,… ## $ arr_delay &lt;dbl&gt; 11, 20, 33, -18, -25, 12, 19, -14, -8, 8, -2, -3, 7, -1… ## $ carrier &lt;chr&gt; &quot;UA&quot;, &quot;UA&quot;, &quot;AA&quot;, &quot;B6&quot;, &quot;DL&quot;, &quot;UA&quot;, &quot;B6&quot;, &quot;EV&quot;, &quot;B6&quot;, &quot;… ## $ flight &lt;int&gt; 1545, 1714, 1141, 725, 461, 1696, 507, 5708, 79, 301, 4… ## $ tailnum &lt;chr&gt; &quot;N14228&quot;, &quot;N24211&quot;, &quot;N619AA&quot;, &quot;N804JB&quot;, &quot;N668DN&quot;, &quot;N394… ## $ origin &lt;chr&gt; &quot;EWR&quot;, &quot;LGA&quot;, &quot;JFK&quot;, &quot;JFK&quot;, &quot;LGA&quot;, &quot;EWR&quot;, &quot;EWR&quot;, &quot;LGA&quot;,… ## $ dest &lt;chr&gt; &quot;IAH&quot;, &quot;IAH&quot;, &quot;MIA&quot;, &quot;BQN&quot;, &quot;ATL&quot;, &quot;ORD&quot;, &quot;FLL&quot;, &quot;IAD&quot;,… ## $ air_time &lt;dbl&gt; 227, 227, 160, 183, 116, 150, 158, 53, 140, 138, 149, 1… ## $ distance &lt;dbl&gt; 1400, 1416, 1089, 1576, 762, 719, 1065, 229, 944, 733, … ## $ hour &lt;dbl&gt; 5, 5, 5, 5, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 6, 6, 6… ## $ minute &lt;dbl&gt; 15, 29, 40, 45, 0, 58, 0, 0, 0, 0, 0, 0, 0, 0, 0, 59, 0… ## $ time_hour &lt;dttm&gt; 2013-01-01 05:00:00, 2013-01-01 05:00:00, 2013-01-01 0… 17.0.1 q1 Select all the variables whose name ends with _time. df_q1 &lt;- flights %&gt;% select(ends_with(&quot;_time&quot;)) df_q1 ## # A tibble: 336,776 × 5 ## dep_time sched_dep_time arr_time sched_arr_time air_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 517 515 830 819 227 ## 2 533 529 850 830 227 ## 3 542 540 923 850 160 ## 4 544 545 1004 1022 183 ## 5 554 600 812 837 116 ## 6 554 558 740 728 150 ## 7 555 600 913 854 158 ## 8 557 600 709 723 53 ## 9 557 600 838 846 140 ## 10 558 600 753 745 138 ## # … with 336,766 more rows The following is a unit test of your code; if you managed to solve task q2 correctly, the following code will execute without error. ## NOTE: No need to change this assertthat::assert_that( all(names(df_q1) %&gt;% str_detect(., &quot;_time$&quot;)) ) ## [1] TRUE print(&quot;Nice!&quot;) ## [1] &quot;Nice!&quot; 17.0.2 q2 Re-arrange the columns to place dest, origin, carrier at the front, but retain all other columns. Hint: The function everything() will be useful! df_q2 &lt;- flights %&gt;% select(dest, origin, carrier, everything()) df_q2 ## # A tibble: 336,776 × 19 ## dest origin carrier year month day dep_time sched_dep_t…¹ dep_d…² arr_t…³ ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 IAH EWR UA 2013 1 1 517 515 2 830 ## 2 IAH LGA UA 2013 1 1 533 529 4 850 ## 3 MIA JFK AA 2013 1 1 542 540 2 923 ## 4 BQN JFK B6 2013 1 1 544 545 -1 1004 ## 5 ATL LGA DL 2013 1 1 554 600 -6 812 ## 6 ORD EWR UA 2013 1 1 554 558 -4 740 ## 7 FLL EWR B6 2013 1 1 555 600 -5 913 ## 8 IAD LGA EV 2013 1 1 557 600 -3 709 ## 9 MCO JFK B6 2013 1 1 557 600 -3 838 ## 10 ORD LGA AA 2013 1 1 558 600 -2 753 ## # … with 336,766 more rows, 9 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, air_time &lt;dbl&gt;, ## # distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, and abbreviated ## # variable names ¹​sched_dep_time, ²​dep_delay, ³​arr_time Use the following to check your code. ## NOTE: No need to change this assertthat::assert_that( assertthat::are_equal(names(df_q2)[1:5], c(&quot;dest&quot;, &quot;origin&quot;, &quot;carrier&quot;, &quot;year&quot;, &quot;month&quot;)) ) ## [1] TRUE print(&quot;Well done!&quot;) ## [1] &quot;Well done!&quot; Since R will only show the first few columns of a tibble, using select() in this fashion will help us see the values of particular columns. 17.0.3 q3 Fix the following code. What is the mistake here? What is the code trying to accomplish? flights %&gt;% filter(dest == &quot;LAX&quot;) ## # A tibble: 16,174 × 19 ## year month day dep_time sched_de…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 2013 1 1 558 600 -2 924 917 7 UA ## 2 2013 1 1 628 630 -2 1016 947 29 UA ## 3 2013 1 1 658 700 -2 1027 1025 2 VX ## 4 2013 1 1 702 700 2 1058 1014 44 B6 ## 5 2013 1 1 743 730 13 1107 1100 7 AA ## 6 2013 1 1 828 823 5 1150 1143 7 UA ## 7 2013 1 1 829 830 -1 1152 1200 -8 UA ## 8 2013 1 1 856 900 -4 1226 1220 6 AA ## 9 2013 1 1 859 900 -1 1223 1225 -2 VX ## 10 2013 1 1 921 900 21 1237 1227 10 DL ## # … with 16,164 more rows, 9 more variables: flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, and abbreviated variable names ## # ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay The next error is far more insidious…. 17.0.4 q4 This code doesn’t quite what the user intended. What went wrong? flights %&gt;% filter(dest == &quot;BOS&quot;) ## # A tibble: 15,508 × 19 ## year month day dep_time sched_de…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 2013 1 1 559 559 0 702 706 -4 B6 ## 2 2013 1 1 639 640 -1 739 749 -10 B6 ## 3 2013 1 1 801 805 -4 900 919 -19 B6 ## 4 2013 1 1 803 810 -7 903 925 -22 AA ## 5 2013 1 1 820 830 -10 940 954 -14 DL ## 6 2013 1 1 923 919 4 1026 1030 -4 B6 ## 7 2013 1 1 957 733 144 1056 853 123 UA ## 8 2013 1 1 1033 1017 16 1130 1136 -6 UA ## 9 2013 1 1 1154 1200 -6 1253 1306 -13 B6 ## 10 2013 1 1 1237 1245 -8 1340 1350 -10 AA ## # … with 15,498 more rows, 9 more variables: flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, and abbreviated variable names ## # ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay It will take practice to get used to when and when not to use quotations. Don’t worry—we’ll get lots of practice! This dataset is called nycflights; in what sense is it focused on New York city? Let’s do a quick check to get an idea: 17.0.5 q5 Perform two filters; first df_q5a &lt;- flights %&gt;% filter( dest == &quot;JFK&quot; | dest == &quot;LGA&quot; | dest == &quot;EWR&quot; ) df_q5b &lt;- flights %&gt;% filter( origin == &quot;JFK&quot; | origin == &quot;LGA&quot; | origin == &quot;EWR&quot; ) Use the following code to check your answer. ## NOTE: No need to change this! assertthat::assert_that( df_q5a %&gt;% mutate(flag = dest %in% c(&quot;JFK&quot;, &quot;LGA&quot;, &quot;EWR&quot;)) %&gt;% summarize(flag = all(flag)) %&gt;% pull(flag) ) ## [1] TRUE assertthat::assert_that( df_q5b %&gt;% mutate(flag = origin %in% c(&quot;JFK&quot;, &quot;LGA&quot;, &quot;EWR&quot;)) %&gt;% summarize(flag = all(flag)) %&gt;% pull(flag) ) ## [1] TRUE print(&quot;Nice!&quot;) ## [1] &quot;Nice!&quot; The fact that all observations have their origin in NYC highlights (but does not itself prove!) that these data were collected to study flights departing from the NYC area. Aside: Data are not just numbers. Data are numbers with context. Every dataset is put together for some reason. This reason will inform what observations (rows) and variables (columns) are in the data, and which are not in the data. Conversely, thinking carefully about what data a person or organization bothered to collect—and what they ignored—can tell you something about the perspective of those who collected the data. Thinking about these issues is partly what separates data science from programming or machine learning. (end-rant) 17.0.6 q6 Sort the flights in descending order by their air_time. Bring air_time, dest to the front. What can you tell about the longest flights? df_q6 &lt;- flights %&gt;% select(air_time, dest, everything()) %&gt;% arrange(desc(air_time)) df_q6 ## # A tibble: 336,776 × 19 ## air_time dest year month day dep_time sched_dep…¹ dep_d…² arr_t…³ sched…⁴ ## &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 695 HNL 2013 3 17 1337 1335 2 1937 1836 ## 2 691 HNL 2013 2 6 853 900 -7 1542 1540 ## 3 686 HNL 2013 3 15 1001 1000 1 1551 1530 ## 4 686 HNL 2013 3 17 1006 1000 6 1607 1530 ## 5 683 HNL 2013 3 16 1001 1000 1 1544 1530 ## 6 679 HNL 2013 2 5 900 900 0 1555 1540 ## 7 676 HNL 2013 11 12 936 930 6 1630 1530 ## 8 676 HNL 2013 3 14 958 1000 -2 1542 1530 ## 9 675 HNL 2013 11 20 1006 1000 6 1639 1555 ## 10 671 HNL 2013 3 15 1342 1335 7 1924 1836 ## # … with 336,766 more rows, 9 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, ## # flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, and abbreviated variable names ## # ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time ## NOTE: No need to change this! assertthat::assert_that( assertthat::are_equal( df_q6 %&gt;% head(1) %&gt;% pull(air_time), flights %&gt;% pull(air_time) %&gt;% max(na.rm = TRUE) ) ) ## [1] TRUE assertthat::assert_that( assertthat::are_equal( df_q6 %&gt;% filter(!is.na(air_time)) %&gt;% tail(1) %&gt;% pull(air_time), flights %&gt;% pull(air_time) %&gt;% min(na.rm = TRUE) ) ) ## [1] TRUE assertthat::assert_that( assertthat::are_equal( names(df_q6)[1:2], c(&quot;air_time&quot;, &quot;dest&quot;) ) ) ## [1] TRUE print(&quot;Great job!&quot;) ## [1] &quot;Great job!&quot; The longest flights are to “HNL”; this makes sense—Honolulu is quite far from NYC! "],["vis-bar-charts.html", "18 Vis: Bar Charts", " 18 Vis: Bar Charts Purpose: Bar charts are a key tool for EDA. In this exercise, we’ll learn how to construct a variety of different bar charts, as well as when—and when not—to use various charts. Reading: Bar Charts Topics: (All topics) Reading Time: ~30 minutes library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✔ ggplot2 3.4.0 ✔ purrr 1.0.1 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.5.0 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() 18.0.1 q1 In the reading, you learned the relation between geom_bar() and geom_col(). Use that knowledge to convert the following geom_bar() plot into the same visual using geom_col(). mpg %&gt;% count(trans) %&gt;% ggplot(aes(x = trans, y = n)) + geom_col() The reading mentioned that when using geom_col() our x-y data should be 1-to-1. This next exercise will probe what happens when our data are not 1-to-1, and yet we use a geom_col(). Note that a one-to-one function is one where each input leads to a single output. For the mpg dataset, we can see that the pairs cty, hwy clearly don’t have this one-to-one property: ## NOTE: Run this chunk for an illustration mpg %&gt;% filter(cty == 20) ## # A tibble: 11 × 11 ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 audi a4 2 2008 4 manu… f 20 31 p comp… ## 2 audi a4 quattro 2 2008 4 manu… 4 20 28 p comp… ## 3 hyundai tiburon 2 2008 4 manu… f 20 28 r subc… ## 4 hyundai tiburon 2 2008 4 auto… f 20 27 r subc… ## 5 subaru forester … 2.5 2008 4 manu… 4 20 27 r suv ## 6 subaru forester … 2.5 2008 4 auto… 4 20 26 r suv ## 7 subaru impreza a… 2.5 2008 4 auto… 4 20 25 p comp… ## 8 subaru impreza a… 2.5 2008 4 auto… 4 20 27 r comp… ## 9 subaru impreza a… 2.5 2008 4 manu… 4 20 27 r comp… ## 10 volkswagen new beetle 2.5 2008 5 manu… f 20 28 r subc… ## 11 volkswagen new beetle 2.5 2008 5 auto… f 20 29 r subc… 18.0.2 q2 The following code attempts to visualize cty, hwy from mpg using geom_col(). There’s something fishy about the hwy values; what’s wrong here? Hint: Try changing the position parameter for geom_col(). mpg %&gt;% ggplot(aes(x = cty, y = hwy)) + geom_col(position = &quot;dodge&quot;) Observations: - Since position = \"stacked\" is the default for geom_col(), we see not the real hwy values, but effectively a sum at each cty value! A more standard way to visualize this kind of data is a scatterplot, which we’ll study later. For now, here’s an example of a more effective way to visualize cty vs hwy: ## NOTE: Run this chunk for an illustration mpg %&gt;% ggplot(aes(cty, hwy)) + geom_point() 18.0.3 q3 The following are two different visualizations of the mpg dataset. Document your observations between the v1 and v2 visuals. Then, determine which—v1 or v2—enabled you to make more observations. What was the difference between the two visuals? ## TODO: Run this code without changing, describe your observations on the data mpg %&gt;% ggplot(aes(class, fill = class)) + geom_bar() Observations: In this dataset: - SUV’s are most numerous, followed by compact and midsize - There are very few 2seater vehicles ## TODO: Run this code without changing, describe your observations on the data mpg %&gt;% ggplot(aes(class, fill = drv)) + geom_bar() Observations: In this dataset: - SUV’s are most numerous, followed by compact and midsize - There are very few 2seater vehicles - pickup’s and SUV’s tend to have 4 wheel drive - compact’s and midsize tend to have f drive - All the 2seater vehicles are r drive Compare v1 and v2: Which visualization—v1 or v2—enabled you to make more observations? v2 enabled me to make more observations What was the difference between v1 and v2? v1 showed the same variable class using two aesthetics v2 showed two variables class and drv using two aesthetics 18.0.4 q4 The following code has a bug; it does not do what its author intended. Identify and fix the bug. What does the resulting graph tell you about the relation between manufacturer and class of cars in this dataset? Note: I use a theme() call to rotate the x-axis labels. We’ll learn how to do this in a future exercise. mpg %&gt;% ggplot(aes(x = manufacturer, fill = class)) + geom_bar(position = &quot;dodge&quot;) + theme(axis.text.x = element_text(angle = 270, vjust = 0.5, hjust = 0)) Observations - Certain manufacturers seem to favor particular classes of car. For instance, in this dataset: - Jeep, Land Rover, Lincoln, and Mercury only have suv’s - Audi, Toyota, and Volkswagen favor compact - Dodge favors pickup 18.0.5 q5 The following graph is hard to read. What other form of faceting would make the visual more convenient to read? Modify the code below with your suggested improvement. mpg %&gt;% ggplot(aes(x = cyl)) + geom_bar() + facet_wrap(~ manufacturer) "],["data-deriving-quantities.html", "19 Data: Deriving Quantities 19.1 Summary Functions", " 19 Data: Deriving Quantities Purpose: Often our data will not tell us directly what we want to know; in these cases we need to derive new quantities from our data. In this exercise, we’ll work with mutate() to create new columns by operating on existing variables, and use group_by() with summarize() to compute aggregate statistics (summaries!) of our data. Reading: Derive Information with dplyr Topics: (All topics, except Challenges) Reading Time: ~60 minutes Note: I’m considering splitting this exercise into two parts; I welcome feedback on this idea. library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✔ ggplot2 3.4.0 ✔ purrr 1.0.1 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.5.0 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() 19.0.1 q1 What is the difference between these two versions? How are they the same? How are they different? ## Version 1 filter(diamonds, cut == &quot;Ideal&quot;) ## # A tibble: 21,551 × 10 ## carat cut color clarity depth table price x y z ## &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 ## 2 0.23 Ideal J VS1 62.8 56 340 3.93 3.9 2.46 ## 3 0.31 Ideal J SI2 62.2 54 344 4.35 4.37 2.71 ## 4 0.3 Ideal I SI2 62 54 348 4.31 4.34 2.68 ## 5 0.33 Ideal I SI2 61.8 55 403 4.49 4.51 2.78 ## 6 0.33 Ideal I SI2 61.2 56 403 4.49 4.5 2.75 ## 7 0.33 Ideal J SI1 61.1 56 403 4.49 4.55 2.76 ## 8 0.23 Ideal G VS1 61.9 54 404 3.93 3.95 2.44 ## 9 0.32 Ideal I SI1 60.9 55 404 4.45 4.48 2.72 ## 10 0.3 Ideal I SI2 61 59 405 4.3 4.33 2.63 ## # … with 21,541 more rows ## Version 2 diamonds %&gt;% filter(cut == &quot;Ideal&quot;) ## # A tibble: 21,551 × 10 ## carat cut color clarity depth table price x y z ## &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 ## 2 0.23 Ideal J VS1 62.8 56 340 3.93 3.9 2.46 ## 3 0.31 Ideal J SI2 62.2 54 344 4.35 4.37 2.71 ## 4 0.3 Ideal I SI2 62 54 348 4.31 4.34 2.68 ## 5 0.33 Ideal I SI2 61.8 55 403 4.49 4.51 2.78 ## 6 0.33 Ideal I SI2 61.2 56 403 4.49 4.5 2.75 ## 7 0.33 Ideal J SI1 61.1 56 403 4.49 4.55 2.76 ## 8 0.23 Ideal G VS1 61.9 54 404 3.93 3.95 2.44 ## 9 0.32 Ideal I SI1 60.9 55 404 4.45 4.48 2.72 ## 10 0.3 Ideal I SI2 61 59 405 4.3 4.33 2.63 ## # … with 21,541 more rows These two lines carry out the same computation. However, Version 2 uses the pipe %&gt;%, which takes its left-hand-side (LHS) and inserts the LHS as the first argument of the right-hand-side (RHS). The reading mentioned various kinds of summary functions, which are summarized in the table below: 19.1 Summary Functions Type Functions Location mean(x), median(x), quantile(x, p), min(x), max(x) Spread sd(x), var(x), IQR(x), mad(x) Position first(x), nth(x, n), last(x) Counts n_distinct(x), n() Logical sum(!is.na(x)), mean(y == 0) 19.1.1 q2 Using summarize() and a logical summary function, determine the number of rows with Ideal cut. Save this value to a column called n_ideal. df_q2 &lt;- diamonds %&gt;% summarize(n_ideal = sum(cut == &quot;Ideal&quot;)) df_q2 ## # A tibble: 1 × 1 ## n_ideal ## &lt;int&gt; ## 1 21551 The following test will verify that your df_q2 is correct: ## NOTE: No need to change this! assertthat::assert_that( assertthat::are_equal( df_q2 %&gt;% pull(n_ideal), 21551 ) ) ## [1] TRUE print(&quot;Great job!&quot;) ## [1] &quot;Great job!&quot; 19.1.2 q3 The function group_by() modifies how other dplyr verbs function. Uncomment the group_by() below, and describe how the result changes. diamonds %&gt;% ## group_by(color, clarity) %&gt;% summarize(price = mean(price)) ## # A tibble: 1 × 1 ## price ## &lt;dbl&gt; ## 1 3933. The commented version computes a summary over the entire dataframe The uncommented version computes summaries over groups of color and clarity 19.1.3 Vectorized Functions Type Functions Arithmetic ops. +, -, *, /, ^ Modular arith. %/%, %% Logical comp. &lt;, &lt;=, &gt;, &gt;=, !=, == Logarithms log(x), log2(x), log10(x) Offsets lead(x), lag(x) Cumulants cumsum(x), cumprod(x), cummin(x), cummax(x), cummean(x) Ranking min_rank(x), row_number(x), dense_rank(x), percent_rank(x), cume_dist(x), ntile(x) 19.1.4 q4 Comment on why the difference is so large. The depth variable is supposedly computed via depth_computed = 100 * 2 * z / (x + y). Compute diff = depth - depth_computed: This is a measure of discrepancy between the given and computed depth. Additionally, compute the coefficient of variation cov = sd(x) / mean(x) for both depth and diff: This is a dimensionless measure of variation in a variable. Assign the resulting tibble to df_q4, and assign the appropriate values to cov_depth and cov_diff. Comment on the relative values of cov_depth and cov_diff; why is cov_diff so large? Note: Confusingly, the documentation for diamonds leaves out the factor of 100 in the computation of depth. df_q4 &lt;- diamonds %&gt;% mutate( depth_computed = 100 * 2 * z / (x + y), diff = depth - depth_computed ) %&gt;% summarize( depth_mean = mean(depth, na.rm = TRUE), depth_sd = sd(depth, na.rm = TRUE), cov_depth = depth_sd / depth_mean, diff_mean = mean(diff, na.rm = TRUE), diff_sd = sd(diff, na.rm = TRUE), cov_diff = diff_sd / diff_mean, c_diff = IQR(diff, na.rm = TRUE) / median(diff, na.rm = TRUE) ) df_q4 ## # A tibble: 1 × 7 ## depth_mean depth_sd cov_depth diff_mean diff_sd cov_diff c_diff ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 61.7 1.43 0.0232 0.00528 2.63 498. -7.50e12 Observations cov_depth is tiny; there’s not much variation in the depth, compared to its scale. cov_diff is enormous! This is because the mean difference between depth and depth_computed is small, which causes the cov to blow up. The following test will verify that your df_q4 is correct: ## NOTE: No need to change this! assertthat::assert_that(abs(df_q4 %&gt;% pull(cov_depth) - 0.02320057) &lt; 1e-3) ## [1] TRUE assertthat::assert_that(abs(df_q4 %&gt;% pull(cov_diff) - 497.5585) &lt; 1e-3) ## [1] TRUE print(&quot;Nice!&quot;) ## [1] &quot;Nice!&quot; There is nonzero difference between depth and the computed depth; this raises some questions about how depth was actually computed! It’s often a good idea to try to check derived quantities in your data, if possible. These can sometimes uncover errors in the data! 19.1.5 q5 Compute and observe Compute the price_mean = mean(price), price_sd = sd(price), and price_cov = price_sd / price_mean for each cut of diamond. What observations can you make about the various cuts? Do those observations match your expectations? df_q5 &lt;- diamonds %&gt;% group_by(cut) %&gt;% summarize( price_mean = mean(price), price_sd = sd(price), price_cov = price_sd / price_mean ) df_q5 ## # A tibble: 5 × 4 ## cut price_mean price_sd price_cov ## &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Fair 4359. 3560. 0.817 ## 2 Good 3929. 3682. 0.937 ## 3 Very Good 3982. 3936. 0.988 ## 4 Premium 4584. 4349. 0.949 ## 5 Ideal 3458. 3808. 1.10 The following test will verify that your df_q5 is correct: ## NOTE: No need to change this! assertthat::assert_that( assertthat::are_equal( df_q5 %&gt;% select(cut, price_cov) %&gt;% mutate(price_cov = round(price_cov, digits = 3)), tibble( cut = c(&quot;Fair&quot;, &quot;Good&quot;, &quot;Very Good&quot;, &quot;Premium&quot;, &quot;Ideal&quot;), price_cov = c(0.817, 0.937, 0.988, 0.949, 1.101) ) %&gt;% mutate(cut = fct_inorder(cut, ordered = TRUE)) ) ) ## [1] TRUE print(&quot;Excellent!&quot;) ## [1] &quot;Excellent!&quot; Observations: I would expect the Ideal diamonds to have the highest price, but that’s not the case! The COV for each cut is very large, on the order of 80 to 110 percent! To me, this implies that the other variables clarity, carat, color account for a large portion of the diamond price. "],["vis-histograms.html", "20 Vis: Histograms", " 20 Vis: Histograms Purpose: Histograms are a key tool for EDA. In this exercise we’ll get a little more practice constructing and interpreting histograms and densities. Reading: Histograms Topics: (All topics) Reading Time: ~20 minutes library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✔ ggplot2 3.4.0 ✔ purrr 1.0.1 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.5.0 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() 20.0.1 q1 Using the graphs generated in the chunks q1-vis1 and q1-vis2 below, answer: Which class has the most vehicles? Which class has the broadest distribution of cty values? Which graph—vis1 or vis2—best helps you answer each question? ## NOTE: No need to modify mpg %&gt;% ggplot(aes(cty, color = class)) + geom_freqpoly(bins = 10) From this graph, it’s easy to see that suv is the most numerous class ## NOTE: No need to modify mpg %&gt;% ggplot(aes(cty, color = class)) + geom_density() From this graph, it’s easy to see that subcompact has the broadest distribution In my opinion, it’s easier to see the broadness of subcompact by the density plot q1-vis2. In the previous exercise, we learned how to facet a graph. Let’s use that part of the grammar of graphics to clean up the graph above. 20.0.2 q2 Modify q1-vis2 to use a facet_wrap() on the class. “Free” the vertical axis with the scales keyword to allow for a different y scale in each facet. mpg %&gt;% ggplot(aes(cty)) + geom_density() + facet_wrap(~class, scales = &quot;free_y&quot;) In the reading, we learned that the “most important thing” to keep in mind with geom_histogram() and geom_freqpoly() is to explore different binwidths. We’ll explore this idea in the next question. 20.0.3 q3 Analyze the following graph; make sure to test different binwidths. What patterns do you see? Which patterns remain as you change the binwidth? ## TODO: Run this chunk; play with differnet bin widths diamonds %&gt;% filter(carat &lt; 1.1) %&gt;% ggplot(aes(carat)) + geom_histogram(binwidth = 0.01, boundary = 0.005) + scale_x_continuous( breaks = seq(0, 1, by = 0.1) ) Observations - The largest number of diamonds tend to fall on or above even 10-ths of a carat. - The peak near 0.5 is very broad, compared to the others. - The peak at 0.3 is most numerous "],["reproducibility-structuring-directories.html", "21 Reproducibility: Structuring Directories 21.1 Reading: Structuring a Project 21.2 Isolate your project 21.3 Structure your subdirectories 21.4 Protect your data! 21.5 Practice: Set up a project", " 21 Reproducibility: Structuring Directories Purpose: A key part of being successful in data science is getting organized. In this exercise, you’ll set yourself up for future success by learning how to structure a project in terms of its folder hierarchy: its directories. You’ll learn how to isolate your projects, structure your subdirectories, and protect your raw data. Reading: Project Structure (Skim: Provided so you can see an example of a well-structured project directory) 21.1 Reading: Structuring a Project In this exercise, we’ll assume that you are working on a project. 21.2 Isolate your project While there are many ways to organize a given project, you should definitely organize your work into individual projects. Each project should have its own directory, and should generally have all its related files under that directory. For instance, data-science-curriculum is in my mind an entire project, containing both exercises and challenges. Thus for me, it makes sense to organize all of these subdirectories under data-science-curriculum. 21.3 Structure your subdirectories Subdirectories are folders (directories!) under the main (root) project folder. For instance, the data-science-curriculum has subdirectories exercises, challenges, and more. Each subdirectory should have some purpose. For instance, there are both exercises and exercises_sequenced directories. I build the exercises in exercises out-of-sequence, because I don’t want to have to decide on which specific day each exercise should occur before I start building it. However, it’s useful for you the student to be able to see all the exercises in daily order, so I have an additional directory where I can place the sequenced exercises. Knitr will automatically create some sensible subdirectories for report files; for instance, when you knit a document knitr will automatically create a filename_files directory with image files. This helps prevent clutter in your directory, and places the file adjacent to the source file in your (sub)directory. Your project can have deeper levels of hierarchy: subdirectors under subdirectories. For instance, the exercises folder has a subdirectory exercises/data; this is where exercise datasets live. In your own projects, you should keep your data unaltered in some kind of data directory. 21.4 Protect your data! Never make alterations to your raw data! Even if your data have errors, it’s far better to document those errors somewhere, such that you have a papertrail for what changed and why. It is far better practice to keep your data unedited, and simply write processed data to an additional file. If your data needs preprocessing—say to fix errors or simply to wrangle data—write a preprocessing Rmarkdown file that takes in the raw data, and writes out a processed version of the data. You can then load the processed data in later scripts, and all of your processing steps will be permanently documented. 21.5 Practice: Set up a project Let’s set you up for future success by creating a directory for your Project 1. 21.5.1 q1 Create a project directory in your personal data science rep called p01-name. If you already know what you’re going to work on, feel free to replace name with something sensible. If you don’t know what to call it yet, don’t worry—you can change it later! 21.5.2 q2 Create a data directory under p01-name; you should then have p01-name/data. This is where you will put any data files you read or write in the project. There’s a trick to committing an empty folder with Git. We’ll need to introduce some kind of file to preserve an empty directory structure. 21.5.3 q3 Create an empty file called .gitignore under your data folder. You can do this from the root of your data science repo by using the following from your command line: # Move to your data science directory $ ~/path/to/your/data-science-work # Replace with your real directory! # Add the special .gitignore file $ touch p01-name/data/.gitignore A .gitignore file is a useful tool; it tells git what kinds of files to ignore when it comes to tracking files. We’re using it here for a different purpose; we can now commit that .gitignore in order to commit our directory structure. $ git add p01-name/data/.gitignore $ git commit -m &quot;add p01 directory structure&quot; Commit and push your directory structure. You can fill this in once you start Project 1. Key Takeaways: Isolate your project by making a folder for that specific project. Structure your project by creating directories for different filetypes: data, analysis code, and outputs. Protect your raw data somewhere, do not make irreversible edits to the raw data. "],["stats-intro-to-densities.html", "22 Stats: Intro to Densities 22.1 Densities 22.2 Drawing Samples: A Model for Randomness 22.3 Not Everything is Normal 22.4 Random Seeds: Setting the State", " 22 Stats: Intro to Densities Purpose: We will use densities to model and understand data. To that end, we’ll need to understand some key ideas about densities. Namely, we’ll discuss random variables, how we model random variables with densities, and the importance of random seeds. We’re not going to learn everything we need to work with data in this single exercise, but this will be our starting point. Reading: (None; this exercise is the reading.) library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✔ ggplot2 3.4.0 ✔ purrr 1.0.1 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.5.0 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() 22.1 Densities Fundamentally, a density is a function: It takes an input, and returns an output. A density has a special interpretation though; it represents the relative chance of particular values occurring. First, let’s look at a density of some real data: mpg %&gt;% filter(class == &quot;midsize&quot;) %&gt;% ggplot(aes(cty, color = class)) + geom_density() This density indicates that a value around 18 is more prevalent than other values of cty, at least for midsize vehicles in this dataset. Note that the plot above was generated by taking a set of cty values and smoothing them to create a density. Next, let’s look at a normal density: ## NOTE: No need to change this! df_norm_d &lt;- tibble(z = seq(-5, +5, length.out = 100)) %&gt;% mutate(d = dnorm(z, mean = 0, sd = 1)) df_norm_d %&gt;% ggplot(aes(z, d)) + geom_line() + labs( y = &quot;density&quot; ) This plot of a normal density shows that the value z = 0 is most likely, while values both larger and smaller are less likely to occur. Note that the the density above was generated by dnorm(z, mean = 0, sd = 1). The values [mean, sd] are called parameters. Rather than requiring an entire dataset to define this density, the normal is fully defined by specifying values for mean and sd. However, note that the normal density is quite a bit simpler than the density of cty values above. This is a general trend; “named” densities like the normal are simplified models we use to represent real data. Picking an appropriate model is part of the challenge in statistics. R comes with a great many densities implemented: Density Code Normal norm Uniform unif Chi-squared chisq … … In R we access a density by prefixing its name with the letter d. 22.1.1 q1 Modify the code below to evaluate a uniform density with min = -1 and max = +1. Assign this value to d. df_q1 &lt;- tibble(x = seq(-5, +5, length.out = 100)) %&gt;% mutate(d = dunif(x, min = -1, max = +1)) Use the following test to check your answer. ## NOTE: No need to change this assertthat::assert_that( all( df_q1 %&gt;% filter(-1 &lt;= x, x &lt;= +1) %&gt;% mutate(flag = d &gt; 0) %&gt;% pull(flag) ) ) ## [1] TRUE assertthat::assert_that( all( df_q1 %&gt;% filter((x &lt; -1) | (+1 &lt; x)) %&gt;% mutate(flag = d == 0) %&gt;% pull(flag) ) ) ## [1] TRUE print(&quot;Nice!&quot;) ## [1] &quot;Nice!&quot; 22.2 Drawing Samples: A Model for Randomness What does it practically mean for a value to be more or less likely? A density gives information about a random variable. A random variable is a quantity that takes a random value every time we observe it. For instance: If we roll a six-sided die, it will (unpredictably) land on values 1 through 6. In my research, I use random variables to model material properties: Every component that comes off an assembly line will have a different strength associated with it. In order to design for a desired rate of failure, I model this variability with a random variable. A single value from a random variable—a roll of the die or a single value of material strength—is called a realization. Let’s illustrate this idea by using a normal density to define a random variable Z. This is sometimes denoted Z ~ norm(mean, sd), which means Z is density as a normal density with mean and sd. The code below illustrates what it looks like when we draw a few samples of Z. ## NOTE: No need to change this! df_norm_samp &lt;- tibble(Z = rnorm(n = 100, mean = 0, sd = 1)) ggplot() + ## Plot a *sample* geom_histogram( data = df_norm_samp, mapping = aes(Z, after_stat(density)), bins = 20 ) + ## Plot a *density* geom_line( data = df_norm_d, mapping = aes(z, d) ) Note that the histogram of Z values looks like a “blocky” version of the density. If we draw many more samples, the histogram will start to strongly resemble the density from which it was drawn: ## NOTE: No need to change this! ggplot() + ## Plot a *sample* geom_histogram( data = tibble(Z = rnorm(n = 1e5, mean = 0, sd = 1)), mapping = aes(Z, after_stat(density)), bins = 40 ) + ## Plot a *density* geom_line( data = df_norm_d, mapping = aes(z, d) ) To clarify some terms: A realization is a single observed value of a random variable. A realization is a single value, e.g. rnorm(n = 1). A sample is a set of realizations. A sample is a set of values, e.g. rnorm(n = 10). A density is a function that describes the relative chance of all the values a random variable might take. A density is a function, e.g. dnorm(x). 22.2.1 q2 Plot densities for Z values in df_q2. Use aesthetics to plot a separate curve for different values of n. Comment on which values of n give a density closer to “normal”. Hint: You may need to use the group aesthetic, in addition to color or fill. ## NOTE: Use the data generated here df_q2 &lt;- map_dfr( c(5, 10, 1e3), function(nsamp) { tibble(Z = rnorm(n = nsamp), n = nsamp) } ) ## TODO: Create the figure df_q2 %&gt;% ggplot(aes(Z, color = n, group = n)) + geom_density() Observations - The larger the value of n, the closer the density tends to be to normal. 22.3 Not Everything is Normal A very common misconception is that drawing more samples will make anything look like a normal density. This is not the case! The following exercise will help us understand that the normal density is just a model, and not what we should expect all randomness to look like. 22.3.1 q3 Run the code below and inspect the resulting histogram. Increase n_diamonds below to increase the number of samples drawn: Do this a few times for an increasing number of samples. Answer the questions under observations below. n_diamonds &lt;- 1000 diamonds %&gt;% filter(cut == &quot;Good&quot;) %&gt;% slice_sample(n = n_diamonds) %&gt;% ggplot(aes(carat)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Observations: The histogram does not look normal for any n_diamonds, and indeed it should not. We’ve seen before that there are very “special” values of carat that gemcutters seem to favor. That “social” feature of diamonds is not at all captured by a normal density. Carat is a measure of weight and weight cannot be negative. The normal density is a good model for some physical phenomena, such as the set of heights among a population. However, it is not always a good model. One reason the normal density “shows up” a lot is because it is involved in estimation, which we’ll talk more about when we later discuss the central limit theorem. 22.4 Random Seeds: Setting the State Remember that random variables are random. That means if we draw samples multiple times, we’re bound to get different values. For example, run the following code chunk multiple times. ## NOTE: No need to change; run this multiple times! rnorm(5) ## [1] -1.6667241 0.3382278 -1.2646835 -0.4150887 0.7439647 What this means is we’ll get a slightly different picture every time we draw a sample. This is the challenge with randomness: Since we could have drawn a different set of samples, we need to know the degree to which we can trust conclusions drawn from data. Being statistically literate means knowing how much to trust your data. ## NOTE: No need to change this! df_samp_multi &lt;- tibble(Z = rnorm(n = 50, mean = 0, sd = 1), Run = &quot;A&quot;) %&gt;% bind_rows( tibble(Z = rnorm(n = 50, mean = 0, sd = 1), Run = &quot;B&quot;) ) %&gt;% bind_rows( tibble(Z = rnorm(n = 50, mean = 0, sd = 1), Run = &quot;C&quot;) ) ggplot() + ## Plot *fitted density* geom_density( data = df_samp_multi, mapping = aes(Z, color = Run, group = Run) ) + ## Plot a *density* geom_line( data = df_norm_d, mapping = aes(z, d) ) However for simulation purposes, we often want to be able to repeat a calculation. In order to do this, we can set the state of our random number generator by setting the seed. To illustrate, try running the following chunk multiple times. ## NOTE: No need to change; run this multiple times! set.seed(101) rnorm(5) ## [1] -0.3260365 0.5524619 -0.6749438 0.2143595 0.3107692 The values are identical every time! "],["data-pivoting-data.html", "23 Data: Pivoting Data 23.1 Tidy Data 23.2 Pivoting: Examples 23.3 Pivoting: Exercises", " 23 Data: Pivoting Data Purpose: Data is easiest to use when it is tidy. In fact, the tidyverse (including ggplot, dplyr, etc.) is specifically designed to use tidy data. But not all data we’ll encounter is tidy! To that end, in this exercise we’ll learn how to tidy our data by pivoting. As a result of learning how to quickly tidy data, you’ll vastly expand the set of datasets you can analyze. Rather than fighting with data, you’ll be able to quickly wrangle and extract insights. Reading: Reshape Data Topics: Welcome, Tidy Data (skip Gathering and Spreading columns) Reading Time: ~10 minutes (this exercise contains more reading material) Optional readings: - selection language Note: Unfortunately, the RStudio primers have not been updated to use the most up-to-date dplyr tools. Rather than learning the out-of-date tools gather, spread, we will instead learn how to use pivot_longer and pivot_wider. library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✔ ggplot2 3.4.0 ✔ purrr 1.0.1 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.5.0 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() 23.1 Tidy Data Tidy data is a form of data where: Each variable is in its own column Each observation is in its own row Each value is in its own cell Not all data are presented in tidy form; in this case it can be difficult to tell what the variables are. Let’s practice distinguishing between the columns and the variables. 23.1.1 q1 What are the variables in the following dataset? ## NOTE: No need to edit; execute cases &lt;- tribble( ~Country, ~`2011`, ~`2012`, ~`2013`, &quot;FR&quot;, 7000, 6900, 7000, &quot;DE&quot;, 5800, 6000, 6200, &quot;US&quot;, 15000, 14000, 13000 ) cases ## # A tibble: 3 × 4 ## Country `2011` `2012` `2013` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 FR 7000 6900 7000 ## 2 DE 5800 6000 6200 ## 3 US 15000 14000 13000 Country, 2011, 2012, and 2013 Country, year, and some unknown quantity (n, count, etc.) FR, DE, and US ## TODO: Modify with your multiple choice number answer q1_answer &lt;- 0 ## NOTE: The following will test your answer if (((q1_answer + 56) %% 3 == 1) &amp; (q1_answer &gt; 0)) { &quot;Correct!&quot; } else { &quot;Incorrect!&quot; } ## [1] &quot;Incorrect!&quot; 23.1.2 q2 What are the variables in the following dataset? ## NOTE: No need to edit; execute alloys &lt;- tribble( ~thick, ~E_00, ~mu_00, ~E_45, ~mu_45, ~rep, 0.022, 10600, 0.321, 10700, 0.329, 1, 0.022, 10600, 0.323, 10500, 0.331, 2, 0.032, 10400, 0.329, 10400, 0.318, 1, 0.032, 10300, 0.319, 10500, 0.326, 2 ) alloys ## # A tibble: 4 × 6 ## thick E_00 mu_00 E_45 mu_45 rep ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.022 10600 0.321 10700 0.329 1 ## 2 0.022 10600 0.323 10500 0.331 2 ## 3 0.032 10400 0.329 10400 0.318 1 ## 4 0.032 10300 0.319 10500 0.326 2 thick, E_00, mu_00, E_45, mu_45, rep thick, E, mu, rep thick, E, mu, rep, angle ## TODO: Modify with your multiple choice number answer q2_answer &lt;- 0 ## NOTE: The following will test your answer if (((q2_answer + 38) %% 3 == 2) &amp; (q2_answer &gt; 0)) { &quot;Correct!&quot; } else { &quot;Incorrect!&quot; } ## [1] &quot;Incorrect!&quot; 23.2 Pivoting: Examples The dplyr package comes with tools to pivot our data into tidy form. There are two key tools: pivot_longer and pivot_wider. The names are suggestive of their use. When our data are too wide we should pivot_longer, and when our data are too long, we should pivot_wider. 23.2.1 Pivot longer First, let’s see how pivot_longer works on the cases data. Run the following code chunk: ## NOTE: No need to edit; execute cases %&gt;% pivot_longer( names_to = &quot;Year&quot;, values_to = &quot;n&quot;, cols = c(`2011`, `2012`, `2013`) ) ## # A tibble: 9 × 3 ## Country Year n ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 FR 2011 7000 ## 2 FR 2012 6900 ## 3 FR 2013 7000 ## 4 DE 2011 5800 ## 5 DE 2012 6000 ## 6 DE 2013 6200 ## 7 US 2011 15000 ## 8 US 2012 14000 ## 9 US 2013 13000 Now these data are tidy! The variable Year is now the name of a column, and its values appear in the cells. Let’s break down the key inputs to pivot_longer: names_to is what we’re going to call the new column whose values will be the original column names values_to is what we’re going to call the new column that will hold the values associated with the original columns cols is the set of columns in the original dataset that we’re going to modify. This takes the same inputs as a call to select, so we can use functions like starts_with, ends_with, contains, etc., or a list of column names enclosed with c() Note that in our case, we had to enclose each column name with ticks so that dplyr does not interpret the integer values as column positions (rather than column names) For more details on selecting variables, see the selection language page However, there’s a problem with the Year column: ## NOTE: No need to edit; execute cases %&gt;% pivot_longer( names_to = &quot;Year&quot;, values_to = &quot;n&quot;, c(`2011`, `2012`, `2013`) ) %&gt;% summarize(Year = mean(Year)) ## Warning in mean.default(Year): argument is not numeric or logical: returning NA ## # A tibble: 1 × 1 ## Year ## &lt;dbl&gt; ## 1 NA The summary failed! That’s because the Year column is full of strings, rather than integers. We can fix this via mutation: ## NOTE: No need to edit; execute cases %&gt;% pivot_longer( names_to = &quot;Year&quot;, values_to = &quot;n&quot;, c(`2011`, `2012`, `2013`) ) %&gt;% mutate(Year = as.integer(Year)) ## # A tibble: 9 × 3 ## Country Year n ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 FR 2011 7000 ## 2 FR 2012 6900 ## 3 FR 2013 7000 ## 4 DE 2011 5800 ## 5 DE 2012 6000 ## 6 DE 2013 6200 ## 7 US 2011 15000 ## 8 US 2012 14000 ## 9 US 2013 13000 Now the data are tidy and of the proper type. Let’s look at a built-in dataset: ## NOTE: No need to edit; execute ansc &lt;- tribble( ~`x-1`, ~`x-2`, ~`y-1`, ~`y-2`, 10, 10, 8.04, 9.14, 8, 8, 6.95, 8.14, 13, 13, 7.58, 8.74, 9, 9, 8.81, 8.77, 11, 11, 8.33, 9.26, 14, 14, 9.96, 8.10, 6, 6, 7.24, 6.13, 4, 4, 4.26, 3.10, 12, 12, 10.84, 9.13, 7, 7, 4.82, 7.26, 5, 5, 5.68, 4.74 ) ansc ## # A tibble: 11 × 4 ## `x-1` `x-2` `y-1` `y-2` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 10 10 8.04 9.14 ## 2 8 8 6.95 8.14 ## 3 13 13 7.58 8.74 ## 4 9 9 8.81 8.77 ## 5 11 11 8.33 9.26 ## 6 14 14 9.96 8.1 ## 7 6 6 7.24 6.13 ## 8 4 4 4.26 3.1 ## 9 12 12 10.8 9.13 ## 10 7 7 4.82 7.26 ## 11 5 5 5.68 4.74 This dataset is too wide; the digit after each x or y denotes a different dataset. The case is tricky to pivot though: We need to separate the trailing digits while preserving the x, y column names. We can use the special “.value” entry in names_to in order to handle this: ## NOTE: No need to edit; execute ansc %&gt;% pivot_longer( names_to = c(&quot;.value&quot;, &quot;set&quot;), names_sep = &quot;-&quot;, cols = everything() ) ## # A tibble: 22 × 3 ## set x y ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 10 8.04 ## 2 2 10 9.14 ## 3 1 8 6.95 ## 4 2 8 8.14 ## 5 1 13 7.58 ## 6 2 13 8.74 ## 7 1 9 8.81 ## 8 2 9 8.77 ## 9 1 11 8.33 ## 10 2 11 9.26 ## # … with 12 more rows Note that: - With .value in names_to, we do not provide the values_to column names. We are instead signaling that the value names come from the column names - everything() is a convenient way to select all columns Let’s look at one more use of pivot_longer on the alloys dataset. ## NOTE: No need to edit; execute alloys %&gt;% pivot_longer( names_to = c(&quot;var&quot;, &quot;angle&quot;), names_sep = &quot;_&quot;, values_to = &quot;val&quot;, cols = c(-thick, -rep) ) ## # A tibble: 16 × 5 ## thick rep var angle val ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 0.022 1 E 00 10600 ## 2 0.022 1 mu 00 0.321 ## 3 0.022 1 E 45 10700 ## 4 0.022 1 mu 45 0.329 ## 5 0.022 2 E 00 10600 ## 6 0.022 2 mu 00 0.323 ## 7 0.022 2 E 45 10500 ## 8 0.022 2 mu 45 0.331 ## 9 0.032 1 E 00 10400 ## 10 0.032 1 mu 00 0.329 ## 11 0.032 1 E 45 10400 ## 12 0.032 1 mu 45 0.318 ## 13 0.032 2 E 00 10300 ## 14 0.032 2 mu 00 0.319 ## 15 0.032 2 E 45 10500 ## 16 0.032 2 mu 45 0.326 Note a few differences from the call of pivot_longer on the cases data: here names_to contains two names; this is to deal with the two components of the merged column names E_00, mu_00, E_45, etc. names_sep allows us to specify a character that separates the components of the merged column names. In our case, the column names are merged with an underscore _ We use the -column syntax with cols to signal that we don’t want the specified columns. This allows us to exclude thick, rep As an alternative, we could have used the more verbose cols = starts_with(\"E\") | starts_with(\"mu\"), which means “starts with”E” OR starts with “mu”” This looks closer to tidy—we’ve taken care of the merged column names—but now we have a different problem: The variables E, mu are now in cells, rather than column names! This is an example of a dataset that is too long. For this, we’ll need to use pivot_wider. 23.2.2 Pivot wider We’ll continue tidying the alloys dataset with pivot_wider. ## NOTE: No need to edit; execute alloys %&gt;% pivot_longer( names_to = c(&quot;var&quot;, &quot;angle&quot;), names_sep = &quot;_&quot;, values_to = &quot;val&quot;, starts_with(&quot;E&quot;) | starts_with(&quot;mu&quot;) ) %&gt;% pivot_wider( names_from = var, # Cell entries to turn into new column names values_from = val # Values to associate with the new column(s) ) ## # A tibble: 8 × 5 ## thick rep angle E mu ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.022 1 00 10600 0.321 ## 2 0.022 1 45 10700 0.329 ## 3 0.022 2 00 10600 0.323 ## 4 0.022 2 45 10500 0.331 ## 5 0.032 1 00 10400 0.329 ## 6 0.032 1 45 10400 0.318 ## 7 0.032 2 00 10300 0.319 ## 8 0.032 2 45 10500 0.326 Note the differences between pivot_longer and pivot_wider: Rather than names_to, we specify names_from; this takes a tidyselect specification. We specify the column(s) of values to turn into new column names Rather than values_to, we specify values_from; this takes a tidyselect specification. We specify the column(s) of values to turn into new values What we just saw above is a general strategy: If you see merged column names, you can: First, pivot_longer with names_sep or names_pattern to unmerge the column names. Next, pivot_wider to tidy the data. Both pivot_longer and pivot_wider have a lot of features; see their documentation for more info. 23.3 Pivoting: Exercises To practice using pivot_longer and pivot_wider, we’re going to work with the following small dataset: ## NOTE: No need to edit; this is setup for the exercises df_base &lt;- tribble( ~`X-0`, ~`X-1`, ~key, 1, 9, &quot;A&quot;, 2, 8, &quot;B&quot;, 3, 7, &quot;C&quot; ) We’re going to play a game: I’m going to modify the data, and your job is to pivot it back to equal df_base. 23.3.1 q3 Recover df_base from df_q3 by using a single pivot and no other functions. ## NOTE: No need to edit; this is setup for the exercise df_q3 &lt;- df_base %&gt;% pivot_longer( names_to = &quot;id&quot;, names_pattern = &quot;(\\\\d)&quot;, names_transform = list(id = as.integer), values_to = &quot;value&quot;, cols = -key ) df_q3 ## # A tibble: 6 × 3 ## key id value ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 A 0 1 ## 2 A 1 9 ## 3 B 0 2 ## 4 B 1 8 ## 5 C 0 3 ## 6 C 1 7 Undo the modification using a single pivot. Don’t worry about column order. df_q3_res &lt;- df_q3 %&gt;% pivot_wider( names_from = id, names_prefix = &quot;X-&quot;, values_from = value ) df_q3_res ## # A tibble: 3 × 3 ## key `X-0` `X-1` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A 1 9 ## 2 B 2 8 ## 3 C 3 7 all_equal(df_base, df_q3_res) # Checks equality; returns TRUE if equal ## New names: ## New names: ## • `X-0` -&gt; `X.0` ## • `X-1` -&gt; `X.1` ## [1] TRUE 23.3.2 q4 Recover df_base from df_q4 by using a single pivot and no other functions. ## NOTE: No need to edit; this is setup for the exercise df_q4 &lt;- df_base %&gt;% pivot_wider( names_from = key, values_from = `X-0` ) df_q4 ## # A tibble: 3 × 4 ## `X-1` A B C ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 9 1 NA NA ## 2 8 NA 2 NA ## 3 7 NA NA 3 Undo the modification using a single pivot. Don’t worry about column order. Hint: You’ll need a way to drop NA values in the pivot (without filtering). Check the documentation for pivot_longer. df_q4_res &lt;- df_q4 %&gt;% pivot_longer( names_to = &quot;key&quot;, values_to = &quot;X-0&quot;, values_drop_na = TRUE, cols = c(A, B, C) ) df_q4_res ## # A tibble: 3 × 3 ## `X-1` key `X-0` ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 9 A 1 ## 2 8 B 2 ## 3 7 C 3 all_equal(df_base, df_q4_res) # Checks equality; returns TRUE if equal ## New names: ## New names: ## • `X-0` -&gt; `X.0` ## • `X-1` -&gt; `X.1` ## [1] TRUE 23.3.3 q5 Recover df_base from df_q5 by using a single pivot and no other functions. ## NOTE: No need to edit; this is setup for the exercise df_q5 &lt;- df_base %&gt;% pivot_wider( names_from = key, values_from = -key ) df_q5 ## # A tibble: 1 × 6 ## `X-0_A` `X-0_B` `X-0_C` `X-1_A` `X-1_B` `X-1_C` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 2 3 9 8 7 Undo the modification using a single pivot. Don’t worry about column order. Hint: For this one, you’ll need to use the special .value entry in names_to. df_q5_res &lt;- df_q5 %&gt;% pivot_longer( names_to = c(&quot;.value&quot;, &quot;key&quot;), names_sep = &quot;_&quot;, cols = everything() ) df_q5_res ## # A tibble: 3 × 3 ## key `X-0` `X-1` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A 1 9 ## 2 B 2 8 ## 3 C 3 7 all_equal(df_base, df_q5_res) # Checks equality; returns TRUE if equal ## New names: ## New names: ## • `X-0` -&gt; `X.0` ## • `X-1` -&gt; `X.1` ## [1] TRUE 23.3.4 q6 Make your own! Using a single pivot on df_base create your own challenge dataframe. You will share this with the rest of the class as a puzzle, so make sure to solve your own challenge so you have a solution! ## NOTE: No need to edit; this is setup for the exercise df_q6 &lt;- df_base %&gt;% glimpse() ## Rows: 3 ## Columns: 3 ## $ `X-0` &lt;dbl&gt; 1, 2, 3 ## $ `X-1` &lt;dbl&gt; 9, 8, 7 ## $ key &lt;chr&gt; &quot;A&quot;, &quot;B&quot;, &quot;C&quot; df_q6 ## # A tibble: 3 × 3 ## `X-0` `X-1` key ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 9 A ## 2 2 8 B ## 3 3 7 C Don’t forget to create a solution! df_q6_res &lt;- df_q6 %&gt;% glimpse() ## Rows: 3 ## Columns: 3 ## $ `X-0` &lt;dbl&gt; 1, 2, 3 ## $ `X-1` &lt;dbl&gt; 9, 8, 7 ## $ key &lt;chr&gt; &quot;A&quot;, &quot;B&quot;, &quot;C&quot; df_q6_res ## # A tibble: 3 × 3 ## `X-0` `X-1` key ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 9 A ## 2 2 8 B ## 3 3 7 C all_equal(df_base, df_q6_res) # Checks equality; returns TRUE if equal ## New names: ## New names: ## • `X-0` -&gt; `X.0` ## • `X-1` -&gt; `X.1` ## [1] TRUE "],["reproducibility-collaborating-with-github.html", "24 Reproducibility: Collaborating with GitHub 24.1 Overview 24.2 Partner Team 24.3 Forking Partner 24.4 Merging Partner 24.5 Forking Partner", " 24 Reproducibility: Collaborating with GitHub Purpose: Git and GitHub are extremely useful for collaboration, but they take some getting used to. In this partner exercise, you will learn how to contribute to a partner’s work, and how to merge contributions to your own work. This will be your first taste of using Git not just as a way to track changes, but also as a powerful collaboration tool. Reading: GitHub Forking (Optional / Reference) 24.1 Overview At a high level, what you and your partner are going to do is create a copy of each others’ public repos, make a change, and request that your partner merge those changes into their own repo. Using this workflow, you and your partner will be able to collaborate asynchronously on the same documents, all while maintaining safe version control of your work. In this case you’ll make trivial edits to the TODO items below, but in practice you can use an identical process to edit things like software and reports. 24.2 Partner Team (You and your partner will edit the TODO items separately and combine, through the magic of git!) Title Person Forking Partner TODO Merging Partner TODO Note: All instructions below are coded for either the Forking partner, the Merging partner, or Both. Note that you will both do all of these steps, but you will take turns being Forking and Merging. Pay attention to when these flags change—that indicates where you and your partner need to take different actions. 24.2.1 q1 (Both) Pick a partner in the class, preferably from your learning team. You must mutually agree to complete this exercise together, which will involve forking your partner’s public repo and making a pull request. 24.3 Forking Partner 24.3.1 q2 (Forking) Navigate to your partner’s GitHub repo page, and click the Fork button on the top-right of the page. Fork button on a repo, near the top-right of the page Depending on how much you’ve used GitHub already, you may get an additional menu once you click Fork. For instance, I’m part of some other organizations, and have the option to fork under those other accounts. Make sure to fork under your own account. Example prompt with multiple organizations Wait for the forking process to finish…. Wait What you’ve just done is create a copy of your partner’s repository; you can now edit this copy as your own. This is called a fork of a repository. Clone your forked repository to your machine to create a local version that you can edit. Remember that you click the “copy to clipboard” button under Code: Clone Then use the command below to clone your fork: # You should be able to paste the git@github.com part you copied above $ git clone git@github.com:YOUR-USERNAME/YOUR-FORKED-REPO $ cd YOUR-FORKED-REPO 24.3.2 q3 (Forking) Add your partner’s repository as a remote. The reading described how to do this from the command line; use the command within your forked repository $ git remote add upstream https://github.com/UPSTREAM-USER/ORIGINAL-PROJECT.git where you should replace UPSTREAM-USER with your partner’s GitHub username, and ORIGINAL-PROJECT with the name of your partner’s GitHub public repo (the one you forked). What this does is allow you to pull any changes your partner makes in their own repo using the command git fetch upstream. We won’t use that command in this exercise, but it’s good to be aware of. 24.3.3 q5 (Forking) Inspect your partner’s repository (your forked-and-cloned version); if they don’t have an exercises directory, create a new one. Within that exercise directory either open their existing e-rep-05-collab-assignment.Rmd, or copy this file into the exercises directory. Edit the TODO above under Forking Partner to be your own name. 24.3.4 q6 (Forking) Commit your changes, and push them to your forked repository. If you do $ git push, you will probably see something like: fatal: The current branch dev_addname has no upstream branch. To push the current branch and set the remote as upstream, use git push --set-upstream origin master If you see this, it’s because Git is not willing to make assumptions about where you want to push. The --set-upstream origin master tells Git to push to origin (which should be set to your forked repository if you cloned it in q2 above), while master tells Git to push the changes to the master branch. Follow the instructions by running $ git push --set-upstream origin master This will push your commit(s) to your forked repo. Now you have the changes in your personal copy of your partner’s work. Our task is now to request that our partner incorporate these changes in their own repository. 24.3.5 q7 (Forking) Navigate your browser to your forked repository on GitHub. You should see a banner like the following: Pull request Click the Pull request to start the process of making a pull request (PR). Add a comment justifying the PR, and click Create pull request. Your partner will now be able to see your PR. Your merging partner will finish the rest of the exercise (though you should still complete the merging part on your own repo!). 24.4 Merging Partner q8 (Merging) Open your public repo GitHub page; you should see that your partner has an open pull request.You can see the list of pull requests by clicking on the Pull requests tab towards the top of your repo. List pull requests Note: You’ll probably also get an automated email from GitHub once your partner opens a PR. Your task is to merge their pull request. Click on the merge request your partner opened; for instance mine was described as Adding exercises dir, rep05.Rmd by mdelrosa. Select PR Hopefully, you will see the message “This branch has no conflicts with the base branch. Merging can be performed automatically.” This means that the edits your partner made don’t conflict with any changes you made. Thus Git can automagically combine all the edits. Click Merge pull request and Confirm merge to proceed. Note: If you don’t see “Merging can be performed automatically,” please reach out to one of the teaching team. We’ll gladly help you out! Merge PR Once done, the page should change to let you know that you completed the merge, as shown below: Merged PR q8 (Merging) Now you will pull the most recent changes to your local copy of your repo. before pulling. Navigate to the directory of your repo. Make sure to commit any stray changes (e.g. in-progress exercises!). Then run git pull. $ cd /path/to/your/repo $ git pull This will pull down the latest changes that your partner made. 24.4.1 q9 (Merging) Replace TODO above under Merging partner to complete the exercise. Commit and push these changes to your repo. 24.5 Forking Partner 24.5.1 q10 (Forking) Now your partner has successfully merged your edits, but in order to keep collaborating, you’ll need to update your fork based on your partner’s current (and future!) edits. Use the following command to fetch your partner’s edits. $ git fetch upstream Note that this hasn’t actually changed your local repo; to see this check out your branches. $ git branch -va Note that I have my own master branch, and also have my partner’s upstream/master that has the merged pull request from q8. Branches for forked repo In order to keep your forked copy of your partner’s repo current, run the following commands. # Make sure you&#39;re on the master branch $ git checkout master # merge your master with your partner&#39;s upstream/master $ git merge upstream/master If you do this after your partner has changed and pushed the assignment in q9, you should see both your and your partner’s name in the assignment document. Aside: That was a lot of work just to get two names in one document. But the advantage in this process is that the document was rigorously controlled through every stage of this process, and you now have a fully reversible and searchable history of the document through all the changes. Git takes a lot of work to learn, but is extraordinarly powerful for collaboration. "],["vis-boxplots-and-counts.html", "25 Vis: Boxplots and Counts", " 25 Vis: Boxplots and Counts Purpose: Boxplots are a key tool for EDA. Like histograms, boxplots give us a sense of “shape” for a distribution. However, a boxplot is a careful summary of shape. This helps us pick out key features of a distribution, and enables easier comparison of different distributions. Reading: Boxplots and Counts Topics: (All topics) Reading Time: ~20 minutes library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✔ ggplot2 3.4.0 ✔ purrr 1.0.1 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.5.0 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() A subtle point from the primer is that we can use dplyr to generate new facts about our data, then use ggplot2 to visualize those facts. We’ll practice this idea in a number of ways. 25.0.1 q1 Use a cut_* verb to create a categorical variable out of carat. Tweak the settings in your cut and document your observations. Hint: Recall that we learned about cut_interval, cut_number, cut_width. Take your pick! diamonds %&gt;% mutate(carat_cut = cut_width(carat, width = 0.5, boundary = 0)) %&gt;% ggplot(aes(x = carat_cut, y = price)) + geom_boxplot() Observations - Price tends to increase with carat - Median price rises dramatically across carat [0, 2] - Median price is roughly constant across carat [2, 4.5] - Across carat [2, 4.5], the whiskers have essentially the same max price - The IQR is quite small at low carat, but increases with carat; the prices become more variable at higher carat 25.0.2 q2 The following code visualizes the count of diamonds of all carats according to their cut and color. Modify the code to consider only diamonds with carat &gt;= 2. Does the most common group of cut and color change? ## NOTE: No need to modify; run and inspect diamonds %&gt;% count(cut, color) %&gt;% ggplot(aes(cut, color, fill = n)) + geom_tile() Modify the following code: diamonds %&gt;% filter(carat &gt;= 2) %&gt;% count(cut, color) %&gt;% ggplot(aes(cut, color, fill = n)) + geom_tile() Observations: - Yes, it used to be G, Ideal, but is now I, Premium 25.0.3 q3 The following plot has overlapping x-axis labels. Use a verb from the reading to flip the coordinates and improve readability. mpg %&gt;% ggplot(aes(manufacturer, hwy)) + geom_boxplot() + coord_flip() This is a simple—but important—trick to remember when visualizing data with many categories. "],["stats-probability.html", "26 Stats: Probability 26.1 Intuitive Definition 26.2 Relation to Distributions 26.3 Sets vs Points 26.4 Two expressions of the same information 26.5 Notes", " 26 Stats: Probability Purpose: Probability is a quantitative measure of uncertainty. It is intimately tied to how we use distributions to model data and to how we express uncertainty. In order to do all these useful things, we’ll need to learn some basics about probability. Reading: (None; this exercise is the reading.) Topics: Frequency, probability, probability density function (PDF), cumulative distribution function (CDF) “Probability is the most important concept in modern science, especially as nobody has the slightest notion what it means.” — Bertrand Russell library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✔ ggplot2 3.4.0 ✔ purrr 1.0.1 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.5.0 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() 26.1 Intuitive Definition In the previous stats exercise, we learned about densities. In this exercise, we’re going to learn a more formal definition using probability. To introduce the idea of probability, let’s first think about frequency. Imagine we have some set of events \\(X\\), and we’re considering some particular subset of cases that we’re interested in \\(A\\). For instance, imagine we’re rolling a 6-sided die, and we’re interested in cases when the number rolled is even. Then the subset of cases is \\(A = \\{2, 4, 6\\}\\), and an example run of rolls might be \\(X = \\{3, 5, 5, 2, 6, 1, 3, 3\\}\\). The frequency with which events in \\(A\\) occurred for a run \\(X\\) is \\[F_X(A) \\equiv \\sum \\frac{\\text{Cases in set }A}{\\text{Total Cases in }X}.\\] For the example above, we have \\[\\begin{aligned} A &amp;= \\{2, 4, 6\\}, \\\\ X &amp;= \\{3, 5, 5, \\mathbf{2}, \\mathbf{6}, 1, 3, 3\\}, \\\\ F_X(A) &amp;= \\frac{2}{8} = 1/4 \\end{aligned}\\] Note that this definition of frequency considers both a set \\(A\\) and a sample \\(X\\). We need to define both \\(A, X\\) in order to compute a frequency. As an example, let’s consider the set \\(A\\) to be the set of \\(Z\\) values such that \\(-1.96 &lt;= Z &lt;= +1.96\\): We denote this set as \\(A = {Z | -1.96 &lt;= Z &lt;= +1.96}\\). Let’s also let \\(Z\\) be a sample from a standard (mean = 0, sd = 1) normal. The following figure illustrates the set \\(A\\) against a standard normal density. ## NOTE: No need to change this! tibble(z = seq(-3, +3, length.out = 500)) %&gt;% mutate(d = dnorm(z)) %&gt;% ggplot(aes(z, d)) + geom_ribbon( data = . %&gt;% filter(-1.96 &lt;= z, z &lt;= +1.96), aes(ymin = 0, ymax = d, fill = &quot;Set A&quot;), alpha = 1 / 3 ) + geom_line() + scale_fill_discrete(name = &quot;&quot;) Note that a frequency is defined not in terms of a density, but rather in terms of a sample \\(X\\). The following example code draws a sample from a standard normal, and computes the frequency with which values in the sample \\(X\\) lie in the set \\(A\\). ## NOTE: No need to change this! df_z &lt;- tibble(z = rnorm(100)) df_z %&gt;% mutate(in_A = (-1.96 &lt;= z) &amp; (z &lt;= +1.96)) %&gt;% summarize(count_total = n(), count_A = sum(in_A), fr = mean(in_A)) ## # A tibble: 1 × 3 ## count_total count_A fr ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 100 92 0.92 Now it’s your turn! 26.1.1 q1 Let \\(A = {Z | Z &lt;= 0}\\). Complete the following code to compute count_total, count_A, and fr. Before executing the code, make a prediction about the value of fr. Did the computed fr value match your prediction? ## NOTE: No need to change this! df_z &lt;- tibble(z = rnorm(100)) df_z %&gt;% mutate(in_A = (z &lt;= 0)) %&gt;% summarize(count_total = n(), count_A = sum(in_A), fr = mean(in_A)) ## # A tibble: 1 × 3 ## count_total count_A fr ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 100 44 0.44 Observations: I predicted fr = 0.5 The value of fr I computed was 0.52, not quite the same. This is due to randomness in the calculation. The following graph visualizes the set \\(A = {z | z &lt;= 0}\\) against a standard normal density. ## NOTE: No need to change this! tibble(z = seq(-3, +3, length.out = 500)) %&gt;% mutate(d = dnorm(z)) %&gt;% ggplot(aes(z, d)) + geom_ribbon( data = . %&gt;% filter(z &lt;= 0), aes(ymin = 0, ymax = d, fill = &quot;Set A&quot;), alpha = 1 / 3 ) + geom_line() + scale_fill_discrete(name = &quot;&quot;) Based on this visual, we might expect fr = 0.5. This was (likely) not the value that our frequency took, but it is the precise value of the probability that \\(Z &lt;= 0.5\\). Remember in the previous stats exercise that when running rnorm with larger values of n we obtained histograms closer to the normal density? Something very similar happens with frequency and probability: ## NOTE: No need to change this! map_dfr( c(10, 100, 1000, 1e4), function(n_samples) { tibble( z = rnorm(n = n_samples), n = n_samples ) %&gt;% mutate(in_A = (z &lt;= 0)) %&gt;% summarize(count_total = n(), count_A = sum(in_A), fr = mean(in_A)) } ) ## # A tibble: 4 × 3 ## count_total count_A fr ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 10 7 0.7 ## 2 100 56 0.56 ## 3 1000 519 0.519 ## 4 10000 4986 0.499 This is because probability is actually defined[1] in terms of the limit \\[\\mathbb{P}_{\\rho}[X \\in A] = \\lim_{n \\to \\infty} F_{X_n}(A),\\] where \\(X_n\\) is a sample of size \\(n\\) drawn from the density \\(X_n \\sim \\rho\\).[2] 26.1.2 q2: Modify the code below to consider the set \\(A = {z | -1.96 &lt;= z &lt;= +1.96}\\). What value does fr appear to be limiting towards? ## TASK: Modify the code below map_dfr( c(10, 100, 1000, 1e4), function(n_samples) { tibble( z = rnorm(n = n_samples), n = n_samples ) %&gt;% mutate(in_A = (-1.96 &lt;= z) &amp; (z &lt;= +1.96)) %&gt;% summarize(count_total = n(), count_A = sum(in_A), fr = mean(in_A)) } ) ## # A tibble: 4 × 3 ## count_total count_A fr ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 10 10 1 ## 2 100 95 0.95 ## 3 1000 946 0.946 ## 4 10000 9515 0.952 Observations: fr is limiting towards ~0.95 Now that we know what probability is; let’s connect the idea to distributions. 26.2 Relation to Distributions A continuous distribution models probability in terms of an integral \\[\\mathbb{P}_{\\rho}[X \\in A] = \\int_{-\\infty}^{+\\infty} \\mathcal{I}_A(x)\\rho(x)\\,dx = \\mathbb{E}_{\\rho}[\\mathcal{I}_A(X)]\\] where \\(\\mathcal{I}_A(x)\\) is the indicator function; a function that takes the value \\(1\\) when \\(x \\in A\\), and the value \\(0\\) when \\(x \\not\\in A\\). The \\[\\mathbb{E}[Y]\\] denotes taking the mean of a random variable; this is also called the expectation of a random variable. The function \\(\\rho(x)\\) is the density of the random variable—we’ll return to this idea in a bit. This definition gives us a geometric way to think about probability; the distribution definition means probability is the area under the density curve within the set \\(A\\). Before concluding this reading, let’s talk about two sticking points about distributions: 26.3 Sets vs Points Note that for continuous distributions, the probability of a single point is zero. First, let’s gather some empirical evidence. 26.3.1 q3 Modify the code below to consider the set \\(A = {z | z = 2}\\). Hint: Remember the difference between = and ==! ## TASK: Modify the code below map_dfr( c(10, 100, 1000, 1e4), function(n_samples) { tibble( z = rnorm(n = n_samples), n = n_samples ) %&gt;% mutate(in_A = (z == 2)) %&gt;% summarize(count_total = n(), count_A = sum(in_A), fr = mean(in_A)) } ) ## # A tibble: 4 × 3 ## count_total count_A fr ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 10 0 0 ## 2 100 0 0 ## 3 1000 0 0 ## 4 10000 0 0 Observations: fr is consistenly zero We can also understand this phenomenon in terms of areas; the following graph visualizes the set \\(A = {z | z = 2}\\) against a standard normal. ## NOTE: No need to change this! tibble(z = seq(-3, +3, length.out = 500)) %&gt;% mutate(d = dnorm(z)) %&gt;% ggplot(aes(z, d)) + geom_segment( data = tibble(z = 2, d = dnorm(2)), mapping = aes(z, 0, xend = 2, yend = d, color = &quot;Set A&quot;) ) + geom_line() + scale_color_discrete(name = &quot;&quot;) Note that this set \\(A\\) has nonzero height but zero width. Zero width corresponds to zero area, and thus zero probability. This is weird. If we’re using a distribution to model something physical, say a material property, this means that any specific material property has zero probability of occurring. But in practice, some specific value will be realized! If you’d like to learn more, take a look at Note [3]. 26.4 Two expressions of the same information There is a bit more terminology associated with distributions. The \\(\\rho(x)\\) we considered above is called a probability density function (PDF); it is the function we integrate in order to obtain a probability. In R, the PDF has the d prefix, for instance dnorm. For example, the standard normal has the following PDF. tibble(z = seq(-3, +3, length.out = 1000)) %&gt;% mutate(d = dnorm(z)) %&gt;% ggplot(aes(z, d)) + geom_line() + labs( x = &quot;z&quot;, y = &quot;Probability Density&quot;, title = &quot;Probability Density Function&quot; ) There is also a cumulative distribution function, which is related to the PDF \\(\\rho(x)\\) via \\[R(x) = \\int_{-\\infty}^x \\rho(s) ds.\\] In R, the CDF has the prefix p, such as pnorm. For example, the standard normal has the following CDF. tibble(z = seq(-3, +3, length.out = 1000)) %&gt;% mutate(p = pnorm(z)) %&gt;% ggplot(aes(z, p)) + geom_line() + labs( x = &quot;z&quot;, y = &quot;Probability&quot;, title = &quot;Cumulative Distribution Function&quot; ) Note that, by definition, the CDF gives the probability over the set \\(A(x) = {x&#39; | x&#39; &lt;= x}\\) (this is just all the values less than the value we’re considering \\(x\\)). Thus the CDF returns a probability (which explains the p prefix for R functions). 26.4.1 q4 Use pnorm to compute the probability that Z ~ norm(mean = 0, sd = 1) is less than or equal to zero. Compare this against your frequency prediction from q1. ## TASK: Compute the probability that Z &lt;= 0, assign to p0 p0 &lt;- pnorm(q = 0) p0 ## [1] 0.5 Use the following code to check your answer. ## NOTE: No need to change this assertthat::assert_that(p0 == 0.5) ## [1] TRUE print(&quot;Nice!&quot;) ## [1] &quot;Nice!&quot; Observations: I predicted fr = 0.5, which matches p0. Note that when our set \\(A\\) is an interval, we can use the CDF to express the associated probability. Note that \\[\\mathbb{P}_{\\rho}[a &lt;= X &lt;= b] = \\int_a^b \\rho(x) dx = \\int_{-\\infty}^b \\rho(x) dx - \\int_{-\\infty}^a \\rho(x) dx.\\] 26.4.2 q5 Using the identity above, use pnorm to compute the probability that \\(-1.96 &lt;= Z &lt;= +1.96\\) with Z ~ norm(mean = 0, sd = 1). ## TASK: Compute the probability that -1.96 &lt;= Z &lt;= +1.96, assign to pI pI &lt;- pnorm(q = +1.96) - pnorm(q = -1.96) pI ## [1] 0.9500042 Use the following code to check your answer. ## NOTE: No need to change this assertthat::assert_that(abs(pI - 0.95) &lt; 1e-3) ## [1] TRUE print(&quot;Nice!&quot;) ## [1] &quot;Nice!&quot; 26.5 Notes [1] This is where things get confusing and potentially controversial. This “limit of frequencies” definition of probability is called “Frequentist probability”, to distinguish it from a “Bayesian probability”. The distinction is meaningful but slippery. We won’t cover this distinction in this course. If you’re curious to learn more, my favorite video on Bayes vs Frequentist is by Kristin Lennox. Note that even Bayesians use this Frequentist definition of probability, for instance in Markov Chain Monte Carlo, the workhorse of Bayesian computation. [2] Technically these samples must be drawn independently and identically distributed, shortened to iid. [3] 3Blue1Brown has a very nice video about continuous probabilities. "],["data-separate-and-unite-columns.html", "27 Data: Separate and Unite Columns 27.1 Punnett Square 27.2 Alloys, Revisited", " 27 Data: Separate and Unite Columns Purpose: Data is easiest to use when it is tidy. In fact, the tidyverse (including ggplot, dplyr, etc.) is specifically designed to use tidy data. Last time we learned how to pivot data, but data can be untidy in other ways. Pivoting helped us when data were locked up in the column headers: This time, we’ll learn how to use separate and unite to deal with cell values that are untidy. Reading: Separate and Unite Columns Topics: Welcome, separate(), unite(), Case study Reading Time: ~30 minutes Notes: - I had trouble running the Case study in my browser. Note that the who dataset is loaded by the tidyverse. You can run the Case study locally if you need to! - The case study uses gather instead of pivot_longer; feel free to use pivot_longer in place. library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✔ ggplot2 3.4.0 ✔ purrr 1.0.1 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.5.0 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() The Case study was already a fair bit of work! Let’s do some simple review with separate and unite. 27.1 Punnett Square Let’s make a Punnett square with unite and some pivoting. You don’t need to remember any biology for this example: Your task is to take genes and turn the data into punnett. punnett &lt;- tribble( ~parent1, ~a, ~A, &quot;a&quot;, &quot;aa&quot;, &quot;aA&quot;, &quot;A&quot;, &quot;Aa&quot;, &quot;AA&quot; ) punnett ## # A tibble: 2 × 3 ## parent1 a A ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 a aa aA ## 2 A Aa AA genes &lt;- expand_grid( parent1 = c(&quot;a&quot;, &quot;A&quot;), parent2 = c(&quot;a&quot;, &quot;A&quot;) ) genes ## # A tibble: 4 × 2 ## parent1 parent2 ## &lt;chr&gt; &lt;chr&gt; ## 1 a a ## 2 a A ## 3 A a ## 4 A A 27.1.1 q1 Use a combination of unite and pivoting to turn genes into the same dataframe as punnett. df_q1 &lt;- genes %&gt;% unite(col = &quot;offspring&quot;, sep = &quot;&quot;, remove = FALSE, parent1, parent2) %&gt;% pivot_wider( names_from = parent2, values_from = offspring ) df_q1 ## # A tibble: 2 × 3 ## parent1 a A ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 a aa aA ## 2 A Aa AA Use the following test to check your answer: ## NOTE: No need to change this assertthat::assert_that( all_equal(df_q1, punnett) ) ## [1] TRUE print(&quot;Well done!&quot;) ## [1] &quot;Well done!&quot; 27.2 Alloys, Revisited In the previous data exercise, we studied an alloys dataset: ## NOTE: No need to edit; execute alloys_mod &lt;- tribble( ~thick, ~E00, ~mu00, ~E45, ~mu45, ~rep, 0.022, 10600, 0.321, 10700, 0.329, 1, 0.022, 10600, 0.323, 10500, 0.331, 2, 0.032, 10400, 0.329, 10400, 0.318, 1, 0.032, 10300, 0.319, 10500, 0.326, 2 ) alloys_mod ## # A tibble: 4 × 6 ## thick E00 mu00 E45 mu45 rep ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.022 10600 0.321 10700 0.329 1 ## 2 0.022 10600 0.323 10500 0.331 2 ## 3 0.032 10400 0.329 10400 0.318 1 ## 4 0.032 10300 0.319 10500 0.326 2 This slightly modified version of the data no longer has a convenient separator to help with pivoting. We’ll use a combination of pivoting and separate to tidy these data. 27.2.1 q2 Use a combination of separate and pivoting to tidy alloys_mod. df_q2 &lt;- alloys_mod %&gt;% pivot_longer( names_to = &quot;varang&quot;, values_to = &quot;value&quot;, cols = c(-thick, -rep) ) %&gt;% separate( col = varang, into = c(&quot;var&quot;, &quot;ang&quot;), sep = -2, convert = TRUE ) %&gt;% pivot_wider( names_from = var, values_from = value ) df_q2 ## # A tibble: 8 × 5 ## thick rep ang E mu ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.022 1 0 10600 0.321 ## 2 0.022 1 45 10700 0.329 ## 3 0.022 2 0 10600 0.323 ## 4 0.022 2 45 10500 0.331 ## 5 0.032 1 0 10400 0.329 ## 6 0.032 1 45 10400 0.318 ## 7 0.032 2 0 10300 0.319 ## 8 0.032 2 45 10500 0.326 Use the following tests to check your work: ## NOTE: No need to change this assertthat::assert_that( (dim(df_q2)[1] == 8) &amp; (dim(df_q2)[2] == 5) ) ## [1] TRUE print(&quot;Nice!&quot;) ## [1] &quot;Nice!&quot; "],["vis-scatterplots-and-layers.html", "28 Vis: Scatterplots and Layers 28.1 A Note on Layers 28.2 Exercises 28.3 Aside: Scatterplot vs bar chart 28.4 Raw populations 28.5 Population changes", " 28 Vis: Scatterplots and Layers Purpose: Scatterplots are a key tool for EDA. Scatteplots help us inspect the relationship between two variables. To enhance our scatterplots, we’ll learn how to use layers in ggplot to add multiple pieces of information to our plots. Reading: Scatterplots Topics: (All topics) Reading Time: ~40 minutes library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✔ ggplot2 3.4.0 ✔ purrr 1.0.1 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.5.0 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() library(ggrepel) 28.1 A Note on Layers In the reading we learned about layers in ggplot. Formally, ggplot is a “layered grammar of graphics”; each layer has the option to use built-in or inherited defaults, or override those defaults. There are two major settings we might want to change: the source of data or the mapping which defines the aesthetics. If we’re being verbose, we write a ggplot call like: ## NOTE: No need to modify! Just example code ggplot( data = mpg, mapping = aes(x = displ, y = hwy) ) + geom_point() However, ggplot makes a number of sensible defaults to help save us typing. Ggplot assumes an order for data, mapping, so we can drop the keywords: ## NOTE: No need to modify! Just example code ggplot( mpg, aes(x = displ, y = hwy) ) + geom_point() Similarly the aesthetic function aes() assumes the first two arguments will be x, y, so we can drop those arguments as well ## NOTE: No need to modify! Just example code ggplot( mpg, aes(displ, hwy) ) + geom_point() Above geom_point() inherits the mapping from the base ggplot call; however, we can override this. This can be helpful for a number of different purposes: ## NOTE: No need to modify! Just example code ggplot(mpg, aes(x = displ)) + geom_point(aes(y = hwy, color = &quot;hwy&quot;)) + geom_point(aes(y = cty, color = &quot;cty&quot;)) Later, we’ll learn more concise ways to construct graphs like the one above. But for now, we’ll practice using layers to add more information to scatterplots. 28.2 Exercises 28.2.1 q1 Add two geom_smooth trends to the following plot. Use “gam” for one trend and “lm” for the other. Comment on how linear or nonlinear the “gam” trend looks. diamonds %&gt;% ggplot(aes(carat, price)) + geom_point() + geom_smooth(aes(color = &quot;gam&quot;), method = &quot;gam&quot;) + geom_smooth(aes(color = &quot;lm&quot;), method = &quot;lm&quot;) ## `geom_smooth()` using formula = &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; ## `geom_smooth()` using formula = &#39;y ~ x&#39; Observations: - No; the “gam” trend curves below then above the linear trend 28.2.2 q2 Add non-overlapping labels to the following scattterplot using the provided df_annotate. Hint 1: geom_label_repel comes from the ggrepel package. Make sure to load it, and adhere to best-practices! Hint 2: You’ll have to use the data keyword to override the data layer! ## TODO: Use df_annotate below to add text labels to the scatterplot df_annotate &lt;- mpg %&gt;% group_by(class) %&gt;% summarize( displ = mean(displ), hwy = mean(hwy) ) mpg %&gt;% ggplot(aes(displ, hwy)) + geom_point(aes(color = class)) + geom_label_repel( data = df_annotate, aes(label = class, fill = class) ) 28.2.3 q3 Study the following scatterplot: Note whether city (cty) or highway (hwy) mileage tends to be greater. Describe the trend (visualized by geom_smooth) in mileage with engine displacement (a measure of engine size). Note: The grey region around the smooth trend is a confidence bound; we’ll discuss these further as we get deeper into statistical literacy. ## NOTE: No need to modify! Just analyze the scatterplot mpg %&gt;% pivot_longer(names_to = &quot;source&quot;, values_to = &quot;mpg&quot;, c(hwy, cty)) %&gt;% ggplot(aes(displ, mpg, color = source)) + geom_point() + geom_smooth() + scale_color_discrete(name = &quot;Mileage Type&quot;) + labs( x = &quot;Engine displacement (liters)&quot;, y = &quot;Mileage (mpg)&quot; ) ## `geom_smooth()` using method = &#39;loess&#39; and formula = &#39;y ~ x&#39; Observations: - hwy mileage tends to be larger; driving on the highway is more efficient - Mileage tends to decrease with engine size; cars with larger engines tend to be less efficient 28.3 Aside: Scatterplot vs bar chart Why use a scatterplot vs a bar chart? A bar chart is useful for emphasizing some threshold. Let’s look at a few examples: 28.4 Raw populations Two visuals of the same data: economics %&gt;% filter(date &gt; lubridate::ymd(&quot;2010-01-01&quot;)) %&gt;% ggplot(aes(date, pop)) + geom_col() Here we’re emphasizing zero, so we don’t see much of a change economics %&gt;% filter(date &gt; lubridate::ymd(&quot;2010-01-01&quot;)) %&gt;% ggplot(aes(date, pop)) + geom_point() Here’s we’re not emphasizing zero; the scale is adjusted to emphasize the trend in the data. 28.5 Population changes Two visuals of the same data: economics %&gt;% mutate(pop_delta = pop - lag(pop)) %&gt;% filter(date &gt; lubridate::ymd(&quot;2005-01-01&quot;)) %&gt;% ggplot(aes(date, pop_delta)) + geom_col() Here we’re emphasizing zero, so we can easily see the month of negative change. economics %&gt;% mutate(pop_delta = pop - lag(pop)) %&gt;% filter(date &gt; lubridate::ymd(&quot;2005-01-01&quot;)) %&gt;% ggplot(aes(date, pop_delta)) + geom_point() Here we’re not emphasizing zero; we can easily see the outlier month, but we have to read the axis to see that this is a case of negative growth. For more, see Bars vs Dots. "],["stats-descriptive-statistics.html", "29 Stats: Descriptive Statistics 29.1 Statistics 29.2 Handling NA’s 29.3 Central Tendency 29.4 Multi-modality 29.5 Quantiles 29.6 Spread 29.7 Dependence 29.8 Notes", " 29 Stats: Descriptive Statistics Purpose: We will use descriptive statistics to make quantitative summaries of a dataset. Descriptive statistics provide a much more compact description than a visualization, and are important when a data consumer wants “just one number”. Reading: (None; this exercise is the reading.) Topics: Mean, standard deviation, median, quantiles, dependence, correlation, robustness library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✔ ggplot2 3.4.0 ✔ purrr 1.0.1 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.5.0 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() library(nycflights13) library(gapminder) library(mvtnorm) ## NOTE: No need to change this! vis_central &lt;- function(df, var) { df %&gt;% ggplot(aes({{var}})) + geom_density() + geom_vline( data = . %&gt;% summarize(mu = mean({{var}}, na.rm = TRUE)), mapping = aes(xintercept = mu, color = &quot;Mean&quot;) ) + geom_vline( data = . %&gt;% summarize(mu = median({{var}}, na.rm = TRUE)), mapping = aes(xintercept = mu, color = &quot;Median&quot;) ) + scale_color_discrete(name = &quot;Location&quot;) } 29.1 Statistics A statistic is a numerical summary of a sample. Statistics are useful because they provide a useful summary about our data. A histogram gives us a rich summary of a datset: for example the departure delay time in the NYC flight data. ## NOTE: No need to change this! flights %&gt;% ggplot(aes(dep_delay)) + geom_histogram(bins = 60) + scale_x_log10() ## Warning in self$trans$transform(x): NaNs produced ## Warning: Transformation introduced infinite values in continuous x-axis ## Warning: Removed 208344 rows containing non-finite values (`stat_bin()`). However, we might be interested in a few questions about these data: What is a typical value for the departure delay? (Location) How variable are departure delay times? (Spread) How much does departure delay co-vary with distance? (Dependence) We can give quantitative answers to all these questions using statistics! 29.2 Handling NA’s Before we can start computing (descriptive) statistics, we need to learn how to deal with data issues. For instance, in the NYC flights data, we have a number of NAs. ## NOTE: No need to change this! flights %&gt;% summarize(across(where(is.numeric), ~sum(is.na(.)))) %&gt;% glimpse ## Rows: 1 ## Columns: 14 ## $ year &lt;int&gt; 0 ## $ month &lt;int&gt; 0 ## $ day &lt;int&gt; 0 ## $ dep_time &lt;int&gt; 8255 ## $ sched_dep_time &lt;int&gt; 0 ## $ dep_delay &lt;int&gt; 8255 ## $ arr_time &lt;int&gt; 8713 ## $ sched_arr_time &lt;int&gt; 0 ## $ arr_delay &lt;int&gt; 9430 ## $ flight &lt;int&gt; 0 ## $ air_time &lt;int&gt; 9430 ## $ distance &lt;int&gt; 0 ## $ hour &lt;int&gt; 0 ## $ minute &lt;int&gt; 0 These NAs will “infect” our computation, and lead to NA summaries. ## NOTE: No need to change this! flights %&gt;% summarize(across(where(is.numeric), mean)) %&gt;% glimpse ## Rows: 1 ## Columns: 14 ## $ year &lt;dbl&gt; 2013 ## $ month &lt;dbl&gt; 6.54851 ## $ day &lt;dbl&gt; 15.71079 ## $ dep_time &lt;dbl&gt; NA ## $ sched_dep_time &lt;dbl&gt; 1344.255 ## $ dep_delay &lt;dbl&gt; NA ## $ arr_time &lt;dbl&gt; NA ## $ sched_arr_time &lt;dbl&gt; 1536.38 ## $ arr_delay &lt;dbl&gt; NA ## $ flight &lt;dbl&gt; 1971.924 ## $ air_time &lt;dbl&gt; NA ## $ distance &lt;dbl&gt; 1039.913 ## $ hour &lt;dbl&gt; 13.18025 ## $ minute &lt;dbl&gt; 26.2301 Let’s learn how to handle this: 29.2.1 q1 The following code returns NA. Look up the documentation for mean and use an additional argument to strip the NA values in the dataset before computing the mean. Make this modification to the code below and report the mean departure delay time. ## TASK: Edit to drop all NAs before computing the mean flights %&gt;% summarize(dep_delay = mean(dep_delay, na.rm = TRUE)) ## # A tibble: 1 × 1 ## dep_delay ## &lt;dbl&gt; ## 1 12.6 Observations: The mean departure delay is about 12.6 minutes 29.3 Central Tendency Central tendency is the idea of where data tend to be “located”—this concept is also called location. It is best thought of as the “center” of the data. The following graph illustrates central tendency. ## NOTE: No need to change this! set.seed(101) tibble(z = rnorm(n = 1e3)) %&gt;% vis_central(z) There are two primary measures of central tendency; the mean and median. The mean is the simple arithmetic average: the sum of all values divided by the total number of values. The mean is denoted by \\(\\overline{x}\\) and defined by \\[\\overline{X} = \\frac{1}{n} \\sum_{i=1}^n X_i,\\] where \\(n\\) is the number of data points, and the \\(X_i\\) are the individual values. The median is the value that separates half the data above and below. Weirdly, there’s no standard symbol for the median, so we’ll just denote it as \\(\\text{Median}[D]\\) to denote the median of a set of data \\(D\\). The median is a robust statistic, which is best illustrated by example. Consider the following two samples v_base and v_outlier. The sample v_outlier has an outlier, a value very different from the other values. Observe what value the mean and median take for these different samples. ## NOTE: No need to change this! v_base &lt;- c(1, 2, 3, 4, 5) v_outlier &lt;- c(v_base, 1e3) tibble( mean_base = mean(v_base), median_base = median(v_base), mean_outlier = mean(v_outlier), median_outlier = median(v_outlier) ) %&gt;% glimpse ## Rows: 1 ## Columns: 4 ## $ mean_base &lt;dbl&gt; 3 ## $ median_base &lt;dbl&gt; 3 ## $ mean_outlier &lt;dbl&gt; 169.1667 ## $ median_outlier &lt;dbl&gt; 3.5 Note that for v_outlier the mean is greatly increased, but the median is only slightly changed. It is in this sense that the median is robust—it is robust to outliers. When one has a dataset with outliers, the median is usually a better measure of central tendency.[1] It can be useful to think about when the mean and median agree or disagree with each other. For instance, with the flights data: ## NOTE: No need to change this! flights %&gt;% vis_central(dep_delay) ## Warning: Removed 8255 rows containing non-finite values (`stat_density()`). the mean and median dep_delay largely agree (relative to all the other data). But for the gapminder data: ## NOTE: No need to change this! gapminder %&gt;% filter(year == max(year)) %&gt;% vis_central(gdpPercap) the mean and median gdpPercap disagree.[2] 29.3.1 q2 The following code computes the mean and median dep_delay for each carrier, and sorts based on mean. Duplicate the code, and sort by median instead. Report your observations on which carriers are in both lists, and which are different. Also comment on what negative dep_delay values mean. Hint: Remember you can check the documentation of a built-in dataset with ?flights! ## NOTE: No need to change this! flights %&gt;% group_by(carrier) %&gt;% summarize( mean = mean(dep_delay, na.rm = TRUE), median = median(dep_delay, na.rm = TRUE) ) %&gt;% arrange(desc(mean)) %&gt;% head(5) ## # A tibble: 5 × 3 ## carrier mean median ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 F9 20.2 0.5 ## 2 EV 20.0 -1 ## 3 YV 19.0 -2 ## 4 FL 18.7 1 ## 5 WN 17.7 1 ## TASK: Duplicate the code above, but sort by `median` instead flights %&gt;% group_by(carrier) %&gt;% summarize( mean = mean(dep_delay, na.rm = TRUE), median = median(dep_delay, na.rm = TRUE) ) %&gt;% arrange(desc(median)) %&gt;% head(5) ## # A tibble: 5 × 3 ## carrier mean median ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 FL 18.7 1 ## 2 WN 17.7 1 ## 3 F9 20.2 0.5 ## 4 UA 12.1 0 ## 5 VX 12.9 0 Observations: The carriers F9, FL, WN are in both lists The carriers EV, YV are top in mean, while UA, VX are top in median Negative values of dep_delay signal early departures 29.4 Multi-modality It may not seem like it, but we’re actually making an assumption when we use the mean (or median) as a typical value. Imagine we had the following data: bind_rows( tibble(X = rnorm(300, mean = -2)), tibble(X = rnorm(300, mean = +2)) ) %&gt;% ggplot(aes(X)) + geom_histogram(bins = 60) + geom_vline(aes(xintercept = mean(X), color = &quot;Mean&quot;)) + geom_vline(aes(xintercept = median(X), color = &quot;Median&quot;)) + scale_color_discrete(name = &quot;Statistic&quot;) Here the mean and median are both close to zero, but zero is an atypical number! This is partly why we don’t only compute descriptive statistics, but also do a deeper dive into our data. Here, we should probably refuse to give a single typical value; instead, it seems there might really be two populations showing up in the same dataset, so we can give two typical numbers, say -2, +2. 29.5 Quantiles Before we can talk about spread, we need to talk about quantiles. A quantile is a value that separates a user-specified fraction of data (or a distribution). For instance, the median is the \\(50%\\) quantile; thus \\(\\text{Median}[D] = Q_{0.5}[D]\\). We can generalize this idea to talk about any quantile between \\(0%\\) and \\(100%\\). The following graph visualizes the \\(25%, 50%, 75%\\) quantiles of a standard normal. Since these are the quarter-quantiles (\\(1/4, 2/4, 3/4\\)), these are often called the quartiles. ## NOTE: No need to change this! tibble(z = seq(-3, +3, length.out = 500)) %&gt;% mutate(d = dnorm(z)) %&gt;% ggplot(aes(z, d)) + geom_line() + geom_segment( data = tibble(p = c(0.25, 0.50, 0.75)) %&gt;% mutate( z = qnorm(p), d = dnorm(z) ), mapping = aes(xend = z, yend = 0, color = as_factor(p)) ) + scale_color_discrete(name = &quot;Quantile&quot;) Note: The function qnorm returns the quantiles of a normal distribution. We’ll focus on quantiles of samples in this exercise. We’ll use the quartiles to define the interquartile range. First, the quantile() function computes quantiles of a sample. For example: ## NOTE: No need to change this! Run for an example flights %&gt;% pull(dep_delay) %&gt;% quantile(., probs = c(0, 0.25, 0.50, 0.75, 1.00), na.rm = TRUE) ## 0% 25% 50% 75% 100% ## -43 -5 -2 11 1301 Like with mean, median, we need to specify if we want to remove NAs. We can provide a list of probs to specify the probabilities of the quantiles. Remember: a probability is a value between \\([0, 1]\\), while a quantile is a value that probably has units, like minutes in the case of dep_delay. Now we can define the interquartile range: \\[IQR[D] = Q_{0.75}[D] - Q_{0.25}[D]\\], where \\(Q_{p}[D]\\) is the \\(p\\)-th quantile of a sample \\(D\\). 29.5.1 q3 Using the function quantile, compute the interquartile range; this is the difference between the \\(75%\\) and \\(25%\\) quantiles. ## NOTE: No need to change this! set.seed(101) v_test_iqr &lt;- rnorm(n = 10) test_iqr &lt;- quantile(v_test_iqr, probs = 0.75) - quantile(v_test_iqr, probs = 0.25) Use the following test to check your answer. ## NOTE: No need to change this! assertthat::assert_that(test_iqr == IQR(v_test_iqr)) ## [1] TRUE print(&quot;Great job!&quot;) ## [1] &quot;Great job!&quot; 29.6 Spread Spread is the concept of how tightly or widely data are spread out. There are two primary measures of spread: the standard deviation, and the interquartile range. The standard deviation (SD) is denoted by \\(s\\) and defined by \\[s = \\sqrt{ \\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\overline{X})^2 },\\] where \\(\\overline{X}\\) is the mean of the data. Note the factor of \\(n-1\\) rather than \\(n\\): This slippery idea is called Bessel’s correction. Note that \\(\\sigma^2\\) is called the variance. By way of analogy, mean is to standard deviation as median is to IQR: The IQR is a robust measure of spread. Returning to our outlier example: ## NOTE: No need to change this! v_base &lt;- c(1, 2, 3, 4, 5) v_outlier &lt;- c(v_base, 1e3) tibble( sd_base = sd(v_base), IQR_base = IQR(v_base), sd_outlier = sd(v_outlier), IQR_outlier = IQR(v_outlier) ) %&gt;% glimpse ## Rows: 1 ## Columns: 4 ## $ sd_base &lt;dbl&gt; 1.581139 ## $ IQR_base &lt;dbl&gt; 2 ## $ sd_outlier &lt;dbl&gt; 407.026 ## $ IQR_outlier &lt;dbl&gt; 2.5 29.6.1 q4 Using the code from q2 as a starting point, compute the standard deviation (sd()) and interquartile range (IQR()), and rank the top five carriers, this time by sd and IQR. Report your observations on which carriers are in both lists, and which are different. Also note and comment on which carrier (among your top-ranked) has the largest difference between sd and IQR. ## TODO: Use code from q2 to compute the sd and IQR, rank as before flights %&gt;% group_by(carrier) %&gt;% summarize( sd = sd(dep_delay, na.rm = TRUE), IQR = IQR(dep_delay, na.rm = TRUE) ) %&gt;% arrange(desc(sd)) %&gt;% head(5) ## # A tibble: 5 × 3 ## carrier sd IQR ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 HA 74.1 6 ## 2 F9 58.4 22 ## 3 FL 52.7 21 ## 4 YV 49.2 30 ## 5 EV 46.6 30 flights %&gt;% group_by(carrier) %&gt;% summarize( sd = sd(dep_delay, na.rm = TRUE), IQR = IQR(dep_delay, na.rm = TRUE) ) %&gt;% arrange(desc(IQR)) %&gt;% head(5) ## # A tibble: 5 × 3 ## carrier sd IQR ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 EV 46.6 30 ## 2 YV 49.2 30 ## 3 9E 45.9 23 ## 4 F9 58.4 22 ## 5 FL 52.7 21 Observations: The carriers F9, FL, YV, EV are in both lists The carrier HA is top in sd, while 9E is top in IQR HA has a large difference between sd and IQR; based on the following vis, it appears that HA has a lot more outliers than other carriers, which bumps up its sd flights %&gt;% filter(carrier %in% c(&quot;HA&quot;, &quot;F9&quot;, &quot;FL&quot;, &quot;YV&quot;, &quot;EV&quot;)) %&gt;% ggplot(aes(carrier, dep_delay)) + geom_boxplot() ## Warning: Removed 2949 rows containing non-finite values (`stat_boxplot()`). 29.7 Dependence So far, we’ve talked about descriptive statistics to consider one variable at a time. To conclude, we’ll talk about statistics to consider dependence between two variables in a dataset. Dependence—like location or spread—is a general idea of relation between two variables. For instance, when it comes to flights we’d expect trips between more distant airports to take longer. If we plot distance vs air_time in a scatterplot, we indeed see this dependence. flights %&gt;% ggplot(aes(air_time, distance)) + geom_point() ## Warning: Removed 9430 rows containing missing values (`geom_point()`). Two flavors of correlation help us make this idea quantitative: the Pearson correlation and Spearman correlation. Unlike our previous quantities for location and spread, these correlations are dimensionless (they have no units), and they are bounded between \\([-1, +1]\\). The Pearson correlation is often denoted by \\(r_{XY}\\), and it specifies the variables being considered \\(X, Y\\). It is defined by \\[r_{XY} = \\frac{\\sum_{i=1}^n (X_i - \\overline{X}) (Y_i - \\overline{Y})}{s_X s_Y}.\\] The Spearman correlation is often denoted by \\(\\rho_{XY}\\), and is actually defined in terms of the Pearson correlation \\(r_{XY}\\), but with the ranks (\\(1\\) to \\(n\\)) rather than the values \\(X_i, Y_i\\). For example, we might expect a strong correlation between the air_time and the distance between airports. The function cor computes the Pearson correlation. ## NOTE: No need to change this! flights %&gt;% summarize(rho = cor(air_time, distance, use = &quot;na.or.complete&quot;)) ## # A tibble: 1 × 1 ## rho ## &lt;dbl&gt; ## 1 0.991 Note: Unfortunately, the function cor doesn’t follow the same pattern as mean or sd. We have to use this use argument to filter NAs. However, we wouldn’t expect any relation between air_time and month. ## NOTE: No need to change this! flights %&gt;% summarize(rho = cor(air_time, month, use = &quot;na.or.complete&quot;)) ## # A tibble: 1 × 1 ## rho ## &lt;dbl&gt; ## 1 0.0109 In the case of a perfect linear relationships the Pearson correlation takes the value \\(+1\\) (for a positive slope) or \\(-1\\) for a negative slope. 29.7.1 q5 Compute the Pearson correlation between x, y below. Play with the slope and observe the change in the correlation. slope &lt;- 0.5 # Play with this value; observe the correlation df_line &lt;- tibble(x = seq(-1, +1, length.out = 50)) %&gt;% mutate(y = slope * x) df_line %&gt;% ggplot(aes(x, y)) + geom_point() Observations: slope values greater than 0 have a positive correlation slope values less than 0 have a negative correlation Note that this means correlation is a measure of dependence; it is not a measure of slope! It is better thought of as how strong the relationship between two variables is. A closer-to-zero correlation indicates a noisy relationship between variables, while a closer-to-one (in absolute value) indicates a more perfect, predictable relationship between the variables. For instance, the following code simulates data with different correlations, and facets the data based on the underlying correlation. ## NOTE: No need to change this! map_dfr( c(-1.0, -0.5, +0.0, +0.5, +1.0), # Chosen correlations function(r) { # Simulate a multivariate gaussian X &lt;- rmvnorm( n = 100, sigma = matrix(c(1, r, r, 1), nrow = 2) ) # Package and return the data tibble( x = X[, 1], y = X[, 2], r = r ) } ) %&gt;% # Plot the data ggplot(aes(x, y)) + geom_point() + facet_wrap(~r) One of the primary differences between Pearson and Spearman is that Pearson is a linear correlation, while Spearman is a nonlinear correlation. For instance, the following data ## NOTE: No need to change this! # Positive slope df_monotone &lt;- tibble(x = seq(-pi/2 + 0.1, +pi/2 - 0.1, length.out = 50)) %&gt;% mutate(y = tan(x)) df_monotone %&gt;% ggplot(aes(x, y)) + geom_point() have a perfect relationship between them. The Pearson correlation does not pick up on this fact, while the Spearman correlation indicates a perfect relation. # Positive slope df_monotone %&gt;% summarize(rho = cor(x, y, method = &quot;pearson&quot;)) ## # A tibble: 1 × 1 ## rho ## &lt;dbl&gt; ## 1 0.846 df_monotone %&gt;% summarize(rho = cor(x, y, method = &quot;spearman&quot;)) ## # A tibble: 1 × 1 ## rho ## &lt;dbl&gt; ## 1 1 One more note about functional relationships: Neither Pearson nor Spearman can pick up on arbitrary dependencies. 29.7.2 q6 Run the code chunk below and look at the visualization: Make a prediction about what you think the correlation will be. Then compute the Pearson correlation between x, y below. ## NOTE: No need to change this! df_quad &lt;- tibble(x = seq(-1, +1, length.out = 50)) %&gt;% mutate(y = x^2 - 0.5) ## TASK: Compute the Pearson and Spearman correlations on `df_quad` df_quad %&gt;% summarize(rho = cor(x, y, method = &quot;pearson&quot;)) ## # A tibble: 1 × 1 ## rho ## &lt;dbl&gt; ## 1 -2.02e-16 df_quad %&gt;% summarize(rho = cor(x, y, method = &quot;spearman&quot;)) ## # A tibble: 1 × 1 ## rho ## &lt;dbl&gt; ## 1 -0.0236 df_quad %&gt;% ggplot(aes(x, y)) + geom_point() Observations: Both correlations are near-zero One last point about correlation: The mean is to Pearson correlation as the median is to Spearman correlation. The median and Spearman’s rho are robust to outliers. ## NOTE: No need to change this! set.seed(101) X &lt;- rmvnorm( n = 25, sigma = matrix(c(1, 0.9, 0.9, 1), nrow = 2) ) df_cor_outliers &lt;- tibble( x = X[, 1], y = X[, 2] ) %&gt;% bind_rows(tibble(x = c(-10.1, -10, 10, 10.1), y = c(-1.2, -1.1, 1.1, 1.2))) df_cor_outliers %&gt;% ggplot(aes(x, y)) + geom_point() df_cor_outliers %&gt;% summarize(rho = cor(x, y, method = &quot;pearson&quot;)) ## # A tibble: 1 × 1 ## rho ## &lt;dbl&gt; ## 1 0.621 df_cor_outliers %&gt;% summarize(rho = cor(x, y, method = &quot;spearman&quot;)) ## # A tibble: 1 × 1 ## rho ## &lt;dbl&gt; ## 1 0.884 29.8 Notes [1] So then why bother with the mean? It turns out the mean is a fundamental idea in statistics, as it’s a key component of a lot of other statistical procedures. You’ll end up using the mean in countless different ways, so it’s worth recognizing its weakness to outliers. [2] As a side note, since dollars are pretty-well divorced from reality (there’s not physical upper bound on perceived value), distributions of dollars can have very large outliers. That’s why you often see median incomes reported, rather than mean income. "],["data-joining-datasets.html", "30 Data: Joining Datasets 30.1 Dangers of Binding! 30.2 Utility of Filtering Joins", " 30 Data: Joining Datasets Purpose: Often our data are scattered across multiple sets. In this case, we need to be able to join data. Reading: Join Data Sets Topics: Welcome, mutating joins, filtering joins, Binds and set operations Reading Time: ~30 minutes library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✔ ggplot2 3.4.0 ✔ purrr 1.0.1 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.5.0 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() library(nycflights13) 30.1 Dangers of Binding! In the reading we learned about bind_cols and bind_rows. ## NOTE: No need to change this; setup beatles1 &lt;- tribble( ~band, ~name, &quot;Beatles&quot;, &quot;John&quot;, &quot;Beatles&quot;, &quot;Paul&quot;, &quot;Beatles&quot;, &quot;George&quot;, &quot;Beatles&quot;, &quot;Ringo&quot; ) beatles2 &lt;- tribble( ~surname, ~instrument, &quot;McCartney&quot;, &quot;bass&quot;, &quot;Harrison&quot;, &quot;guitar&quot;, &quot;Starr&quot;, &quot;drums&quot;, &quot;Lennon&quot;, &quot;guitar&quot; ) bind_cols(beatles1, beatles2) ## # A tibble: 4 × 4 ## band name surname instrument ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Beatles John McCartney bass ## 2 Beatles Paul Harrison guitar ## 3 Beatles George Starr drums ## 4 Beatles Ringo Lennon guitar 30.1.1 q1 Describe what is wrong with the result of bind_cols above and how it happened. The rows of beatles1 and beatles2 were not ordered identically; therefore the wrong names and surnames were combined We’ll use the following beatles3 to correctly join the data. ## NOTE: No need to change this; setup beatles3 &lt;- tribble( ~name, ~surname, &quot;John&quot;, &quot;Lennon&quot;, &quot;Paul&quot;, &quot;McCartney&quot;, &quot;George&quot;, &quot;Harrison&quot;, &quot;Ringo&quot;, &quot;Starr&quot; ) beatles_joined &lt;- tribble( ~band, ~name, ~surname, ~instrument, &quot;Beatles&quot;, &quot;John&quot;, &quot;Lennon&quot;, &quot;guitar&quot;, &quot;Beatles&quot;, &quot;Paul&quot;, &quot;McCartney&quot;, &quot;bass&quot;, &quot;Beatles&quot;, &quot;George&quot;, &quot;Harrison&quot;, &quot;guitar&quot;, &quot;Beatles&quot;, &quot;Ringo&quot;, &quot;Starr&quot;, &quot;drums&quot; ) 30.1.2 q2 Use the following beatles3 to correctly join beatles1 df_q2 &lt;- beatles1 %&gt;% left_join( beatles3, by = &quot;name&quot; ) %&gt;% left_join( beatles2, by = &quot;surname&quot; ) df_q2 ## # A tibble: 4 × 4 ## band name surname instrument ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Beatles John Lennon guitar ## 2 Beatles Paul McCartney bass ## 3 Beatles George Harrison guitar ## 4 Beatles Ringo Starr drums Use the following test to check your work: ## NOTE: No need to change this assertthat::assert_that(all_equal(df_q2, beatles_joined)) ## [1] TRUE print(&quot;Nice!&quot;) ## [1] &quot;Nice!&quot; There’s a very important lesson here: In general, don’t trust bind_cols. It’s easy in the example above to tell there’s a problem because the data are small; when working with larger datasets, R will happily give you the wrong answer if you give it the wrong instructions. Whenever possible, use some form of join to combine datasets. 30.2 Utility of Filtering Joins Filtering joins are an elegant way to produce complicated filters. They are especially helpful because you can first inspect what criteria you’ll filter on, then perform the filter. We’ll use the tidyr tool expand_grid to make such a criteria dataframe, then apply it to filter the flights data. 30.2.1 q3 Create a “grid” of values Use expand_grid to create a criteria dataframe with the month equal to 8, 9 and the airport identifiers in dest for the San Francisco, San Jose, and Oakland airports. Hint 1: To find the airport identifiers, you can either use str_detect to filter the airports dataset, or use Google! Hint 2: Remember to look up the documentation for a function you don’t yet know! criteria &lt;- expand_grid( month = c(8, 9), dest = c(&quot;SJC&quot;, &quot;SFO&quot;, &quot;OAK&quot;) ) criteria ## # A tibble: 6 × 2 ## month dest ## &lt;dbl&gt; &lt;chr&gt; ## 1 8 SJC ## 2 8 SFO ## 3 8 OAK ## 4 9 SJC ## 5 9 SFO ## 6 9 OAK Use the following test to check your work: ## NOTE: No need to change this assertthat::assert_that( all_equal( criteria, criteria %&gt;% semi_join( airports %&gt;% filter( str_detect(name, &quot;San Jose&quot;) | str_detect(name, &quot;San Francisco&quot;) | str_detect(name, &quot;Metropolitan Oakland&quot;) ), by = c(&quot;dest&quot; = &quot;faa&quot;) ) ) ) ## [1] TRUE assertthat::assert_that( all_equal( criteria, criteria %&gt;% filter(month %in% c(8, 9)) ) ) ## [1] TRUE print(&quot;Well done!&quot;) ## [1] &quot;Well done!&quot; 30.2.2 q4 Use the criteria dataframe you produced above to filter flights on dest and month. Hint: Remember to use a filtering join to take advantage of the criteria dataset we built above! df_q4 &lt;- flights %&gt;% semi_join( criteria, by = c(&quot;dest&quot;, &quot;month&quot;) ) df_q4 ## # A tibble: 2,584 × 19 ## year month day dep_time sched_de…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 2013 8 1 554 559 -5 909 902 7 UA ## 2 2013 8 1 601 601 0 916 915 1 UA ## 3 2013 8 1 657 700 -3 1016 1016 0 DL ## 4 2013 8 1 723 730 -7 1040 1045 -5 VX ## 5 2013 8 1 738 740 -2 1111 1055 16 VX ## 6 2013 8 1 745 743 2 1117 1103 14 UA ## 7 2013 8 1 810 755 15 1120 1115 5 AA ## 8 2013 8 1 825 829 -4 1156 1143 13 UA ## 9 2013 8 1 838 840 -2 1230 1143 47 UA ## 10 2013 8 1 851 853 -2 1227 1212 15 B6 ## # … with 2,574 more rows, 9 more variables: flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, and abbreviated variable names ## # ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay Use the following test to check your work: ## NOTE: No need to change this assertthat::assert_that( all_equal( df_q4, df_q4 %&gt;% filter( month %in% c(8, 9), dest %in% c(&quot;SJC&quot;, &quot;SFO&quot;, &quot;OAK&quot;) ) ) ) ## [1] TRUE print(&quot;Nice!&quot;) ## [1] &quot;Nice!&quot; "],["vis-lines.html", "31 Vis: Lines", " 31 Vis: Lines Purpose: Line plots are a key tool for EDA. In contrast with a scatterplot, a line plot assumes the data have a function relation. This can create an issue if we try to plot data that do not satisfy our assumptions. In this exercise, we’ll practice some best-practices for constructing line plots. Reading: Line plots Topics: Welcome, Line graphs, Similar geoms (skip Maps) Reading Time: ~30 minutes library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✔ ggplot2 3.4.0 ✔ purrr 1.0.1 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.5.0 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() library(gapminder) 31.0.1 q1 The following graph doesn’t work as its author intended. Based on what we learned in the reading, fix the following code. gapminder %&gt;% filter(continent == &quot;Asia&quot;) %&gt;% ggplot(aes(year, lifeExp, color = country)) + geom_line() 31.0.2 q2 A line plot makes a certain assumption about the underlying data. What assumption is this? How does that assumption relate to the following graph? Put differently, why is the use of geom_line a bad idea for the following dataset? ## TODO: No need to edit; just answer the questions mpg %&gt;% ggplot(aes(displ, hwy)) + geom_line() Observations: - A line plot assumes the underlying data have a function relationship; that is, that there is one y value for every x value - The mpg dataset does not have a function relation between displ and hwy; there are cars with identical values of displ but different values of hwy 31.0.3 q3 The following graph shows both the raw data and a smoothed version. Describe the trends that you can see in the different curves. ## TODO: No need to edit; just interpret the graph economics %&gt;% ggplot(aes(date, unemploy)) + geom_line(aes(color = &quot;Raw&quot;)) + geom_smooth(aes(color = &quot;Smoothed&quot;), se = FALSE) + scale_color_discrete(name = &quot;Source&quot;) ## `geom_smooth()` using method = &#39;loess&#39; and formula = &#39;y ~ x&#39; Observations: - The Raw data indicate short-term cyclical patterns that occur over a few years - The Smoothed data indicate a longer-term trend occurring over decades "],["communication-narrative-basics.html", "32 Communication: Narrative Basics 32.1 Exposition 32.2 Exercises: Judging Narrative Content", " 32 Communication: Narrative Basics Purpose: The point of data science is not to gather a random pile of facts; it’s to do something with those facts. One of our key goals in data science is to help drive decisions and action—we accomplish this by communicating our data story. Since story is fundamentally an exercise in narrative, we’ll start our data story training with some narrative basics. Reading: Randy Olson’s TED Talk, introducing the And, But, Therefore (ABT) framework. Reading Time: ~ 10 minutes Lesson Plan: Ensure all students submit their narrative spectrum classifications and justification through the Google Form. Set aside a full-class meeting to discuss the student results. Visualize the results, and see what (if any) consensus the class came to. Ask a few students who classified the graphs differently to share their perspective. After gathering student input, offer your own perspective on the graph. Repeat for each example. Make sure to emphasize that different people will read graphs differently; we don’t have complete control over the narrative when presenting a graph. But the simpler and more focused our graph, the better control we have over the narrative experienced. (TODO: Future exercises/activities should emphasize this!) 32.1 Exposition Scientist-turned-storyteller Randy Olson[1] introduced the narrative spectrum. Three points along the spectrum are listed below: TLA Framework Narrative Spectrum AAA And, And, And Non-narrative ABT And, But, Therefore Just right! DHY Despite, However, Yet Overly-narrative The narrative spectrum runs from non-narrative: introducing no conflict or tension, to overly-narrative: introducing too many conflicting ideas. Olson observed that the middle of the narrative spectrum is just the right amount of narrative content. The extreme points tend to be boring: A child may tell a story like “We went to the store, AND the man had a hat, AND I lost a shoe, AND we went home.” This story lacks any narrative content: no part of the story relates to any other part, so no conflict or drama can arise. (AAA) An extremely learned professor may tell a story like “Kolmogorov proposed a 5/3 power law. HOWEVER Smith found 3/8 power law behavior. YET Chandrasekhar discovered a 2/3 power law….” This story swings in the opposite direction; there is too much conflict, and most listeners will be totally lost. This is the proverbial random pile of facts we need to avoid when communicating. (DHY) Using the ABT framework can help us get started with framing a story. For example: “Data science is the use of computation and statistics to learn from data AND we want to use data science to help people make decisions BUT a random pile of facts will lose our audience THEREFORE we will study narrative to help tell our data story.” The AND part of the framework is our exposition; every story needs some setup. The BUT part introduces some conflict—in a hollywood story this could be a murder, but in science it could be an unexplained phenomenon. THEREFORE is where we pay off the exposition and conflict. In our hollywood story its where we solve the murder. In science its where we learn something about reality, and pose the next exciting question to investigate. Note: The ABT framework is not the only way to tell a story. It is a simplified framework to help us get started! 32.2 Exercises: Judging Narrative Content Now let’s put all that narrative theory to use! We’re going to judge a number of graphs based on their narrative content, placing them at a point on Olson’s narrative spectrum. To do so, we’re going to need some framing for the following exercise: For this exercise, pretend that you are going to show the following graphs to a very busy data scientist. The following graphs are not intended for you to use to discover things. They are intended to communicate your findings to someone else. Your data science colleague is smart and competent (she knows what a boxplot is, understands variability, etc.), but she’s also busy. You need to present a figure that tells a story quickly, or she’s going to use her limited time to think about something else. Your task: Study the following graphs and determine the closest point—AAA, ABT, or DHY—near which the example lies on narrative spectrum. Keep in mind your intended audience (your colleague) when judging how much or how little narrative content is present. ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✔ ggplot2 3.4.0 ✔ purrr 1.0.1 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.5.0 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() 32.2.1 q1 Identify the point on the narrative spectrum, and justify your answer. Hint: Try telling yourself a story based on the graph! This can be your justification for the narrative spectrum point you select. ## Saving 7 x 5 in image Classify: ABT Justify At low carat, an improvement in cut tends to decrease price. However, at a higher carat, an improvement in cut tends to increase price. This graph conveys a small amount of information, but still manages to introduce some conflict. 32.2.2 q2 Identify the point on the narrative spectrum, and justify your answer. ## `geom_smooth()` using method = &#39;gam&#39; and formula = &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; ## Saving 7 x 5 in image ## `geom_smooth()` using method = &#39;gam&#39; and formula = &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; Classify: DHY Justify Diamonds tend to be price ordered by their cut. However, at low carat the Fair diamonds tend to be most pricey. Yet Good, Very Good and Premium diamonds tend to overlap. There are too many conflicting ideas in this graph to quickly tell a compelling story. 32.2.3 q3 Identify the point on the narrative spectrum, and justify your answer. ## Saving 7 x 5 in image Classify: AAA Justify The highest carat diamonds tend to be Fair, and Fair diamonds tend to vary a lot in depth, and price varies widely for all cut values, and some diamonds have x, y, z values at zero…. 32.2.4 q4 Turn in all your answers via google forms. Link "],["bibliography.html", "33 Bibliography", " 33 Bibliography [1] Olson, “Houston, We Have a Narrative” (2015) "],["data-working-with-strings.html", "34 Data: Working with strings 34.1 Intro to Stringr 34.2 Removal 34.3 Regex in Other Functions 34.4 Notes", " 34 Data: Working with strings Purpose: Strings show up in data science all the time. Even when all our variables are numeric, our column names are generally strings. To strengthen our ability to work with strings, we’ll learn how to use regular expressions and apply them to wrangling and tidying data. Reading: RegexOne; All lessons in the Interactive Tutorial, Additional Practice Problems are optional Topics: Regular expressions, stringr package functions, pivoting Note: The stringr cheatsheet is a helpful reference for this exercise! library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✔ ggplot2 3.4.0 ✔ purrr 1.0.1 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.5.0 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() 34.1 Intro to Stringr Within the Tidyverse, the package stringr contains a large number of functions for helping us with strings. We’re going to learn a number of useful functions for working with strings using regular expressions. 34.1.1 Detect The function str_detect() allows us to detect the presence of a particular pattern. For instance, we can give it a fixed pattern such as: ## NOTE: No need to edit strings &lt;- c( &quot;Team Alpha&quot;, &quot;Team Beta&quot;, &quot;Group 1&quot;, &quot;Group 2&quot; ) str_detect( string = strings, pattern = &quot;Team&quot; ) ## [1] TRUE TRUE FALSE FALSE str_detect() checks whether the given pattern is within the given string. This function returns a boolean—a TRUE or FALSE value—and furthermore it is vectorized—it returns a boolean vector of T/F values corresponding to each original entry. Since str_detect() returns boolean values, we can use it as a helper in filter() calls. For instance, in the mpg dataset there are automobiles with trans that are automatic or manual. ## NOTE: No need to change this! mpg %&gt;% select(trans) %&gt;% glimpse() ## Rows: 234 ## Columns: 1 ## $ trans &lt;chr&gt; &quot;auto(l5)&quot;, &quot;manual(m5)&quot;, &quot;manual(m6)&quot;, &quot;auto(av)&quot;, &quot;auto(l5)&quot;, … We can’t simply check whether trans == \"auto\", because no string will exactly match that fixed pattern. But we can instead check for a substring. ## NOTE: No need to change this! mpg %&gt;% filter(str_detect(trans, &quot;auto&quot;)) ## # A tibble: 157 × 11 ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 audi a4 1.8 1999 4 auto… f 18 29 p comp… ## 2 audi a4 2 2008 4 auto… f 21 30 p comp… ## 3 audi a4 2.8 1999 6 auto… f 16 26 p comp… ## 4 audi a4 3.1 2008 6 auto… f 18 27 p comp… ## 5 audi a4 quattro 1.8 1999 4 auto… 4 16 25 p comp… ## 6 audi a4 quattro 2 2008 4 auto… 4 19 27 p comp… ## 7 audi a4 quattro 2.8 1999 6 auto… 4 15 25 p comp… ## 8 audi a4 quattro 3.1 2008 6 auto… 4 17 25 p comp… ## 9 audi a6 quattro 2.8 1999 6 auto… 4 15 24 p mids… ## 10 audi a6 quattro 3.1 2008 6 auto… 4 17 25 p mids… ## # … with 147 more rows 34.1.2 q1 Filter the mpg dataset down to manual vehicles using str_detect(). df_q1 &lt;- mpg %&gt;% filter(str_detect(trans, &quot;manual&quot;)) df_q1 %&gt;% glimpse() ## Rows: 77 ## Columns: 11 ## $ manufacturer &lt;chr&gt; &quot;audi&quot;, &quot;audi&quot;, &quot;audi&quot;, &quot;audi&quot;, &quot;audi&quot;, &quot;audi&quot;, &quot;audi&quot;, &quot;… ## $ model &lt;chr&gt; &quot;a4&quot;, &quot;a4&quot;, &quot;a4&quot;, &quot;a4 quattro&quot;, &quot;a4 quattro&quot;, &quot;a4 quattro… ## $ displ &lt;dbl&gt; 1.8, 2.0, 2.8, 1.8, 2.0, 2.8, 3.1, 5.7, 6.2, 7.0, 3.7, 3.… ## $ year &lt;int&gt; 1999, 2008, 1999, 1999, 2008, 1999, 2008, 1999, 2008, 200… ## $ cyl &lt;int&gt; 4, 4, 6, 4, 4, 6, 6, 8, 8, 8, 6, 6, 8, 8, 8, 8, 8, 6, 6, … ## $ trans &lt;chr&gt; &quot;manual(m5)&quot;, &quot;manual(m6)&quot;, &quot;manual(m5)&quot;, &quot;manual(m5)&quot;, &quot;… ## $ drv &lt;chr&gt; &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;4&quot;, &quot;4&quot;, &quot;4&quot;, &quot;4&quot;, &quot;r&quot;, &quot;r&quot;, &quot;r&quot;, &quot;4&quot;, &quot;4… ## $ cty &lt;int&gt; 21, 20, 18, 18, 20, 17, 15, 16, 16, 15, 15, 14, 11, 12, 1… ## $ hwy &lt;int&gt; 29, 31, 26, 26, 28, 25, 25, 26, 26, 24, 19, 17, 17, 16, 1… ## $ fl &lt;chr&gt; &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;r&quot;, &quot;r… ## $ class &lt;chr&gt; &quot;compact&quot;, &quot;compact&quot;, &quot;compact&quot;, &quot;compact&quot;, &quot;compact&quot;, &quot;c… Use the following test to check your work. ## NOTE: No need to change this! assertthat::assert_that( all( df_q1 %&gt;% pull(trans) %&gt;% str_detect(., &quot;manual&quot;) ) ) ## [1] TRUE print(&quot;Great job!&quot;) ## [1] &quot;Great job!&quot; Part of the power of learning regular expressions is that we can write patterns, rather than exact matches. Notice that the drv variable in mpg takes either character or digit values. What if we wanted to filter out all the cases that had digits? mpg %&gt;% filter( !str_detect(drv, &quot;\\\\d&quot;) ) %&gt;% glimpse() ## Rows: 131 ## Columns: 11 ## $ manufacturer &lt;chr&gt; &quot;audi&quot;, &quot;audi&quot;, &quot;audi&quot;, &quot;audi&quot;, &quot;audi&quot;, &quot;audi&quot;, &quot;audi&quot;, &quot;… ## $ model &lt;chr&gt; &quot;a4&quot;, &quot;a4&quot;, &quot;a4&quot;, &quot;a4&quot;, &quot;a4&quot;, &quot;a4&quot;, &quot;a4&quot;, &quot;c1500 suburban… ## $ displ &lt;dbl&gt; 1.8, 1.8, 2.0, 2.0, 2.8, 2.8, 3.1, 5.3, 5.3, 5.3, 5.7, 6.… ## $ year &lt;int&gt; 1999, 1999, 2008, 2008, 1999, 1999, 2008, 2008, 2008, 200… ## $ cyl &lt;int&gt; 4, 4, 4, 4, 6, 6, 6, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 4, 4, … ## $ trans &lt;chr&gt; &quot;auto(l5)&quot;, &quot;manual(m5)&quot;, &quot;manual(m6)&quot;, &quot;auto(av)&quot;, &quot;auto… ## $ drv &lt;chr&gt; &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;r&quot;, &quot;r&quot;, &quot;r&quot;, &quot;r&quot;, &quot;r… ## $ cty &lt;int&gt; 18, 21, 20, 21, 16, 18, 18, 14, 11, 14, 13, 12, 16, 15, 1… ## $ hwy &lt;int&gt; 29, 29, 31, 30, 26, 26, 27, 20, 15, 20, 17, 17, 26, 23, 2… ## $ fl &lt;chr&gt; &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;r&quot;, &quot;e&quot;, &quot;r&quot;, &quot;r&quot;, &quot;r… ## $ class &lt;chr&gt; &quot;compact&quot;, &quot;compact&quot;, &quot;compact&quot;, &quot;compact&quot;, &quot;compact&quot;, &quot;c… Recall (from the reading) that \\d is a regular expression referring to a single digit. However, a trick thing about R is that we have to double the slash \\\\ in order to get the correct behavior [1]. 34.1.3 q2 Use str_detect() and an appropriate regular expression to filter mpg for only those values of trans that have a digit. df_q2 &lt;- mpg %&gt;% filter(str_detect(trans, &quot;\\\\d&quot;)) df_q2 %&gt;% glimpse() ## Rows: 229 ## Columns: 11 ## $ manufacturer &lt;chr&gt; &quot;audi&quot;, &quot;audi&quot;, &quot;audi&quot;, &quot;audi&quot;, &quot;audi&quot;, &quot;audi&quot;, &quot;audi&quot;, &quot;… ## $ model &lt;chr&gt; &quot;a4&quot;, &quot;a4&quot;, &quot;a4&quot;, &quot;a4&quot;, &quot;a4&quot;, &quot;a4 quattro&quot;, &quot;a4 quattro&quot;,… ## $ displ &lt;dbl&gt; 1.8, 1.8, 2.0, 2.8, 2.8, 1.8, 1.8, 2.0, 2.0, 2.8, 2.8, 3.… ## $ year &lt;int&gt; 1999, 1999, 2008, 1999, 1999, 1999, 1999, 2008, 2008, 199… ## $ cyl &lt;int&gt; 4, 4, 4, 6, 6, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6, 8, 8, 8, 8, … ## $ trans &lt;chr&gt; &quot;auto(l5)&quot;, &quot;manual(m5)&quot;, &quot;manual(m6)&quot;, &quot;auto(l5)&quot;, &quot;manu… ## $ drv &lt;chr&gt; &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;4&quot;, &quot;4&quot;, &quot;4&quot;, &quot;4&quot;, &quot;4&quot;, &quot;4&quot;, &quot;4… ## $ cty &lt;int&gt; 18, 21, 20, 16, 18, 18, 16, 20, 19, 15, 17, 17, 15, 15, 1… ## $ hwy &lt;int&gt; 29, 29, 31, 26, 26, 26, 25, 28, 27, 25, 25, 25, 25, 24, 2… ## $ fl &lt;chr&gt; &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p… ## $ class &lt;chr&gt; &quot;compact&quot;, &quot;compact&quot;, &quot;compact&quot;, &quot;compact&quot;, &quot;compact&quot;, &quot;c… Use the following test to check your work. ## NOTE: No need to change this! assertthat::assert_that( all( df_q2 %&gt;% pull(trans) %&gt;% str_detect(., &quot;\\\\d&quot;) ) ) ## [1] TRUE print(&quot;Nice!&quot;) ## [1] &quot;Nice!&quot; 34.1.4 Extract While str_detect() is useful for filtering, str_extract() is useful with mutate(). This function returns the first extracted substring, as demonstrated below. ## NOTE: No need to change this! str_extract( string = c(&quot;abc&quot;, &quot;xyz&quot;, &quot;123&quot;), pattern = &quot;\\\\d{3}&quot; ) ## [1] NA NA &quot;123&quot; Note that if str_extract() doesn’t find a extract, it will return NA. Also, here that I’m using a quantifier; as we saw in the reading, {} notation will allow us to specify the number of repetitions to seek. ## NOTE: No need to change this! str_extract( string = c(&quot;abc&quot;, &quot;xyz&quot;, &quot;123&quot;), pattern = &quot;\\\\d{2}&quot; ) ## [1] NA NA &quot;12&quot; Notice that this only returns the first two digits in the extract, and neglects the third. If we don’t know the specific number we’re looking for, we can use + to select one or more characters: ## NOTE: No need to change this! str_extract( string = c(&quot;abc&quot;, &quot;xyz&quot;, &quot;123&quot;), pattern = &quot;\\\\d+&quot; ) ## [1] NA NA &quot;123&quot; We can also use the [[:alpha:]] special symbol to select alphabetic characters only: ## NOTE: No need to change this! str_extract( string = c(&quot;abc&quot;, &quot;xyz&quot;, &quot;123&quot;), pattern = &quot;[[:alpha:]]+&quot; ) ## [1] &quot;abc&quot; &quot;xyz&quot; NA And finally the wildcard . allows us to match any character: ## NOTE: No need to change this! str_extract( string = c(&quot;abc&quot;, &quot;xyz&quot;, &quot;123&quot;), pattern = &quot;.+&quot; ) ## [1] &quot;abc&quot; &quot;xyz&quot; &quot;123&quot; 34.1.5 q3 Match alphabet characters Notice that the trans column of mpg has many entries of the form auto|manual\\\\([[:alpha:]]\\\\d\\\\); use str_mutate() to create a new column tmp with just the code inside the parentheses extracting [[:alpha:]]\\\\d. ## TASK: Mutate `trans` to extract df_q3 &lt;- mpg %&gt;% mutate(tmp = str_extract(trans, &quot;[[:alpha:]]\\\\d&quot;)) df_q3 %&gt;% select(tmp) ## # A tibble: 234 × 1 ## tmp ## &lt;chr&gt; ## 1 l5 ## 2 m5 ## 3 m6 ## 4 &lt;NA&gt; ## 5 l5 ## 6 m5 ## 7 &lt;NA&gt; ## 8 m5 ## 9 l5 ## 10 m6 ## # … with 224 more rows Use the following test to check your work. ## NOTE: No need to change this! assertthat::assert_that( (df_q3 %&gt;% filter(is.na(tmp)) %&gt;% dim(.) %&gt;% .[[1]]) == 5 ) ## [1] TRUE print(&quot;Well done!&quot;) ## [1] &quot;Well done!&quot; 34.1.6 Match and Capture Groups The str_match() function is similar to str_extract(), but it allows us to specify multiple “pieces” of a string to match with capture groups. A capture group is a pattern within parentheses; for instance, imagine we were trying to parse phone numbers, all with different formatting. We could use three capture groups for the three pieces of the phone number: ## NOTE: No need to edit; execute phone_numbers &lt;- c( &quot;(814) 555 1234&quot;, &quot;650-555-1234&quot;, &quot;8005551234&quot; ) str_match( phone_numbers, &quot;(\\\\d{3}).*(\\\\d{3}).*(\\\\d{4})&quot; ) ## [,1] [,2] [,3] [,4] ## [1,] &quot;814) 555 1234&quot; &quot;814&quot; &quot;555&quot; &quot;1234&quot; ## [2,] &quot;650-555-1234&quot; &quot;650&quot; &quot;555&quot; &quot;1234&quot; ## [3,] &quot;8005551234&quot; &quot;800&quot; &quot;555&quot; &quot;1234&quot; Remember that the . character is a wildcard. Here I use the * quantifier for zero or more instances; this takes care of cases where there is no gap between characters, or when there are spaces or dashes between. 34.1.7 q4 Modify the pattern below to extract the x, y pairs separately. ## NOTE: No need to edit this setup points &lt;- c( &quot;x=1, y=2&quot;, &quot;x=3, y=2&quot;, &quot;x=10, y=4&quot; ) q4 &lt;- str_match( points, pattern = &quot;x=(\\\\d+), y=(\\\\d+)&quot; ) q4 ## [,1] [,2] [,3] ## [1,] &quot;x=1, y=2&quot; &quot;1&quot; &quot;2&quot; ## [2,] &quot;x=3, y=2&quot; &quot;3&quot; &quot;2&quot; ## [3,] &quot;x=10, y=4&quot; &quot;10&quot; &quot;4&quot; Use the following test to check your work. ## NOTE: No need to change this! assertthat::assert_that( all( q4[, -1] == t(matrix(as.character(c(1, 2, 3, 2, 10, 4)), nrow = 2)) ) ) ## [1] TRUE print(&quot;Excellent!&quot;) ## [1] &quot;Excellent!&quot; 34.2 Removal One last stringr function that’s helpful to know: str_remove() will simply remove the first matched pattern in a string. This is particularly helpful for dealing with prefixes and suffixes. ## NOTE: No need to edit; execute string_quantiles &lt;- c( &quot;q0.01&quot;, &quot;q0.5&quot;, &quot;q0.999&quot; ) string_quantiles %&gt;% str_remove(., &quot;q&quot;) %&gt;% as.numeric() ## [1] 0.010 0.500 0.999 34.2.1 q5 Use str_remove() to get mutate trans to remove the parentheses and all characters between. Hint: Note that parentheses are special characters, so you’ll need to escape them as you did above. df_q5 &lt;- mpg %&gt;% mutate(trans = str_remove(trans, &quot;\\\\(.*\\\\)&quot;)) df_q5 ## # A tibble: 234 × 11 ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 audi a4 1.8 1999 4 auto f 18 29 p comp… ## 2 audi a4 1.8 1999 4 manu… f 21 29 p comp… ## 3 audi a4 2 2008 4 manu… f 20 31 p comp… ## 4 audi a4 2 2008 4 auto f 21 30 p comp… ## 5 audi a4 2.8 1999 6 auto f 16 26 p comp… ## 6 audi a4 2.8 1999 6 manu… f 18 26 p comp… ## 7 audi a4 3.1 2008 6 auto f 18 27 p comp… ## 8 audi a4 quattro 1.8 1999 4 manu… 4 18 26 p comp… ## 9 audi a4 quattro 1.8 1999 4 auto 4 16 25 p comp… ## 10 audi a4 quattro 2 2008 4 manu… 4 20 28 p comp… ## # … with 224 more rows Use the following test to check your work. ## NOTE: No need to change this! assertthat::assert_that( all( df_q5 %&gt;% pull(trans) %&gt;% str_detect(., &quot;\\\\(.*\\\\)&quot;) %&gt;% !. ) ) ## [1] TRUE print(&quot;Well done!&quot;) ## [1] &quot;Well done!&quot; 34.3 Regex in Other Functions Now we’re going to put all these ideas together—special characters, quantifiers, and capture groups—in order to solve a data tidying issue. Other functions like pivot_longer and pivot_wider also take regex patterns. We can use these to help solve data tidying problems. Let’s return to the alloy data from e-data03-pivot-basics; the version of the data below do not have the convenient _ separators in the column names. ## NOTE: No need to edit; execute alloys &lt;- tribble( ~thick, ~E00, ~mu00, ~E45, ~mu45, ~rep, 0.022, 10600, 0.321, 10700, 0.329, 1, 0.022, 10600, 0.323, 10500, 0.331, 2, 0.032, 10400, 0.329, 10400, 0.318, 1, 0.032, 10300, 0.319, 10500, 0.326, 2 ) alloys ## # A tibble: 4 × 6 ## thick E00 mu00 E45 mu45 rep ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.022 10600 0.321 10700 0.329 1 ## 2 0.022 10600 0.323 10500 0.331 2 ## 3 0.032 10400 0.329 10400 0.318 1 ## 4 0.032 10300 0.319 10500 0.326 2 As described in the RegexOne tutorial, you can use capture groups in parentheses (...) to define different groups in your regex pattern. These can be used along with the pivot_ functions, for instance when you want to break apart column names into multiple groups. 34.3.1 q6 Use your knowledge of regular expressions along with the names_pattern argument to successfully tidy the alloys data. ## TASK: Tidy `alloys` df_q6 &lt;- alloys %&gt;% pivot_longer( names_to = c(&quot;property&quot;, &quot;angle&quot;), names_pattern = &quot;([[:alpha:]]+)(\\\\d+)&quot;, values_to = &quot;value&quot;, cols = matches(&quot;\\\\d&quot;) ) %&gt;% mutate(angle = as.integer(angle)) df_q6 ## # A tibble: 16 × 5 ## thick rep property angle value ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0.022 1 E 0 10600 ## 2 0.022 1 mu 0 0.321 ## 3 0.022 1 E 45 10700 ## 4 0.022 1 mu 45 0.329 ## 5 0.022 2 E 0 10600 ## 6 0.022 2 mu 0 0.323 ## 7 0.022 2 E 45 10500 ## 8 0.022 2 mu 45 0.331 ## 9 0.032 1 E 0 10400 ## 10 0.032 1 mu 0 0.329 ## 11 0.032 1 E 45 10400 ## 12 0.032 1 mu 45 0.318 ## 13 0.032 2 E 0 10300 ## 14 0.032 2 mu 0 0.319 ## 15 0.032 2 E 45 10500 ## 16 0.032 2 mu 45 0.326 Use the following test to check your work. ## NOTE: No need to change this! assertthat::assert_that(dim(df_q6)[1] == 16) ## [1] TRUE assertthat::assert_that(dim(df_q6)[2] == 5) ## [1] TRUE print(&quot;Awesome!&quot;) ## [1] &quot;Awesome!&quot; 34.4 Notes [1] This is because \\ has a special meaning in R, and we need to “escape” the slash by doubling it \\\\. "],["vis-themes.html", "35 Vis: Themes", " 35 Vis: Themes Purpose: Themes are key for aesthetic purposes; to make really good-looking graphs, we’ll need to use theme(). Reading: theme() documentation (Use as a reference; don’t read the whole thing!) 35.0.1 q1 Use theme_void() and guides() (with an argument) to remove everything in this plot except the points. mpg %&gt;% ggplot(aes(displ, hwy, color = class)) + geom_point() mpg %&gt;% ggplot(aes(displ, hwy, color = class)) + geom_point() + guides(color = &quot;none&quot;) + theme_void() When I make presentation-quality figures, I often start with the following stub code: ## NOTE: No need to edit; feel free to re-use this code! theme_common &lt;- function() { theme_minimal() %+replace% theme( axis.text.x = element_text(size = 12), axis.text.y = element_text(size = 12), axis.title.x = element_text(margin = margin(4, 4, 4, 4), size = 16), axis.title.y = element_text(margin = margin(4, 4, 4, 4), size = 16, angle = 90), legend.title = element_text(size = 16), legend.text = element_text(size = 12), strip.text.x = element_text(size = 12), strip.text.y = element_text(size = 12), panel.grid.major = element_line(color = &quot;grey90&quot;), panel.grid.minor = element_line(color = &quot;grey90&quot;), aspect.ratio = 4 / 4, plot.margin = unit(c(t = +0, b = +0, r = +0, l = +0), &quot;cm&quot;), plot.title = element_text(size = 18), plot.title.position = &quot;plot&quot;, plot.subtitle = element_text(size = 16), plot.caption = element_text(size = 12) ) } The %+replace magic above allows you to use theme_common() within your own ggplot calls. 35.0.2 q2 Use theme_common() with the following graph. Document what’s changed by the theme() arguments. mpg %&gt;% ggplot(aes(displ, hwy, color = class)) + geom_point() + labs( x = &quot;Engine Displacement (L)&quot;, y = &quot;Highway Fuel Economy (mpg)&quot; ) mpg %&gt;% ggplot(aes(displ, hwy, color = class)) + geom_point() + theme_common() + labs( x = &quot;Engine Displacement (L)&quot;, y = &quot;Highway Fuel Economy (mpg)&quot; ) Observations: The text is larger, hence more readable The background was flipped grey to white The guide lines have been flipped from white to grey Calling theme_common(), along with settings labs() and making some smart choices about geoms and annotations is often all you need to make a really high-quality graph. 35.0.3 q3 Make the following plot as ugly as possible; the more theme() arguments you use, the better! Hint: Use the theme() settings from q2 above as a starting point, and read the documentation for theme() to learn how to do more horrible things to this graph. mpg %&gt;% ggplot(aes(displ, hwy, color = class)) + geom_point() + theme( axis.text.x = element_text(size = 32) ) Here’s one possible graph: mpg %&gt;% ggplot(aes(displ, hwy, color = class)) + geom_point() + guides(color = &quot;none&quot;) + theme( line = element_line(size = 3, color = &quot;purple&quot;), rect = element_rect(fill = &quot;red&quot;), axis.text.x = element_text(size = 32, angle = 117), axis.text.y = element_text(size = 32, angle = 129), axis.title.x = element_text(size = 32, family = &quot;Comic Sans MS&quot;), axis.title.y = element_text(size = 32, family = &quot;Comic Sans MS&quot;) ) ## Warning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0. ## ℹ Please use the `linewidth` argument instead. "],["data-pipes-and-placeholders.html", "36 Data: Pipes and Placeholders", " 36 Data: Pipes and Placeholders Purpose: The pipe %&gt;% has additional functionality than what we’ve used so far. In this exercise we’ll learn about the placeholder ., which will give us more control over how data flows between our functions. Reading: The Pipe library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✔ ggplot2 3.4.0 ✔ purrr 1.0.1 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.5.0 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() 36.0.1 q1 Re-write the following code to use the placeholder. Hint: This may feel very simple, in which case good. This is not a trick question. diamonds %&gt;% glimpse(.) ## Rows: 53,940 ## Columns: 10 ## $ carat &lt;dbl&gt; 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, 0.23, 0.… ## $ cut &lt;ord&gt; Ideal, Premium, Good, Premium, Good, Very Good, Very Good, Ver… ## $ color &lt;ord&gt; E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J, J, J, I,… ## $ clarity &lt;ord&gt; SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, SI1, VS1, … ## $ depth &lt;dbl&gt; 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, 59.4, 64… ## $ table &lt;dbl&gt; 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54, 62, 58… ## $ price &lt;int&gt; 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339, 340, 34… ## $ x &lt;dbl&gt; 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, 4.00, 4.… ## $ y &lt;dbl&gt; 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, 4.05, 4.… ## $ z &lt;dbl&gt; 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, 2.39, 2.… 36.0.2 q2 Fix the lambda expression The reading discussed Using lambda expressions with %&gt;%; use this part of the reading to explain why the following code fails. Then fix the code so it runs without error. 2 %&gt;% {. * .} ## [1] 4 36.0.3 q3 Re-write the following code using the placeholder . operator to simplify the second filter. Hint: You should be able to simplify the second call to filter down to just filter(cut == \"Fair\"). diamonds %&gt;% filter(carat &lt;= 0.3) %&gt;% ggplot(aes(carat, price)) + geom_point() + geom_point( data = . %&gt;% filter(cut == &quot;Fair&quot;), color = &quot;red&quot; ) The placeholder even works at “later” points in a pipeline. We can use it to help simplify code, as you did above. "],["stats-populations-and-estimation.html", "37 Stats: Populations and Estimation 37.1 Population 37.2 Estimates 37.3 Intermediate conclusion 37.4 Standard Error 37.5 Fast Summary", " 37 Stats: Populations and Estimation Purpose: Often, our data do not include all the facts that are relevant to the decision we are trying to make. Statistics is the science of determining the conclusions we can confidently make, based on our available data. To make sense of this, we need to understand the distinction between a sample and a population, and how this distinction leads to estimation. Reading: Statistician proves that statistics are boring Topics: Population, sample, estimate, sampling distribution, standard error library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✔ ggplot2 3.4.0 ✔ purrr 1.0.1 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.5.0 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() library(nycflights13) When using descriptive statistics to help us answer a question, there are (at least) two questions we should ask ourselves: Does the statistic we chose relate to the problem we care about? Do we have all the facts we need (the population) or do we have limited information (a sample from some well-defined population)? We already discussed (1) by learning about descriptive statistics and their meaning. Now we’ll discuss (2) by learning the distinction between populations and samples. 37.1 Population Let’s start by looking at an artificial population: ## NOTE: No need to change this! tibble(z = seq(-4, +4, length.out = 500)) %&gt;% mutate(d = dnorm(z)) %&gt;% ggplot(aes(z, d)) + geom_line() Here our population is an infinite pool of observations all following the standard normal distribution. If this sounds abstract and unrealistic, good! Remember that the normal distribution (and indeed all named distributions) are abstract, mathematical objects that we use to model real phenomena. Remember that a sample is a set of observations “drawn” from the population. The following is an example of three different samples from the same normal distribution, with different sample sizes. ## NOTE: No need to change this! set.seed(101) tibble(z = seq(-4, +4, length.out = 500)) %&gt;% mutate(d = dnorm(z)) %&gt;% ggplot() + geom_histogram( data = map_dfr( c(10, 1e2, 1e3), function(n) {tibble(Z = rnorm(n), n = n)} ), mapping = aes(Z, y = ..density.., color = &quot;Sample&quot;) ) + geom_line(aes(z, d, color = &quot;Population&quot;)) + facet_grid(~n) ## Warning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0. ## ℹ Please use `after_stat(density)` instead. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. As we’ve seen before, as we draw more samples, their histogram tends to look more like the underlying population. Now let’s look at a real example of a population: ## NOTE: No need to change this! flights %&gt;% ggplot() + geom_freqpoly(aes(air_time, color = &quot;(All)&quot;)) + geom_freqpoly(aes(air_time, color = origin)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 9430 rows containing non-finite values (`stat_bin()`). ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 9430 rows containing non-finite values (`stat_bin()`). This is the set of all flights originating from EWR, LGA, and JFK in 2013, in terms of their air_time. Note that this distribution is decidedly not normal; we would be foolish to try to model it as such! As we saw in the reading, the choice of the “correct” population is not an exercise in math. This is a decision that you must make based on the problem you are trying to solve. For instance, if we care about all flights into the NYC area, then the (All) population is correct. But if we care only about flights out of LGA, the population is different. No amount of math can save you if you can’t pick the appropriate population for your problem! When your data are not the entire population, any statistic you compute is an estimate. 37.2 Estimates When we don’t have all the facts and instead only have a sample, we perform estimation to extrapolate from our available data to the population we care about. The following code draws multiple samples from a standard normal of size n_observations, and does so n_samples times. We’ll visualize these data in a later chunk. ## NOTE: No need to change this! n_observations &lt;- 3 n_samples &lt;- 5e3 df_sample &lt;- map_dfr( 1:n_samples, function(id) { tibble( Z = rnorm(n_observations), id = id ) } ) Some terminology: We call a statistic of a population a population statistic; for instance, the population mean. A population statistic is also called a parameter. We call a statistics of a sample a sample statistic; for instance, the sample mean. A sample statistic is also called an estimate. The chunk compute-samples generated n_samples = 5e3 of n_observations = 3 each. You can think of each sample as an “alternative universe” where we happened to pick 3 particular values. The following chunk visualizes just the first few samples: df_sample %&gt;% filter(id &lt;= 6) %&gt;% ggplot(aes(Z, &quot;&quot;)) + geom_point() + facet_grid(id ~ .) + labs( x = &quot;Realized Values&quot;, y = &quot;Samples&quot; ) Every one of these samples has its own sample mean; let’s add that as an additional point: df_sample %&gt;% filter(id &lt;= 6) %&gt;% ggplot(aes(Z, &quot;&quot;)) + geom_point() + geom_point( data = . %&gt;% group_by(id) %&gt;% summarize(Z = mean(Z)), mapping = aes(color = &quot;Sample Mean&quot;), size = 4 ) + scale_color_discrete(name = &quot;&quot;) + theme(legend.position = &quot;bottom&quot;) + facet_grid(id ~ .) + labs( x = &quot;Realized Values&quot;, y = &quot;Samples&quot; ) Thus, there is a “red dot” associated with each of the 5,000 samples. Let’s visualize the individual sample mean values as a histogram: ## NOTE: No need to change this! df_sample %&gt;% group_by(id) %&gt;% summarize(mean = mean(Z)) %&gt;% ggplot(aes(mean)) + geom_histogram() + geom_vline(xintercept = 0, linetype = 2) + labs( x = &quot;Sample Mean&quot; ) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Remember that the standard normal has population mean zero (vertical line); the distribution we see here is of the sample mean values. These results indicate that we frequently land near zero (the true population value) but we obtain values as wide as -2 and +2. This is because we have limited data from our population, and our estimate is not guaranteed to be close to its population value. As we gather more data, we’ll tend to produce better estimates. To illustrate the effects of more data, I use a little mathematical theory to quickly visualize estimates of the mean at different sample sizes. ## NOTE: No need to change this! map_dfr( c(3, 12, 48, 192), function(n) { tibble(z = seq(-4, +4, length.out = 500)) %&gt;% mutate( d = dnorm(z, sd = 1 / sqrt(n)), n = n ) } ) %&gt;% ggplot() + geom_line(aes(z, d, color = as.factor(n), group = as.factor(n))) + scale_color_discrete(name = &quot;Samples&quot;) + labs( x = &quot;Estimated Mean&quot;, title = &quot;Sampling Distributions: Estimated Mean&quot;, caption = &quot;Population: Normal&quot; ) As we might expect, the distribution of estimated means concentrates on the true value of zero as we increase the sample size \\(n\\). As we gather more data, our estimate has a greater probability of landing close to the true value. The distribution for an estimate is called a sampling distribution; the visualization above is a lineup of sampling distributions for the estimated mean. It happens that all of those distributions are normal. However, the sampling distribution is not guaranteed to look like the underlying population. For example, let’s look at the sample standard deviation. ## NOTE: No need to change this! df_sample %&gt;% group_by(id) %&gt;% summarize(sd = sd(Z)) %&gt;% ggplot(aes(sd)) + geom_histogram() + labs( x = &quot;Estimated Standard Deviation&quot; ) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Note that this doesn’t look much like a normal distribution. This should make some intuitive sense: The standard deviation is guaranteed to be non-negative, so it can’t possibly follow a normal distribution, which can take values anywhere from \\(-\\infty\\) to \\(+\\infty\\). 37.2.1 q1 Modify the code below to draw samples from a uniform distribution (rather than a normal). Describe (in words) what the resulting sampling distribution looks like. Does the sampling distribution look like a normal distribution? ## TASK: Modify the code below to sample from a uniform distribution df_samp_unif &lt;- map_dfr( 1:n_samples, function(id) { tibble( Z = runif(n_observations), id = id ) } ) df_samp_unif %&gt;% group_by(id) %&gt;% summarize(stat = mean(Z)) %&gt;% ggplot(aes(stat)) + geom_histogram() + labs( x = &quot;Estimated Mean&quot;, title = &quot;Sampling Distribution: Estimated Mean&quot;, caption = &quot;Population: Uniform&quot; ) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Observations: 37.3 Intermediate conclusion A sampling distribution is the distribution for a sample estimate. It is induced by the population, but is also a function of the specific statistic we’re considering. We will use statistics to help make sense of sampling distributions. 37.4 Standard Error The standard deviation of a sampling distribution gets a special name: the standard error. The standard error of an estimated mean is \\[\\text{SE} = \\sigma / \\sqrt{n}.\\] This is a formula worth memorizing; it implies that doubling the precision of an estimated mean requires quadrupling the sample size. It also tells us that a more variable population (larger \\(\\sigma\\)) will make estimation more difficult (larger \\(\\text{SE}\\)). The standard error is a convenient way to summarize the accuracy of an estimation setting; the larger our standard error, the less accurate our estimates will tend to be. 37.4.1 q2 Compute the standard error for the sample mean under the following settings. Which setting will tend to produce more accurate estimates? ## TASK: Compute the standard error se_q2.1 &lt;- 4 / sqrt(16) se_q2.2 &lt;- 8 / sqrt(32) Use the following tests to check your work. ## NOTE: No need to change this! assertthat::assert_that(assertthat::are_equal(se_q2.1, 1)) ## [1] TRUE assertthat::assert_that(assertthat::are_equal(se_q2.2, sqrt(2))) ## [1] TRUE print(&quot;Nice!&quot;) ## [1] &quot;Nice!&quot; Observations: Setting q2.1 will tend to be more accurate because its standard error is lower Two notes: Note the language above: The standard error tells us about settings (population \\(\\sigma\\) and sample count \\(n\\)), not estimates themselves. The accuracy of an individual estimate would depend on \\(\\hat{\\mu} - \\mu\\), but we in practice never know \\(\\mu\\) exactly. The standard error will tell us how variable \\(\\hat{\\mu}\\) will be on average, but does not give us any information about the specific value of \\(\\hat{\\mu} - \\mu\\) for any given estimate \\(\\hat{\\mu}\\). The standard error gives us an idea of how accurate our estimate will tend to be, but due to randomness we don’t know the true accuracy of our estimate. Note that we used the population standard deviation above; in practice we’ll only have a sample standard deviation. In this case we can use a plug-in estimate for the standard error \\[\\hat{\\text{SE}} = s / \\sqrt{n},\\] where the hat on \\(\\text{SE}\\) denotes that this quantity is an estimate, and \\(s\\) is the sample standard deviation. 37.4.2 q3 Compute the sample standard error of the sample mean for the sample below. Compare your estimate against the true value se_q2.1. State how similar or different the values are, and explain the difference. ## NOTE: No need to change this! set.seed(101) n_sample &lt;- 20 z_sample &lt;- rnorm(n = n_sample, mean = 2, sd = 4) ## TASK: Compute the sample standard error for `z_sample` se_sample &lt;- sd(z_sample) / sqrt(n_sample) Use the following tests to check your work. ## NOTE: No need to change this! assertthat::assert_that( assertthat::are_equal( se_sample, sd(z_sample) / sqrt(n_sample) ) ) ## [1] TRUE print(&quot;Well done!&quot;) ## [1] &quot;Well done!&quot; Observations: I find that se_sample is about 3/4 of the true value, which is a large difference. The value se_sample is just an estimate; it is inaccurate due to randomness. 37.5 Fast Summary The population is the set of all things we care about. No amount of math can help you here: You are responsible for defining your population. If we have the whole population, we don’t need statistics! When we don’t have all the data from the population, we need to estimate. The combined effects of random sampling, the shape of the population, and our chosen statistic all give rise to a sampling distribution for our estimated statistic. The standard deviation of the sampling distribution is called the standard error; it is a measure of accuracy of the sampling procedure, not the estimate itself. "],["data-window-functions.html", "38 Data: Window Functions 38.1 Lead and Lag 38.2 Ranks", " 38 Data: Window Functions Purpose: Window functions are another family of dplyr verbs that are related to aggregates like mean and sd. These functions are useful for building up more complicated filters, enabling aesthetic tricks in plots, and some advanced data wrangling we’ll do next exercise. Reading: Window Functions, Types of window functions, Ranking functions, and Lead and lag library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✔ ggplot2 3.4.0 ✔ purrr 1.0.1 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.5.0 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() library(ggrepel) 38.1 Lead and Lag The lead and lag functions simply provide a “shifted” copy of a vector. ## NOTE: No need to edit this; just an example v &lt;- c(1, 2, 3, 4, 5) lead(v) ## [1] 2 3 4 5 NA lag(v) ## [1] NA 1 2 3 4 These are particularly useful for computing things like differences: ## NOTE: No need to edit this; just an example x &lt;- seq(-1, +1, length.out = 6) f &lt;- x ^ 2 ## Forward finite difference df_dx &lt;- (lead(f) - f) / (lead(x) - x) df_dx ## [1] -1.600000e+00 -8.000000e-01 2.255141e-16 8.000000e-01 1.600000e+00 ## [6] NA Make sure to order your data or use the order_by argument when using lead or lag! GGplot automatically reorders your data when making a line plot, but lead and lag will use the order of the data you provide. 38.1.1 q1 Use a window function modify the following visual to color each segment differently based on whether the period of time was increasing or decreasing. economics %&gt;% arrange(date) %&gt;% mutate( delta = lead(unemploy, order_by = date) - unemploy, Positive = delta &gt; 0 ) %&gt;% ggplot(aes(date, unemploy, color = Positive)) + geom_segment(aes( xend = lead(date, order_by = date), yend = lead(unemploy, order_by = date) )) ## Warning: Removed 1 rows containing missing values (`geom_segment()`). 38.2 Ranks The rank functions allow you to assign (integer) ranks to smallest (or largest) values of a vector. ## NOTE: No need to edit this; just an example v &lt;- c(1, 1, 2, 3, 5) row_number(v) ## [1] 1 2 3 4 5 min_rank(v) ## [1] 1 1 3 4 5 dense_rank(v) ## [1] 1 1 2 3 4 You can use the desc() function (or a negative sign) to reverse the ranking order. ## NOTE: No need to edit this; just an example v &lt;- c(1, 1, 2, 3, 5) row_number(desc(v)) ## [1] 4 5 3 2 1 min_rank(desc(v)) ## [1] 4 4 3 2 1 dense_rank(-v) ## [1] 4 4 3 2 1 I find it difficult to remember how the rank functions behave, so I created the following visual to help remind myself how they function.. ## NOTE: No need to edit this; just an example set.seed(101) tribble( ~x, ~y, 0, 0, 1, 0, 1, 1, 0, 2, 2, 2, 0, 3, 2, 3, 3, 3 ) %&gt;% mutate( rk_row = row_number(y), rk_min = min_rank(y), rk_dense = dense_rank(y) ) %&gt;% pivot_longer( names_to = &quot;fcn&quot;, names_prefix = &quot;rk_&quot;, values_to = &quot;rk&quot;, cols = c(-x, -y) ) %&gt;% ggplot(aes(x, y)) + geom_point(size = 4) + geom_point( data = . %&gt;% filter(rk &lt;= 3), size = 3, color = &quot;orange&quot; ) + geom_label(aes(label = rk), nudge_x = 0.2, nudge_y = 0.2) + facet_wrap(~fcn) + theme_minimal() + theme(panel.border = element_rect(color = &quot;black&quot;, fill = NA, size = 1)) + labs( x = &quot;&quot;, y = &quot;Minimum Three Ranks&quot; ) ## Warning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0. ## ℹ Please use the `linewidth` argument instead. 38.2.1 q2 Use a rank function to filter the largest 3 hwy values and all vehicles that have those values. q2 &lt;- mpg %&gt;% filter(dense_rank(desc(hwy)) &lt;= 3) q2 ## # A tibble: 4 × 11 ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 toyota corolla 1.8 2008 4 manua… f 28 37 r comp… ## 2 volkswagen jetta 1.9 1999 4 manua… f 33 44 d comp… ## 3 volkswagen new beetle 1.9 1999 4 manua… f 35 44 d subc… ## 4 volkswagen new beetle 1.9 1999 4 auto(… f 29 41 d subc… Use the following test to check your work. ## NOTE: No need to change this assertthat::assert_that(dim(q2)[1] == 4) ## [1] TRUE print(&quot;Nice!&quot;) ## [1] &quot;Nice!&quot; "],["stats-moment-arithmetic.html", "39 Stats: Moment Arithmetic 39.1 Moments 39.2 Moment Arithmetic 39.3 Expectation 39.4 Variance 39.5 Standardization 39.6 Standard Error", " 39 Stats: Moment Arithmetic Purpose: In a future exercise, we will need to be able to do some basic arithmetic with moments of a distribution. To prepare for this later exercise, we’ll do some practice now. Reading: (None, this is the reading) Topics: Moments, moment arithmetic, standardization 39.1 Moments Moments are a particular kind of statistic. There is a general, mathematical definition of a moment, but we will only need to talk about two in this class. We’ve already seen the mean; this is also called the expectation. For a random variable \\(X\\), the expectation is defined in terms of its pdf \\(\\rho(x)\\) via \\[\\mathbb{E}[X] = \\int x \\rho(x) dx.\\] We’ve also seen the standard deviation \\(\\sigma\\). This is related to the variance \\(\\sigma^2\\), which is defined for a random variable \\(X\\) in terms of the expectation \\[\\sigma^2 \\equiv \\mathbb{V}[X] = \\mathbb{E}[(X - \\mathbb{E}[X])^2].\\] For instance, a standard normal \\(Z\\) has \\[ \\begin{aligned} \\mathbb{E}[Z] &amp;= 0 \\\\ \\mathbb{V}[Z] &amp;= 1 \\end{aligned} \\] For future exercises, we’ll need to learn how to do basic arithmetic with these two moments. 39.2 Moment Arithmetic We will need to be able to do some basic arithmetic with the mean and variance. The following exercises will help you remember this basic arithmetic. 39.3 Expectation The expectation is linear, that is \\[\\mathbb{E}[aX + c] = a \\mathbb{E}[X] + c.\\] We can use this fact to compute the mean of simply transformed random variables. 39.3.1 q1 Compute the mean of \\(2 Z + 3\\), where \\(Z\\) is a standard normal. ## TASK: Compute the mean of 2 Z + 3 E_q1 &lt;- 3 Use the following test to check your answer. ## NOTE: No need to change this! assertthat::assert_that(assertthat::are_equal(E_q1, 3)) ## [1] TRUE print(&quot;Nice!&quot;) ## [1] &quot;Nice!&quot; Since the expectation is linear, it also satisfies \\[\\mathbb{E}[aX + bY] = a \\mathbb{E}[X] + b \\mathbb{E}[Y].\\] 39.3.2 q2 Compute the mean of \\(2 Z_1 + 3 Z_2\\), where \\(Z_1, Z_2\\) are separate standard normals. ## TASK: Compute the mean of 2 Z1 + 3 Z2 E_q2 &lt;- 2 * 0 + 3 * 0 Use the following test to check your answer. ## NOTE: No need to change this! assertthat::assert_that(assertthat::are_equal(E_q2, 0)) ## [1] TRUE print(&quot;Great!&quot;) ## [1] &quot;Great!&quot; 39.4 Variance Remember that variance is the square of standard deviation. Variance satisfies the property \\[\\mathbb{V}[aX + c] = a^2 \\mathbb{V}[X].\\] 39.4.1 q3 Compute the variance of \\(2 Z + 3\\), where \\(Z\\) is a standard normal. ## TASK: Compute the mean of 2 Z + 3 V_q3 &lt;- 2 ^ 2 Use the following test to check your answer. ## NOTE: No need to change this! assertthat::assert_that(assertthat::are_equal(V_q3, 4)) ## [1] TRUE print(&quot;Well done!&quot;) ## [1] &quot;Well done!&quot; The variance of a sum of random variables is a bit more complicated \\[\\mathbb{V}[aX + bY] = a^2 \\mathbb{V}[X] + b^2 \\mathbb{V}[Y] + 2ab \\text{Cov}[X, Y],\\] where \\(\\text{Cov}[X, Y]\\) denotes the covariance of \\(X, Y\\). Covariance is closely related to correlation, which we discussed in e-stat03-descriptive. If two random variables \\(X, Y\\) are uncorrelated, then \\(\\text{Cov}[X, Y] = 0\\). 39.4.2 q4 Compute the variance of \\(2 Z_1 + 3 Z_2\\), where \\(Z_1, Z_2\\) are uncorrelated standard normals. ## TASK: Compute the variance of 2 Z1 + 3 Z2 V_q4 &lt;- 2^2 + 3^2 Use the following test to check your answer. ## NOTE: No need to change this! assertthat::assert_that(assertthat::are_equal(V_q4, 13)) ## [1] TRUE print(&quot;Well done!&quot;) ## [1] &quot;Well done!&quot; 39.5 Standardization The following two exercises illustrate two important transformations. 39.5.1 q5 Compute the mean and variance of \\((X - 1) / 2\\), where \\[\\mathbb{E}[X] = 1, \\mathbb{V}[X] = 4\\]. ## TASK: Compute the mean and variance E_q3 &lt;- 0 V_q3 &lt;- 1 Use the following test to check your answer. ## NOTE: No need to change this! assertthat::assert_that(assertthat::are_equal(E_q3, 0)) ## [1] TRUE assertthat::assert_that(assertthat::are_equal(V_q3, 1)) ## [1] TRUE print(&quot;Well done!&quot;) ## [1] &quot;Well done!&quot; This process of centering (setting the mean to zero) and scaling a random variable is called standardization. For instance, if \\(X\\) is a normal random variable, then \\((X - \\mu) / \\sigma = Z\\) is a standard normal. 39.5.2 q6 Compute the mean and variance of \\(1 + 2 Z\\), where \\(Z\\) is a standard normal. ## TASK: Compute the mean and variance E_q4 &lt;- 1 V_q4 &lt;- 4 Use the following test to check your answer. ## NOTE: No need to change this! assertthat::assert_that(assertthat::are_equal(E_q4, 1)) ## [1] TRUE assertthat::assert_that(assertthat::are_equal(V_q4, 4)) ## [1] TRUE print(&quot;Excellent!&quot;) ## [1] &quot;Excellent!&quot; This example illustrates that we can create a normal with desired mean and standard deviation by transforming a standard normal \\(\\mu + \\sigma Z = X\\). 39.6 Standard Error The variance satisfies the property \\[\\mathbb{V}[aX + bY] = a^2 \\mathbb{V}[X] + b^2 \\mathbb{V}[Y] + 2 \\text{Cov}[X, Y],\\] where \\[\\text{Cov}[X, Y] = \\mathbb{E}[(X - \\mathbb{E}[X])(Y - \\mathbb{E}[Y])]\\] is the covariance between \\(X\\) and \\(Y\\). If \\(X, Y\\) are independent, then the covariance between them is zero. Using this expression, we can prove that the standard error of the sample mean \\(\\overline{X}\\) is \\(\\sigma / \\sqrt{n}\\). 39.6.1 q7 (Bonus) Use the identity above to prove that \\[\\mathbb{V}[\\overline{X}] = \\sigma^2 / n,\\] where \\[\\overline{X} = \\frac{1}{n}\\sum_{i=1}^n X_i\\], \\(\\sigma^2 = \\mathbb{V}[X]\\), and the \\(X_i\\) are mutually independent. The quantity \\[\\sqrt{\\mathbb{V}[\\overline{X}]}\\] is called the standard error of the mean; more generally the standard error for a statistic is the standard deviation of its sampling distribution. We’ll return to this concept in e-stat06. "],["stats-the-central-limit-theorem-and-confidence-intervals.html", "40 Stats: The Central Limit Theorem and Confidence Intervals 40.1 Central Limit Theorem 40.2 Confidence Intervals 40.3 Making Comparisons with CI 40.4 (Bonus) Deriving an Approximate Confidence Interval 40.5 Notes", " 40 Stats: The Central Limit Theorem and Confidence Intervals Purpose: When studying sampled data, we need a principled way to report our results with their uncertainties. Confidence intervals (CI) are an excellent way to summarize results, and the central limit theorem (CLT) helps us to construct these intervals. Reading: (None, this is the reading) Topics: The central limit theorem (CLT), confidence intervals library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✔ ggplot2 3.4.0 ✔ purrr 1.0.1 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.5.0 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() library(nycflights13) 40.1 Central Limit Theorem Let’s return to a result from e-stat04-population: ## NOTE: No need to edit this set.seed(101) n_observations &lt;- 9 n_samples &lt;- 5e3 df_samp_unif &lt;- map_dfr( 1:n_samples, function(id) { tibble( Z = runif(n_observations), id = id ) } ) df_samp_unif %&gt;% group_by(id) %&gt;% summarize(stat = mean(Z)) %&gt;% ggplot(aes(stat)) + geom_histogram() + labs( x = &quot;Estimated Mean&quot;, title = &quot;Sampling Distribution: Estimated Mean&quot;, caption = &quot;Population: Uniform&quot; ) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. If you said that the sampling distribution from the exercise above looks roughly normal, then you are correct! This is an example of the central limit theorem, a central idea in statistics. Here we’ll introduce the central limit theorem (CLT), use it to approximate the sampling distribution for the sample mean, and in turn use that to construct an approximate confidence interval. For populations satisfying mild conditions[1], the sample mean \\(\\overline{X}\\) converges to a normal distribution as the sample size \\(n\\) approaches infinity. Specifically \\[\\overline{X} \\stackrel{d}{\\to} N(\\mu, \\sigma^2 / n),\\] where \\(\\mu\\) is the mean of the population, \\(\\sigma\\) is the standard deviation of the population, and \\(\\stackrel{d}{\\to}\\) means converges in distribution, a technical definition that is beyond the scope of this lesson. Below I simulate sampling from a uniform distribution and compute the mean at different sample sizes to illustrate the CLT: ## NOTE: No need to change this! set.seed(101) n_repl &lt;- 5e3 df_clt &lt;- map_dfr( 1:n_repl, function(id) { map_dfr( c(1, 2, 9, 81, 729), function(n) { tibble( Z = runif(n), n = n, id = id ) } ) } ) %&gt;% group_by(n, id) %&gt;% summarize(mean = mean(Z), sd = sd(Z)) ## `summarise()` has grouped output by &#39;n&#39;. You can override using the `.groups` ## argument. Let’s visualize the sampling distribution for each sample size: df_clt %&gt;% ggplot(aes(mean)) + geom_density() + facet_wrap(~n, scales = &quot;free&quot;) At just 1 our sample mean is \\(X_1 / 1\\)—we’re just drawing single observations from the population, so we see a uniform. At 2 we something that looks like a tent. By 9 samples we see a distribution that looks quite normal. The CLT doesn’t work for all problems. The CLT is often used for sums of random variables—the mean is one such sum. However, something like a quantile is not estimated by a sum of random variables, so we can’t use the CLT to approximate a sampling distribution. Later we’ll learn a more general tool to approximate sampling distributions for general statistics—the bootstrap. Note that the CLT tells us about estimates like the sample mean, it does not tell us anything about the distribution of the underlying population. We will use the CLT to help construct confidence intervals. 40.2 Confidence Intervals Let’s learn about confidence intervals by way of example. I’ll lay out a procedure, then explain how it works. First, let’s use some moment arithmetic to build a normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma / \\sqrt{n}\\) out of a standard normal \\(Z\\). This gives us \\[X = \\mu + (\\sigma / \\sqrt{n}) Z.\\] Now imagine we wanted to select two endpoints to give us the middle \\(95%\\) of this distribution. We could do this with \\(qnorm()\\) with the appropriate values of mean, sd. But using the definition of \\(X\\) above, we can also do this by using the appropriate quantiles of the standard normal \\(Z\\). The following code gives the upper quantile. ## NOTE: No need to change this! q95 &lt;- qnorm( 1 - (1 - 0.95) / 2 ) q95 ## [1] 1.959964 This is approximately 1.96 when seeking a \\(95%\\) confidence level. Since the standard normal distribution is symmetric about zero, we can use the same value q95 with a negative sign for the appropriate lower quantile. Here’s the procedure, we’ll build lower and upper bounds for an interval based on the sample mean and sample standard error \\([\\hat{mu} - q_{95} \\hat{\\text{SE}}, \\hat{mu} + q_{95} \\hat{\\text{SE}}]\\). I construct this interval for each sample in df_clt, and check whether the interval contains the population mean of 0.5. The following code visualizes the first 100 intervals. ## NOTE: No need to change this! df_clt %&gt;% filter( n &gt; 1, id &lt;= 100 ) %&gt;% mutate( se = sd / sqrt(n), lo = mean - q95 * se, hi = mean + q95 * se ) %&gt;% ggplot(aes(id)) + geom_hline(yintercept = 0.5, linetype = 2) + geom_errorbar(aes( ymin = lo, ymax = hi, color = (lo &lt;= 0.5) &amp; (0.5 &lt;= hi) )) + facet_grid(n~.) + scale_color_discrete(name = &quot;CI Contains True Mean&quot;) + theme(legend.position = &quot;bottom&quot;) + labs( x = &quot;Replication&quot;, y = &quot;Estimated Mean&quot; ) Some observations to note: The confidence intervals tend to be larger when \\(n\\) is small, and shrink as \\(n\\) increases. We tend to have more “misses” when \\(n\\) is small. Every confidence interval either does or does not include the true value. Therefore a single confidence interval actually has no probability associated with it. The “confidence” is not in a single interval, but rather in the procedure that generated the interval. The following code estimates the frequency with which each interval includes the true mean; this quantity is called coverage, and it should match the nominal \\(95%\\) we selected above. ## NOTE: No need to change this! df_clt %&gt;% filter(n &gt; 1) %&gt;% mutate( se = sd / sqrt(n), lo = mean - q95 * se, hi = mean + q95 * se, flag = (lo &lt;= 0.5) &amp; (0.5 &lt;= hi) ) %&gt;% group_by(n) %&gt;% summarize(coverage = mean(flag)) ## # A tibble: 4 × 2 ## n coverage ## &lt;dbl&gt; &lt;dbl&gt; ## 1 2 0.661 ## 2 9 0.908 ## 3 81 0.945 ## 4 729 0.950 Some observations to note: The coverage is well below our desired \\(95%\\) when \\(n\\) is small; this is because we are making an approximation. As \\(n\\) increases, the coverage tends towards our desired \\(95%\\). This animation is the best visual explain I’ve found on how confidence intervals are constructed [2]. 40.2.1 q1 Using the CLT, approximate a \\(99\\%\\) confidence interval for the population mean using the sample z_q1. ## TASK: Estimate a 99% confidence interval with the sample below set.seed(101) z_q1 &lt;- rnorm(n = 100, mean = 1, sd = 2) lo_q1 &lt;- mean(z_q1) - qnorm( 1 - (1 - 0.99) / 2 ) * sd(z_q1) / sqrt(100) hi_q1 &lt;- mean(z_q1) + qnorm( 1 - (1 - 0.99) / 2 ) * sd(z_q1) / sqrt(100) Use the following tests to check your answer. ## NOTE: No need to change this! assertthat::assert_that(abs(lo_q1 - 0.4444163) &lt; 1e-6) ## [1] TRUE assertthat::assert_that(abs(hi_q1 - 1.406819) &lt; 1e-6) ## [1] TRUE print(&quot;Nice!&quot;) ## [1] &quot;Nice!&quot; 40.3 Making Comparisons with CI Why would we bother with constructing a confidence interval? Let’s take a look at a real example with the NYC flight data. Let’s suppose we were trying to determine whether the mean arrival delay time of American Airlines (AA) flights is greater than zero. We have the population of 2013 flights, so we can answer this definitively: ## NOTE: No need to change this! df_flights_aa &lt;- flights %&gt;% filter(carrier == &quot;AA&quot;) %&gt;% summarize(across( arr_delay, c( &quot;mean&quot; = ~mean(., na.rm = TRUE), &quot;sd&quot; = ~sd(., na.rm = TRUE), &quot;n&quot; = ~length(.) ) )) df_flights_aa ## # A tibble: 1 × 3 ## arr_delay_mean arr_delay_sd arr_delay_n ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 0.364 42.5 32729 The arr_delay_mean is greater than zero, so case closed. But imagine we only had a sample of flights, rather than the whole population. The following code randomly samples the AA flights, and repeats this process at a few different sample sizes. I also construct confidence intervals: If the confidence interval has its lower bound greater than zero, then we can be reasonably confident the mean delay time is greater than zero. ## NOTE: No need to change this! set.seed(101) # Downsample at different sample sizes, construct a confidence interval df_flights_sampled &lt;- map_dfr( c(5, 10, 25, 50, 100, 250, 500), # Sample sizes function(n) { flights %&gt;% filter(carrier == &quot;AA&quot;) %&gt;% slice_sample(n = n) %&gt;% summarize(across( arr_delay, c( &quot;mean&quot; = ~mean(., na.rm = TRUE), &quot;se&quot; = ~sd(., na.rm = TRUE) / length(.) ) )) %&gt;% mutate( arr_delay_lo = arr_delay_mean - 1.96 * arr_delay_se, arr_delay_hi = arr_delay_mean + 1.96 * arr_delay_se, n = n ) } ) # Visualize df_flights_sampled %&gt;% ggplot(aes(n, arr_delay_mean)) + geom_hline( data = df_flights_aa, mapping = aes(yintercept = arr_delay_mean), size = 0.1 ) + geom_hline(yintercept = 0, color = &quot;white&quot;, size = 2) + geom_errorbar(aes( ymin = arr_delay_lo, ymax = arr_delay_hi, color = (0 &lt; arr_delay_lo) )) + geom_point() + scale_x_log10() + scale_color_discrete(name = &quot;Confidently Greater than Zero?&quot;) + theme(legend.position = &quot;bottom&quot;) + labs( x = &quot;Samples&quot;, y = &quot;Arrival Delay (minutes)&quot;, title = &quot;American Airlines Delays&quot; ) ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. These confidence intervals illustrate a number of different sampling scenarios. In some of them, we correctly determine that the mean arrival delay is confidently greater than zero. The case at \\(n = 100\\) is inconclusive; the CI is compatible with both positive and negative mean delay times. Note the two lowest \\(n\\) cases; there we “confidently” determine that the mean arrival delay is negative [3]. Any time we are doing estimation we are in danger of making an incorrect conclusion, even when we do the statistics correctly! Obtaining data simply decreases the probability of making a false conclusion [4]. However, combining all our available information to form a confidence interval is a principled way to report our results. A confidence interval gives us a plausible range of values for the population value, and by its width gives us a sense of how accurate our estimate is likely to be. 40.4 (Bonus) Deriving an Approximate Confidence Interval (This is bonus content provided for the curious reader.) Under the CLT, the sampling distribution for the sample mean is \\[\\overline{X} \\sim N(\\mu, \\sigma^2 / n).\\] We can standardize this quantity to form \\[(\\overline{X} - \\mu) / (\\sigma / \\sqrt{n}) \\sim N(0, 1^2).\\] This is called a pivotal quantity; it is a quantity whose distribution does not depend on the parameters we are trying to estimate. The lower and upper quantiles corresponding to a symmetric \\(C\\) confidence level are q_C = qnorm( 1 - (1 - C) / 2 ) and -q_C, which means \\[\\mathbb{P}[-q_C &lt; (\\overline{X} - \\mu) / (\\sigma / \\sqrt{n}) &lt; +q_C] = C.\\] With a small amount of arithmetic, we can re-arrange the inequalities inside the probability statement to write \\[\\mathbb{P}[\\overline{X} - q_C (\\sigma / \\sqrt{n}) &lt; \\mu &lt; \\overline{X} + q_C (\\sigma / \\sqrt{n})] = C.\\] Using a plug-in estimate for \\(\\sigma\\) gives the procedure defined above. 40.5 Notes [1] Namely, the population must have finite mean and finite variance. [2] This the best visualization of the confidence interval concept that I have ever found. Click through Frequentist Inference &gt; Confidence Interval to see the animation. [3] Part of the issue here is that we are not accounting for the additional variability that arises from estimating the standard deviation. Using a t-distribution to construct more conservative confidence intervals helps at lower sample sizes. [4] The process of making decisions about what to believe about reality based on data is called hypothesis testing. We’ll talk about this soon! "],["data-reading-excel-sheets.html", "41 Data: Reading Excel Sheets 41.1 Wrangling Basics 41.2 Danger Zone", " 41 Data: Reading Excel Sheets Purpose: The Tidyverse is built to work with tidy data. Unfortunately, most data in the wild are not tidy. The good news is that we have a lot of tools for wrangling data into tidy form. The bad news is that “every untidy dataset is untidy in its own way.” I can’t show you you every crazy way people decide to store their data. But I can walk you through a worked example to show some common techniques. In this case study, I’ll take you through the process of wrangling a messy Excel spreadsheet into machine-readable form. You will both learn some general tools for wrangling data, and you can keep this notebook as a recipe for future messy datasets of similar form. Reading: (None) library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✔ ggplot2 3.4.0 ✔ purrr 1.0.1 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.5.0 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() library(readxl) # For reading Excel sheets library(httr) # For downloading files ## Use my tidy-exercises copy of UNDOC data for stability url_undoc &lt;- &quot;https://github.com/zdelrosario/tidy-exercises/blob/master/2019/2019-12-10-news-plots/GSH2013_Homicide_count_and_rate.xlsx?raw=true&quot; filename &lt;- &quot;./data/undoc.xlsx&quot; I keep a copy of the example data in a personal repo; download a local copy. ## NOTE: No need to edit curl::curl_download( url_undoc, filename ) 41.1 Wrangling Basics 41.1.1 q1 Run the following code and pay attention to the column names. Open the downloaded Excel sheet and compare. Why are the column names so weird? ## NOTE: No need to edit; run and inspect df_q1 &lt;- read_excel(filename) ## New names: ## • `` -&gt; `...2` ## • `` -&gt; `...3` ## • `` -&gt; `...4` ## • `` -&gt; `...5` ## • `` -&gt; `...6` ## • `` -&gt; `...7` ## • `` -&gt; `...8` ## • `` -&gt; `...9` ## • `` -&gt; `...10` ## • `` -&gt; `...11` ## • `` -&gt; `...12` ## • `` -&gt; `...13` ## • `` -&gt; `...14` ## • `` -&gt; `...15` ## • `` -&gt; `...16` ## • `` -&gt; `...17` ## • `` -&gt; `...18` ## • `` -&gt; `...19` df_q1 %&gt;% glimpse ## Rows: 447 ## Columns: 19 ## $ `Intentional homicide count and rate per 100,000 population, by country/territory (2000-2012)` &lt;chr&gt; … ## $ ...2 &lt;chr&gt; … ## $ ...3 &lt;chr&gt; … ## $ ...4 &lt;chr&gt; … ## $ ...5 &lt;chr&gt; … ## $ ...6 &lt;chr&gt; … ## $ ...7 &lt;chr&gt; … ## $ ...8 &lt;dbl&gt; … ## $ ...9 &lt;dbl&gt; … ## $ ...10 &lt;dbl&gt; … ## $ ...11 &lt;dbl&gt; … ## $ ...12 &lt;dbl&gt; … ## $ ...13 &lt;dbl&gt; … ## $ ...14 &lt;dbl&gt; … ## $ ...15 &lt;dbl&gt; … ## $ ...16 &lt;chr&gt; … ## $ ...17 &lt;chr&gt; … ## $ ...18 &lt;chr&gt; … ## $ ...19 &lt;chr&gt; … Observations: The top row is filled with expository text. The actual column names are several rows down. Most read_ functions have a skip argument you can use to skip over the first few lines. Use this argument in the next task to deal with the top of the Excel sheet. 41.1.2 q2 Read the Excel sheet. Open the target Excel sheet (located at ./data/undoc.xlsx) and find which line (row) at which the year column headers are located. Use the skip keyword to start your read at that line. ## TODO: df_q2 &lt;- read_excel( filename, skip = 6 ) ## New names: ## • `` -&gt; `...1` ## • `` -&gt; `...2` ## • `` -&gt; `...3` ## • `` -&gt; `...4` ## • `` -&gt; `...5` ## • `` -&gt; `...6` df_q2 %&gt;% glimpse ## Rows: 444 ## Columns: 19 ## $ ...1 &lt;chr&gt; &quot;Africa&quot;, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ ...2 &lt;chr&gt; &quot;Eastern Africa&quot;, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ ...3 &lt;chr&gt; &quot;Burundi&quot;, NA, &quot;Comoros&quot;, NA, &quot;Djibouti&quot;, NA, &quot;Eritrea&quot;, NA, &quot;E… ## $ ...4 &lt;chr&gt; &quot;PH&quot;, NA, &quot;PH&quot;, NA, &quot;PH&quot;, NA, &quot;PH&quot;, NA, &quot;PH&quot;, NA, &quot;CJ&quot;, NA, &quot;PH… ## $ ...5 &lt;chr&gt; &quot;WHO&quot;, NA, &quot;WHO&quot;, NA, &quot;WHO&quot;, NA, &quot;WHO&quot;, NA, &quot;WHO&quot;, NA, &quot;CTS&quot;, N… ## $ ...6 &lt;chr&gt; &quot;Rate&quot;, &quot;Count&quot;, &quot;Rate&quot;, &quot;Count&quot;, &quot;Rate&quot;, &quot;Count&quot;, &quot;Rate&quot;, &quot;Cou… ## $ `2000` &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 6.2, 70… ## $ `2001` &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 7.7, 90… ## $ `2002` &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 4.8, 56… ## $ `2003` &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 2.5, 30… ## $ `2004` &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 4.0, 1395.0, NA, NA, 3.… ## $ `2005` &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 3.5, 1260.0, NA, NA, 1.… ## $ `2006` &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 3.5, 1286.0, NA, NA, 6.… ## $ `2007` &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 3.4, 1281.0, NA, NA, 5.… ## $ `2008` &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 3.6, 1413.0, NA, NA, 5.… ## $ `2009` &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, &quot;5.6&quot;, &quot;2218&quot;, NA, NA, … ## $ `2010` &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, &quot;5.5&quot;, &quot;2239&quot;, NA, NA, … ## $ `2011` &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, &quot;6.3&quot;, &quot;2641&quot;, NA, NA, … ## $ `2012` &lt;chr&gt; &quot;8&quot;, &quot;790&quot;, &quot;10&quot;, &quot;72&quot;, &quot;10.1&quot;, &quot;87&quot;, &quot;7.1&quot;, &quot;437&quot;, &quot;12&quot;, &quot;1104… Use the following test to check your work. ## NOTE: No need to change this assertthat::assert_that(setequal( (df_q2 %&gt;% names() %&gt;% .[7:19]), as.character(seq(2000, 2012)) )) ## [1] TRUE print(&quot;Nice!&quot;) ## [1] &quot;Nice!&quot; Let’s take stock of where we are: df_q2 %&gt;% head() ## # A tibble: 6 × 19 ## ...1 ...2 ...3 ...4 ...5 ...6 `2000` `2001` `2002` `2003` `2004` `2005` ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Africa East… Buru… PH WHO Rate NA NA NA NA NA NA ## 2 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; Count NA NA NA NA NA NA ## 3 &lt;NA&gt; &lt;NA&gt; Como… PH WHO Rate NA NA NA NA NA NA ## 4 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; Count NA NA NA NA NA NA ## 5 &lt;NA&gt; &lt;NA&gt; Djib… PH WHO Rate NA NA NA NA NA NA ## 6 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; Count NA NA NA NA NA NA ## # … with 7 more variables: `2006` &lt;dbl&gt;, `2007` &lt;dbl&gt;, `2008` &lt;dbl&gt;, ## # `2009` &lt;chr&gt;, `2010` &lt;chr&gt;, `2011` &lt;chr&gt;, `2012` &lt;chr&gt; We still have problems: The first few columns don’t have sensible names. The col_names argument allows us to set manual names at the read phase. Some of the columns are of the wrong type; for instance 2012 is a chr vector. We can use the col_types argument to set manual column types. 41.1.3 q3 Change the column names and types. Use the provided names in col_names_undoc with the col_names argument to set manual column names. Use the col_types argument to set all years to \"numeric\", and the rest to \"text\". Hint 1: Since you’re providing manual col_names, you will have to adjust your skip value! Hint 2: You can use a named vector for col_types to help keep type of which type is assigned to which variable, for instance c(\"variable\" = \"type\"). ## NOTE: Use these column names col_names_undoc &lt;- c( &quot;region&quot;, &quot;sub_region&quot;, &quot;territory&quot;, &quot;source&quot;, &quot;org&quot;, &quot;indicator&quot;, &quot;2000&quot;, &quot;2001&quot;, &quot;2002&quot;, &quot;2003&quot;, &quot;2004&quot;, &quot;2005&quot;, &quot;2006&quot;, &quot;2007&quot;, &quot;2008&quot;, &quot;2009&quot;, &quot;2010&quot;, &quot;2011&quot;, &quot;2012&quot; ) ## TASK: Use the arguments `skip`, `col_names`, and `col_types` df_q3 &lt;- read_excel( filename, sheet = 1, skip = 7, col_names = col_names_undoc, col_types = c( &quot;region&quot; = &quot;text&quot;, &quot;sub_region&quot; = &quot;text&quot;, &quot;territory&quot; = &quot;text&quot;, &quot;source&quot; = &quot;text&quot;, &quot;org&quot; = &quot;text&quot;, &quot;indicator&quot; = &quot;text&quot;, &quot;2000&quot; = &quot;numeric&quot;, &quot;2001&quot; = &quot;numeric&quot;, &quot;2002&quot; = &quot;numeric&quot;, &quot;2003&quot; = &quot;numeric&quot;, &quot;2004&quot; = &quot;numeric&quot;, &quot;2005&quot; = &quot;numeric&quot;, &quot;2006&quot; = &quot;numeric&quot;, &quot;2007&quot; = &quot;numeric&quot;, &quot;2008&quot; = &quot;numeric&quot;, &quot;2009&quot; = &quot;numeric&quot;, &quot;2010&quot; = &quot;numeric&quot;, &quot;2011&quot; = &quot;numeric&quot;, &quot;2012&quot; = &quot;numeric&quot; ) ) ## Warning: Expecting numeric in P315 / R315C16: got &#39;2366*&#39; ## Warning: Expecting numeric in Q315 / R315C17: got &#39;1923*&#39; ## Warning: Expecting numeric in R315 / R315C18: got &#39;1866*&#39; ## Warning: Expecting numeric in S381 / R381C19: got &#39;x&#39; ## Warning: Expecting numeric in S431 / R431C19: got &#39;x&#39; ## Warning: Expecting numeric in S433 / R433C19: got &#39;x&#39; ## Warning: Expecting numeric in S435 / R435C19: got &#39;x&#39; ## Warning: Expecting numeric in S439 / R439C19: got &#39;x&#39; ## Warning: Expecting numeric in S445 / R445C19: got &#39;x&#39; Use the following test to check your work. ## NOTE: No need to change this assertthat::assert_that(setequal( (df_q3 %&gt;% names()), col_names_undoc )) ## [1] TRUE assertthat::assert_that((df_q3 %&gt;% slice(1) %&gt;% pull(`2012`)) == 8) ## [1] TRUE print(&quot;Great!&quot;) ## [1] &quot;Great!&quot; 41.2 Danger Zone Now let’s take a look at the head of the data: df_q3 %&gt;% head() ## # A tibble: 6 × 19 ## region sub_r…¹ terri…² source org indic…³ `2000` `2001` `2002` `2003` `2004` ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Africa Easter… Burundi PH WHO Rate NA NA NA NA NA ## 2 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; Count NA NA NA NA NA ## 3 &lt;NA&gt; &lt;NA&gt; Comoros PH WHO Rate NA NA NA NA NA ## 4 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; Count NA NA NA NA NA ## 5 &lt;NA&gt; &lt;NA&gt; Djibou… PH WHO Rate NA NA NA NA NA ## 6 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; Count NA NA NA NA NA ## # … with 8 more variables: `2005` &lt;dbl&gt;, `2006` &lt;dbl&gt;, `2007` &lt;dbl&gt;, ## # `2008` &lt;dbl&gt;, `2009` &lt;dbl&gt;, `2010` &lt;dbl&gt;, `2011` &lt;dbl&gt;, `2012` &lt;dbl&gt;, and ## # abbreviated variable names ¹​sub_region, ²​territory, ³​indicator Irritatingly, many of the cell values are left implicit; as humans reading these data, we can tell that the entries in region under Africa also have the value Africa. However, the computer can’t tell this! We need to make these values explicit by filling them in. To that end, I’m going to guide you through some slightly advanced Tidyverse code to lag-fill the missing values. To that end, I’ll define and demonstrate two helper functions: First, the following function counts the number of rows with NA entries in a chosen set of columns: ## Helper function to count num rows w/ NA in vars_lagged rowAny &lt;- function(x) rowSums(x) &gt; 0 countna &lt;- function(df, vars_lagged) { df %&gt;% filter(rowAny(across(vars_lagged, is.na))) %&gt;% dim %&gt;% .[[1]] } countna(df_q3, c(&quot;region&quot;)) ## Warning: Using an external vector in selections was deprecated in tidyselect 1.1.0. ## ℹ Please use `all_of()` or `any_of()` instead. ## # Was: ## data %&gt;% select(vars_lagged) ## ## # Now: ## data %&gt;% select(all_of(vars_lagged)) ## ## See &lt;https://tidyselect.r-lib.org/reference/faq-external-vector.html&gt;. ## [1] 435 Ideally we want this count to be zero. To fill-in values, we will use the following function to do one round of lag-filling: lagfill &lt;- function(df, vars_lagged) { df %&gt;% mutate(across( vars_lagged, function(var) { if_else( is.na(var) &amp; !is.na(lag(var)), lag(var), var ) } )) } df_tmp &lt;- df_q3 %&gt;% lagfill(c(&quot;region&quot;)) countna(df_tmp, c(&quot;region&quot;)) ## [1] 429 We can see that lagfill has filled the Africa value in row 2, as well as a number of other rows as evidenced by the reduced value returned by countna. What we’ll do is continually run lagfill until we reduce countna to zero. We could do this by repeatedly running the function manually, but that would be silly. Instead, we’ll run a while loop to automatically run the function until countna reaches zero. 41.2.1 q4 I have already provided the while loop below; fill in vars_lagged with the names of the columns where cell values are implicit. Hint: Think about which columns have implicit values, and which truly have missing values. ## Choose variables to lag-fill vars_lagged &lt;- c(&quot;region&quot;, &quot;sub_region&quot;, &quot;territory&quot;, &quot;source&quot;, &quot;org&quot;) ## NOTE: No need to edit this ## Trim head and notes df_q4 &lt;- df_q3 %&gt;% slice(-(n()-5:-n())) ## Repeatedly lag-fill until NA&#39;s are gone while (countna(df_q4, vars_lagged) &gt; 0) { df_q4 &lt;- df_q4 %&gt;% lagfill(vars_lagged) } And we’re done! All of the particularly tricky wrangling is now done. You could now use pivoting to tidy the data into long form. "],["stats-error-and-bias.html", "42 Stats: Error and Bias 42.1 Errors", " 42 Stats: Error and Bias Purpose: Error is a subtle concept. Often statistics concepts are introduced with a host of assumptions on the errors. In this short exercise, we’ll reminder ourselves what errors are and learn what happens when one standard assumption—unbiasedness—is violated. Prerequisites: c02-michelson, e-stat06-clt ## Note: No need to edit this chunk! library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✔ ggplot2 3.4.0 ✔ purrr 1.0.1 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.5.0 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() library(googlesheets4) url &lt;- &quot;https://docs.google.com/spreadsheets/d/1av_SXn4j0-4Rk0mQFik3LLr-uf0YdA06i3ugE6n-Zdo/edit?usp=sharing&quot; c_true &lt;- 299792.458 # Exact speed of light in a vacuum (km / s) c_michelson &lt;- 299944.00 # Michelson&#39;s speed estimate (km / s) meas_adjust &lt;- +92 # Michelson&#39;s speed of light adjustment (km / s) c_michelson_uncertainty &lt;- 51 # Michelson&#39;s measurement uncertainty (km / s) gs4_deauth() ss &lt;- gs4_get(url) df_michelson &lt;- read_sheet(ss) %&gt;% select(Date, Distinctness, Temp, Velocity) %&gt;% mutate( Distinctness = as_factor(Distinctness), c_meas = Velocity + meas_adjust ) ## ✔ Reading from &quot;michelson1879&quot;. ## ✔ Range &#39;Sheet1&#39;. 42.1 Errors Let’s re-examine the Michelson speed of light data to discuss the concept of error. Let \\(c\\) denote the true speed of light, and let \\(\\hat{c}_i\\) denote the i-th measurement by Michelson. Then the error \\(\\epsilon_{c,i}\\) is: \\[\\epsilon_{c,i} \\equiv \\hat{c}_i - c.\\] Note that these are errors (and not some other quantity) because they are differences against the true value \\(c\\). Very frequently in statistics, we assume that the errors are unbiased; that is we assume \\(\\mathbb{E}[\\epsilon] = 0\\). Let’s take a look at what happens when that assumption is violated. 42.1.1 q1 Compute the errors \\(\\epsilon_c\\) using Michelson’s measurements c_meas and the true speed of light c_true. ## TASK: Compute `epsilon_c` df_q1 &lt;- df_michelson %&gt;% mutate(epsilon_c = c_meas - c_true) df_q1 %&gt;% ggplot(aes(epsilon_c)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. We can use descriptive statistics in order to summarize the errors. This will give us a quantification of the uncertainty in our measurements: remember that uncertainty is our assessment of the error. 42.1.2 q2 Estimate the mean and standard deviation of \\(\\epsilon_c\\) from df_q1. Is the error mean large or small, compared to its standard deviation? How about compared to Michelson’s uncertainty c_michelson_uncertainty? ## TASK: Estimate `epsilon_mean` and `epsilon_sd` from df_q1 df_q2 &lt;- df_q1 %&gt;% summarize( epsilon_mean = mean(epsilon_c), epsilon_sd = sd(epsilon_c) ) Observations: Use the following tests to check your answers. ## NOTE: No need to change this! assertthat::assert_that(abs((df_q2 %&gt;% pull(epsilon_mean)) - 151.942) &lt; 1e-3) ## [1] TRUE assertthat::assert_that(abs((df_q2 %&gt;% pull(epsilon_sd)) - 79.01055) &lt; 1e-3) ## [1] TRUE print(&quot;Great job!&quot;) ## [1] &quot;Great job!&quot; Generally, we want our errors to have zero mean—the case where the errors have zero mean is called unbiased. The quantity \\(\\mathbb{E}[\\epsilon]\\) is called bias, and an estimate such as \\(\\hat{c}\\) with \\(\\mathbb{E}[\\epsilon] \\neq 0\\) is called biased. What can happen when our estimates are biased? In that case, increased data may not improve our estimate, and our statistical tools—such as confidence intervals—may give us a false impression of the true error. The next example will show us what happens if we apply confidence intervals in a biased-data setting like Michelson’s data. 42.1.3 q3 Use a CLT approximation to construct a \\(99%\\) confidence interval on the mean of c_meas. Check (with the provided code) if your CI includes the true speed of light. Hint: This computation should not use the true speed of light \\(c_true\\) in any way. ## TASK: Compute a 99% confidence interval on the mean of c_meas C &lt;- 0.99 q &lt;- qnorm( 1 - (1 - C) / 2 ) df_q3 &lt;- df_q1 %&gt;% summarize( c_meas_mean = mean(c_meas), c_meas_sd = sd(c_meas), n_samp = n(), c_lo = c_meas_mean - q * c_meas_sd / sqrt(n_samp), c_hi = c_meas_mean + q * c_meas_sd / sqrt(n_samp) ) ## NOTE: This checks if the CI contains c_true (df_q3 %&gt;% pull(c_lo) &lt;= c_true) &amp; (c_true &lt;= df_q3 %&gt;% pull(c_hi)) ## [1] FALSE Use the following tests to check your answers. ## NOTE: No need to change this! assertthat::assert_that(abs((df_q3 %&gt;% pull(c_lo)) - 299924.048) &lt; 1e-3) ## [1] TRUE assertthat::assert_that(abs((df_q3 %&gt;% pull(c_hi)) - 299964.752) &lt; 1e-3) ## [1] TRUE print(&quot;Well done!&quot;) ## [1] &quot;Well done!&quot; Once you correctly compute a CI for c_meas, you should find that the interval does not include c_true. CI is never guaranteed to include its true value—it is a probabilistic construction, after all. However, we saw above that the errors are biased; even if we were to gather more data, our confidence intervals would converge on the wrong value. Statistics are not a cure-all! "],["data-map-basics.html", "43 Data: Map Basics 43.1 Formulas", " 43 Data: Map Basics Purpose: The map() function and its variants are extremely useful for automating iterative tasks. We’ll learn the basics through this short exercise. Reading: Introduction to Iteration and Map (you can skip the Case Study). library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✔ ggplot2 3.4.0 ✔ purrr 1.0.1 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.5.0 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() 43.1 Formulas The primer introduced map() as a way to apply a function to a list. # NOTE: No need to change this example map_dbl(c(1, 2, 3), log) ## [1] 0.0000000 0.6931472 1.0986123 This is very helpful when we have a builtin or otherwise defined function, but what about when we need a more special-purpose function for a specific case? In this instance we can use R’s formula notation. For example, to compute powers of 10, we could do: # NOTE: No need to change this example map_dbl(c(1, 2, 3), ~ 10 ^ .x) ## [1] 10 100 1000 The tilde ~ operator signals to R that we’re doing something special: defining a formula. The .x symbol is the argument for this new function. Basically, we are taking a formal function definition, such as # NOTE: No need to change this example pow10 &lt;- function(x) {10 ^ x} And defining a more compact version with ~ 10 ^ x.. We’ve actually already seen this formula notation when we use facet_grid() and facet_wrap(), though it’s used in a very different way in that context. 43.1.1 q1 Use map_chr() to prepend the string \"N: \" to the numbers in v_nums. Use formula notation with str_c() as your map function. Hint: The function str_c() combines two or more objects into one string. v_nums &lt;- c(1, 2, 3) v_q1 &lt;- map_chr(v_nums, ~ str_c(&quot;N: &quot;, .x)) v_q1 ## [1] &quot;N: 1&quot; &quot;N: 2&quot; &quot;N: 3&quot; Use the following test to check your work. ## NOTE: No need to change this! assertthat::assert_that(setequal(v_q1, c(&quot;N: 1&quot;, &quot;N: 2&quot;, &quot;N: 3&quot;))) ## [1] TRUE print(&quot;Great job!&quot;) ## [1] &quot;Great job!&quot; Formula notation is another way to pass arguments to functions; I find this a little more readable than passing arguments to map(). 43.1.2 q2 Use map_dbl() to compute the log with base = 2 of the numbers in v_nums. Use formula notation with log() as your map function. v_q2 &lt;- map_dbl(v_nums, ~ log(.x, base = 2)) v_q2 ## [1] 0.000000 1.000000 1.584963 ## NOTE: No need to change this! assertthat::assert_that(setequal(v_q2, log(v_nums, base = 2))) ## [1] TRUE print(&quot;Nice!&quot;) ## [1] &quot;Nice!&quot; "],["data-factors.html", "44 Data: Factors 44.1 Organization 44.2 Visual Tricks", " 44 Data: Factors Purpose: Factors are an important type of variables. Since they’re largely in a class of their own, there are special tools available in the package forcats to help wrangle factors. Reading: (None) library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✔ ggplot2 3.4.0 ✔ purrr 1.0.1 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.5.0 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() library(gapminder) A factor is a variable that only takes fixed, often non-numeric, values. Factors are sometimes called categorical variables. We’ve already seen 44.1 Organization 44.1.1 q1 The following chunk displays the levels of the factor continent. Run the following code chunk and note in what order they are listed. ## NOTE: No need to edit this diamonds %&gt;% pull(cut) %&gt;% levels() ## [1] &quot;Fair&quot; &quot;Good&quot; &quot;Very Good&quot; &quot;Premium&quot; &quot;Ideal&quot; ## TASK: Determine what order the factors are listed in. Observations: The factor levels are ordered in terms of increasing quality of diamond cut. The levels are essentially a measure of quality; we would expect price to (generally) increase with improved cut. 44.1.2 q2 Determine the levels for the continent variable in the gapminder dataset. Note the order of the levels. ## TASK: Determine the levels of the variable gapminder %&gt;% pull(continent) %&gt;% levels() ## [1] &quot;Africa&quot; &quot;Americas&quot; &quot;Asia&quot; &quot;Europe&quot; &quot;Oceania&quot; ## TASK: Determine what order the factors are listed in. Observations: The factor levels are ordered alphabetically. The forcats package has tools for working with factors. For instance, we can assign manual factor levels with the function fct_relevel(). This is generally used in a mutate(); for instance mutate(x = fct_relevel(x, \"a\", \"b\", \"c\"). 44.1.3 q3 Relevel the continents. Copy your code from q2 and introduce a mutate using fct_relevel() to reorder continent. Choose which levels to reorder and what order in which to put them. Note how the resulting order is changed when you call levels() at the end of the pipe. gapminder %&gt;% mutate( continent = fct_relevel( continent, &quot;Oceania&quot; ) ) %&gt;% pull(continent) %&gt;% levels() ## [1] &quot;Oceania&quot; &quot;Africa&quot; &quot;Americas&quot; &quot;Asia&quot; &quot;Europe&quot; Observations: Calling fct_reorder() as I do in the solution brings “Oceania” to the front, but leaves the other factors alone. 44.2 Visual Tricks When factors do no have any meaningful order, it is generally better to sort them on another variable, rather mpg %&gt;% mutate(manufacturer = fct_reorder(manufacturer, cty)) %&gt;% ggplot(aes(manufacturer, cty)) + geom_boxplot() + coord_flip() The function fct_reorder(f, x) allows you to reorder the factor f based on another variable x. This will “match” the order between the two. 44.2.1 q4 Use fct_reorder() to sort manufacturer to match the order of cty. ## TASK: Modify the following code to sort the factor `manufacturer` based on ## `cty`. mpg %&gt;% mutate(manufacturer = fct_reorder(manufacturer, cty)) %&gt;% ggplot(aes(manufacturer, cty)) + geom_boxplot() + coord_flip() Observations: Before - Toyota and Nissan seem have the most variable vehicles in this dataset, in terms of cty. - Volkswagon has a number of high cty outliers. Sorted - Honda has the most efficient vehicles in this sample. - Lincoln and Land Rover have the least efficient vehicles in this sample. - Mercury has a remarkably consistent set of cty values; perhaps this is a small sample. The function fct_reorder2(f, x, y) allows us to sort on two variables; this is most useful when making line plots. 44.2.2 q5 Sort the countries by values. Use fct_reorder2() to sort country to match the order of x = year, y = pop. Pay attention to the rightmost edge of the curves and the legend order. How does fct_reorder2() sort factors? ## TASK: Modify the following code to sort the factor `country` based on `year` ## and `pop`. gapminder %&gt;% filter(dense_rank(country) &lt;= 7) %&gt;% mutate(country = fct_reorder2(country, year, pop)) %&gt;% ggplot(aes(year, pop, color = country)) + geom_line() + scale_y_log10() Observations: The factors are sorted such that the rightmost points on the lines are vertically ordered the same as the legend. This small, simple trick is extremely helpful for creating easily-readable line graphs. "],["stats-fitting-distributions.html", "45 Stats: Fitting Distributions 45.1 Aside: Masking 45.2 Distribution Parameters and Fitting 45.3 Notes", " 45 Stats: Fitting Distributions Purpose: We use distributions to model random quantities. However, in order to model physical phenomena, we should fit the distributions using data. In this short exercise you’ll learn some functions for fitting distributions to data. library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✔ ggplot2 3.4.0 ✔ purrr 1.0.1 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.5.0 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() library(MASS) ## ## Attaching package: &#39;MASS&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## select library(broom) 45.1 Aside: Masking Note that when we load the MASS and tidyverse packages, we will find that their functions conflict. To deal with this, we’ll need to learn how to specify a namespace when calling a function. To do this, use the :: notation; i.e. namespace::function. For instance, to call filter from dplyr, we would write dplyr::filter(). One of the specific conflicts between MASS and tidyverse is the select function. Try running the chunk below; it will throw an error: diamonds %&gt;% select(carat, cut) %&gt;% glimpse() This error occurs because MASS also provides a select function. 45.1.1 q0 Fix the following code! Use the namespace :: operator to use the correct select() function. diamonds %&gt;% dplyr::select(carat, cut) %&gt;% glimpse() ## Rows: 53,940 ## Columns: 2 ## $ carat &lt;dbl&gt; 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, 0.23, 0.30… ## $ cut &lt;ord&gt; Ideal, Premium, Good, Premium, Good, Very Good, Very Good, Very … 45.2 Distribution Parameters and Fitting The function rnorm() requires values for mean and sd; while rnorm() has defaults for these arguments, if we are trying to model a random event in the real world, we should set mean, sd based on data. The process of estimating parameters such as mean, sd for a distribution is called fitting. Fitting a distribution is often accomplished through maximum likelihood estimation (MLE); rather than discuss the gory details of MLE, we will simply use MLE as a technology to do useful work. First, let’s look at an example of MLE carried out with the function MASS::fitdistr(). ## NOTE: No need to edit this setup set.seed(101) df_data_norm &lt;- tibble(x = rnorm(50, mean = 2, sd = 1)) ## NOTE: Example use of fitdistr() df_est_norm &lt;- df_data_norm %&gt;% pull(x) %&gt;% fitdistr(densfun = &quot;normal&quot;) %&gt;% tidy() df_est_norm ## # A tibble: 2 × 3 ## term estimate std.error ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 mean 1.88 0.131 ## 2 sd 0.923 0.0923 Notes: fitdistr() takes a vector; I use the function pull(x) to pull the vector x out of the dataframe. fitdistr() returns a messy output; the function broom::tidy() automagically cleans up the output and provides a tibble. 45.2.1 q1 Compute the sample mean and standard deviation of x in df_data_norm. Compare these values to those you computed with fitdistr(). ## TASK: Compute the sample mean and sd of `df_data_norm %&gt;% pull(x)` mean_est &lt;- df_data_norm %&gt;% pull(x) %&gt;% mean() sd_est &lt;- df_data_norm %&gt;% pull(x) %&gt;% sd() mean_est ## [1] 1.876029 sd_est ## [1] 0.9321467 Observations: The values are exactly the same! Estimating parameters for a normal distribution is easy because it is parameterized in terms of the mean and standard deviation. The advantage of using fitdistr() is that it will allow us to work with a much wider selection of distribution models. 45.2.2 q2 Use the function fitdistr() to fit a \"weibull\" distribution to the realizations y in df_data_weibull. Note: The weibull distribution is used to model many physical phenomena, including the strength of composite materials. ## NOTE: No need to edit this setup set.seed(101) df_data_weibull &lt;- tibble(y = rweibull(50, shape = 2, scale = 4)) ## TASK: Use the `fitdistr()` function to estimate parameters df_q2 &lt;- df_data_weibull %&gt;% pull(y) %&gt;% fitdistr(densfun = &quot;weibull&quot;) %&gt;% tidy() df_q2 ## # A tibble: 2 × 3 ## term estimate std.error ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 shape 2.18 0.239 ## 2 scale 4.00 0.274 Once we’ve fit a distribution, we can use the estimated parameters to approximate quantities like probabilities. If we were using the distribution for y to model a material strength, we would estimate probabilities to compute the rate of failure for mechanical components—we could then use this information to make design decisions. 45.2.3 q3 Extract the estimates shape_est and scale_est from df_q2, and use them to estimate the probability that Y &lt;= 2. Hint: pr_true contains the true probability; modify that code to compute the estimated probability. ## NOTE: No need to modify this line pr_true &lt;- pweibull(q = 2, shape = 2, scale = 4) set.seed(101) shape_est &lt;- df_q2 %&gt;% filter(term == &quot;shape&quot;) %&gt;% pull(estimate) scale_est &lt;- df_q2 %&gt;% filter(term == &quot;scale&quot;) %&gt;% pull(estimate) pr_est &lt;- pweibull(q = 2, shape = shape_est, scale = scale_est) pr_true ## [1] 0.2211992 pr_est ## [1] 0.1988446 You’ll probably find that pr_true != pr_est! As we saw in e-stat06-clt we should really compute a confidence interval to assess our degree of confidence in this probability estimate. However, it’s not obvious how we can use the ideas of the Central Limit Theorem to put a confidence interval around `pr_est. In the next exercise we’ll learn a very general technique for estimating confidence intervals. 45.3 Notes [1] For another tutorial on fitting distributions in R, see this R-bloggers post. "],["vis-perceptual-basics.html", "46 Vis: Perceptual Basics 46.1 Criticize these graphs!", " 46 Vis: Perceptual Basics Purpose: Creating a presentation-quality graph is an exercise in communication. In order to create graphs that other people can understand, we should know some stuff about how humans see data. Through the required “reading” (video) you’ll learn about visual perception, then put these ideas to use criticizing some graphs. Later, you’ll use these ideas to improve some graphs. Reading: How Humans See Data (Video) library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✔ ggplot2 3.4.0 ✔ purrr 1.0.1 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.5.0 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() 46.1 Criticize these graphs! Using the ideas from the reading (video), state some issues with the following graphs. As a reminder, the visual hierarchy is: Position along a common scale Position on identical but nonaligned scales Length Angle; Slope (With slope not too close to 0, \\(\\pi/2\\), or \\(\\pi\\).) Area Volume; Density; Color saturation Color hue 46.1.1 q1 What are some issues with the following graph? Don’t just say “it’s bad”—use concepts from the required reading. ## NOTE: No need to edit; run and criticize mpg %&gt;% ggplot(aes(manufacturer, cty)) + geom_boxplot() + coord_flip() Observations: An alphabetical ordering of factors is almost never meaningful. We’ll learn how to reorder factors in e-data11-factors: mpg %&gt;% ggplot(aes(fct_reorder(manufacturer, cty), cty)) + geom_boxplot() + coord_flip() 46.1.2 q2 What are some issues with the following graph? Don’t just say “it’s bad”—use concepts from the required reading. ## NOTE: No need to edit; run and criticize as_tibble(mtcars) %&gt;% mutate(model = rownames(mtcars)) %&gt;% ggplot(aes(x = &quot;&quot;, y = &quot;&quot;, size = mpg)) + geom_point() + facet_wrap(~model) Area is low on the visual hierarchy; it is difficult to see the difference between mpg values. 46.1.3 q3 What are some issues with the following graph? Don’t just say “it’s bad”—use concepts from the required reading. ## NOTE: No need to edit; run and criticize diamonds %&gt;% ggplot(aes(clarity, fill = cut)) + geom_bar() Stacked bar charts force us to make comparisons using length, rather than position along a common axis. 46.1.4 q4 What are some issues with the following graph? Don’t just say “it’s bad”—use concepts from the required reading. ## NOTE: No need to edit; run and criticize diamonds %&gt;% ggplot(aes(x = &quot;&quot;, fill = cut)) + geom_bar() + coord_polar(&quot;y&quot;) + labs(x = &quot;&quot;) A pie chart has encodes numbers as angles, which is low on the visual hierarchy. "],["stats-the-bootstrap-some-recipies.html", "47 Stats: The Bootstrap, Some Recipies 47.1 A Simple Example: Estimating the Mean 47.2 A Worked Example: Probability Estimate 47.3 Notes", " 47 Stats: The Bootstrap, Some Recipies Purpose: Confidence intervals are an important tool for assessing our estimates. However, our tools so far for estimating confidence intervals rely on assumptions (normality, applicability of the CLT) that limit the statistics we can study. In this exercise we’ll learn about a general-purpose tool we can use to approximate CI—the bootstrap. library(MASS) library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✔ ggplot2 3.4.0 ✔ purrr 1.0.1 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.5.0 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ✖ dplyr::select() masks MASS::select() library(broom) library(rsample) 47.1 A Simple Example: Estimating the Mean first, imagine that we have a sample from some population. ## NOTE: No need to edit this setup set.seed(101) df_data_norm &lt;- tibble(x = rnorm(50)) df_data_norm %&gt;% ggplot(aes(x)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. The set of samples—so long as it is representative of the population—is our best available approximation of the population. What the bootstrap does is operationalize this observation: We treat our sample as a population, and sample from it randomly. What that means is we generate some number of new bootstrap samples from our available sample. Visually, that looks like the following: ## NOTE: No need to edit this setup df_resample_norm &lt;- bootstraps(df_data_norm, times = 1000) %&gt;% mutate(df = map(splits, ~ analysis(.x))) df_resample_norm %&gt;% slice(1:9) %&gt;% unnest(df) %&gt;% ggplot(aes(x)) + geom_histogram() + facet_wrap(~ id) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Every panel in this figure depicts a single bootstrap resample, drawn from our original sample. Each bootstrap resample plays the role of a single sample; we construct a resample, compute a single statistic for each bootstrap resample, and we do this whole process some number of times. In the example above, I set times = 1000; generally larger is better, but a good rule of thumb is to do 1000 resamples. Notes: The bootstraps() function comes from the rsample package, which implements many different resampling strategies (beyond the bootstrap). The analysis() function also comes from rsample; this is a special function we need to call when working with a resampling of the data [1]. We saw the map() function in e-data10-map; using map() above is necessary in part because we need to call analysis(). Since analysis() is not vectorized, we need the map to use this function on every split in splits. ## NOTE: No need to edit this example v_mean_est &lt;- map_dbl( df_resample_norm %&gt;% pull(df), ~ summarize(.x, mean_est = mean(x)) %&gt;% pull(mean_est) ) v_mean_est[1:9] ## [1] -0.066678997 -0.244149601 0.049938113 -0.074652483 0.008309007 ## [6] -0.196989467 -0.252095066 -0.238739640 -0.189294459 47.1.1 q1 Modify the code above to use within a mutate() call on df_resample_norm. Assign the mean estimates to the new column mean_est. df_q1 &lt;- df_resample_norm %&gt;% mutate( mean_est = map_dbl( df, ~ summarize(.x, mean_est = mean(x)) %&gt;% pull(mean_est) ) ) df_q1 ## # Bootstrap sampling ## # A tibble: 1,000 × 4 ## splits id df mean_est ## &lt;list&gt; &lt;chr&gt; &lt;list&gt; &lt;dbl&gt; ## 1 &lt;split [50/16]&gt; Bootstrap0001 &lt;tibble [50 × 1]&gt; -0.0667 ## 2 &lt;split [50/21]&gt; Bootstrap0002 &lt;tibble [50 × 1]&gt; -0.244 ## 3 &lt;split [50/20]&gt; Bootstrap0003 &lt;tibble [50 × 1]&gt; 0.0499 ## 4 &lt;split [50/15]&gt; Bootstrap0004 &lt;tibble [50 × 1]&gt; -0.0747 ## 5 &lt;split [50/21]&gt; Bootstrap0005 &lt;tibble [50 × 1]&gt; 0.00831 ## 6 &lt;split [50/16]&gt; Bootstrap0006 &lt;tibble [50 × 1]&gt; -0.197 ## 7 &lt;split [50/19]&gt; Bootstrap0007 &lt;tibble [50 × 1]&gt; -0.252 ## 8 &lt;split [50/17]&gt; Bootstrap0008 &lt;tibble [50 × 1]&gt; -0.239 ## 9 &lt;split [50/17]&gt; Bootstrap0009 &lt;tibble [50 × 1]&gt; -0.189 ## 10 &lt;split [50/18]&gt; Bootstrap0010 &lt;tibble [50 × 1]&gt; -0.136 ## # … with 990 more rows The following test will verify that your df_q1 is correct: ## NOTE: No need to change this! assertthat::assert_that( assertthat::are_equal( df_q1 %&gt;% pull(mean_est), v_mean_est ) ) ## [1] TRUE print(&quot;Great job!&quot;) ## [1] &quot;Great job!&quot; What we have now in df_q1 %&gt;% pull(mean_est) is an approximation of the sampling distribution for the mean estimate. Remember that a confidence interval is a construction based on the sampling distribution, so this is the object we need! From this point, our job would be to work the mathematical manipulations necessary to construct a confidence interval from the quantiles of df_q1 %&gt;% pull(mean_est). Thankfully, the rsample package has already worked out those details for us! The rsample function int_pctl() will compute (percentile) confidence intervals from a bootstrap resampling, but we need to compute our own statistics. Remember the fitdistr() function from the previous exercise? # NOTE: No need to change this demo code df_data_norm %&gt;% pull(x) %&gt;% fitdistr(densfun = &quot;normal&quot;) %&gt;% tidy() ## # A tibble: 2 × 3 ## term estimate std.error ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 mean -0.124 0.131 ## 2 sd 0.923 0.0923 The output of fitdistr(), after run through tidy(), is exactly what int_pctl() expects. Note that the output here is a tibble with a term column and two statistics: the estimate and the std.error. To use int_pctl(), we’ll have to provide statistics in this compatible form. 47.1.2 q2 Modify the code below following recall-fitdistr to provide tidy results to int_pctl(). Hint: You should only have to modify the formula (~) line. df_q2 &lt;- df_resample_norm %&gt;% mutate( estimates = map( splits, ~ analysis(.x) %&gt;% pull(x) %&gt;% fitdistr(densfun = &quot;normal&quot;) %&gt;% tidy() ) ) # NOTE: The following function call will work once you correctly edit the code above int_pctl(df_q2, estimates) ## # A tibble: 2 × 6 ## term .lower .estimate .upper .alpha .method ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 mean -0.378 -0.118 0.124 0.05 percentile ## 2 sd 0.750 0.911 1.07 0.05 percentile Once you learn how to provide statistics in the form that int_pctl() is expecting, you’re off to the races! You can use the bootstrap to compute confidence intervals for very general settings. One of the important things to remember is that the bootstrap is an approximation. The bootstrap relies on a number of assumptions; there are many, but two important ones are: The data are representative of the population Resampling is performed sufficiently many times The next two tasks will study what happens when these two assumptions are not met. 47.1.3 q3 (Representative sample) Read the following code before running it, and make a hypothesis about the result. Is the sample entering bootstraps() representative of the population rnorm(mean = 0, sd = 1)? How are the bootstrap results affected? ## TASK: Read this code; will the data be representative of the population ## rnorm(mean = 0, sd = 1)? tibble(x = rnorm(n = 100)) %&gt;% filter(x &lt; 0) %&gt;% bootstraps(times = 1000) %&gt;% mutate( estimates = map( splits, ~ analysis(.x) %&gt;% pull(x) %&gt;% fitdistr(densfun = &quot;normal&quot;) %&gt;% tidy() ) ) %&gt;% int_pctl(estimates) ## # A tibble: 2 × 6 ## term .lower .estimate .upper .alpha .method ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 mean -0.965 -0.770 -0.605 0.05 percentile ## 2 sd 0.425 0.572 0.718 0.05 percentile Observations: The sample is not at all representative; we are totally missing all positive samples. Correspondingly, the mean is much lower than it should be, and the standard deviation is too small. The following code generates 100 different samples from a normal distribution (each with n = 10), and computes a very coarse bootstrap for each one. 47.1.4 q4 (Number of replicates) First run this code, and comment on whether the approximate coverage probability is close to the nominal 0.95. Increase the value of n_boot and re-run; at what point does the coverage probability approach the desired 0.95? Note: At higher values of n_boot, the following code can take a long while to run. I recommend keeping n_boot &lt;= 1000. ## TASK: Run this code, set.seed(101) times &lt;- 100 # Number of bootstrap resamples df_q4 &lt;- map_dfr( seq(1, 100), # Number of replicates function(repl) { tibble(x = rnorm(10)) %&gt;% bootstraps(times = times) %&gt;% mutate( estimates = map( splits, ~ analysis(.x) %&gt;% pull(x) %&gt;% fitdistr(densfun = &quot;normal&quot;) %&gt;% tidy() ) ) %&gt;% int_pctl(estimates, alpha = 1 - 0.95) %&gt;% mutate(repl = repl) } ) ## Warning: Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Warning in bootstraps(., times = times): Some assessment sets contained zero ## rows. ## Warning: Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Warning in bootstraps(., times = times): Some assessment sets contained zero ## rows. ## Warning: Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Recommend at least 1000 non-missing bootstrap resamples for terms: `mean`, `sd`. ## Estimate the coverage probability of the bootstrap intervals df_q4 %&gt;% filter(term == &quot;mean&quot;) %&gt;% mutate(cover = (.lower &lt;= 0) &amp; (0 &lt;= .upper)) %&gt;% summarize(mean(cover)) ## # A tibble: 1 × 1 ## `mean(cover)` ## &lt;dbl&gt; ## 1 0.9 Observations: I find a coverage probability around 0.73 at n_boot = 10. This is much smaller than desired. At n_boot = 1000 I find an estimated coverage probability around 0.88, which is closer but not perfect. Aside: The rsample function int_pctl actually complains when you give it fewer than 1000 replicates. Since you’ll usually be running the bootstrap only a handful of times (rather than 100 above), you need not be stingy with bootstrap replicates. Do at least 1000 in most cases. 47.2 A Worked Example: Probability Estimate To finish, I’ll present some example code on how you can apply the bootstrap to a more complicated problem. In the previous exercise e-stat08-fit-dist we estimated a probability based on a fitted distribution. Now we have the tools to produce a bootstrap-approximated a confidence interval for that probability estimate. Remember that we had the following setup: sampling from a weibull distribution and estimating parameters with fitdistr(). ## NOTE: No need to change this example code set.seed(101) df_data_w &lt;- tibble(y = rweibull(50, shape = 2, scale = 4)) pr_true &lt;- pweibull(q = 2, shape = 2, scale = 4) df_data_w %&gt;% pull(y) %&gt;% fitdistr(densfun = &quot;weibull&quot;) %&gt;% tidy() ## # A tibble: 2 × 3 ## term estimate std.error ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 shape 2.18 0.239 ## 2 scale 4.00 0.274 In order to approximate a confidence interval for our probability estimate, we’ll need to provide the probability value in the form that int_pctl() expects. Below I define a helper function that takes each split, extracts the estimated parameters, and uses them to compute a probability estimate. I then add that value as a new row to the output of tidy(), making sure to populate the columns estimate and term. ## NOTE: No need to change this example code; but feel free to adapt it! fit_fun &lt;- function(split) { ## Fit distribution df_tmp &lt;- analysis(split) %&gt;% pull(y) %&gt;% fitdistr(densfun = &quot;weibull&quot;) %&gt;% tidy() ## Extract statistics scale_est &lt;- df_tmp %&gt;% filter(term == &quot;scale&quot;) %&gt;% pull(estimate) shape_est &lt;- df_tmp %&gt;% filter(term == &quot;shape&quot;) %&gt;% pull(estimate) ## Add probability estimate in tidy form df_tmp %&gt;% bind_rows(tibble( estimate = pweibull(q = 2, scale = scale_est, shape = shape_est), term = &quot;pr&quot; )) } df_resample_pr &lt;- bootstraps(df_data_w, times = 1000) %&gt;% mutate(estimates = map(splits, fit_fun)) ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced Now I’ve got all the information I need to pass to df_resample_pr: ## NOTE: No need to change this example code int_pctl(df_resample_pr, estimates) ## # A tibble: 3 × 6 ## term .lower .estimate .upper .alpha .method ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 pr 0.121 0.197 0.278 0.05 percentile ## 2 scale 3.50 3.99 4.51 0.05 percentile ## 3 shape 1.87 2.23 2.68 0.05 percentile pr_true ## [1] 0.2211992 When I run this, I find that the confidence interval contains pr_true as one might hope! 47.3 Notes [1] This is because rsample does some fancy stuff under the hood. Basically bootstraps does not make any additional copies of the data; the price we pay for this efficiency is the need to call analysis(). [2] For a slightly more mathematical treatment of the bootstrap, try these MIT course notes "],["data-a-simple-data-pipeline.html", "48 Data: A Simple Data Pipeline 48.1 Reading a Sheet with googlesheets4 48.2 Public sheets 48.3 Private sheets 48.4 Setting up a Form + Sheet 48.5 Notes", " 48 Data: A Simple Data Pipeline Purpose: Analyzing existing data is helpful, but it’s even more important to be able to obtain relevant data. One kind of data is survey data, which is helpful for understanding things about people. In this short exercise you’ll learn how to set up your own survey, link it to a cloud-based sheet, and automatically download that sheet for local data analysis. Reading: (None, this exercise is the reading) library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✔ ggplot2 3.4.0 ✔ purrr 1.0.1 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.5.0 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() library(googlesheets4) 48.1 Reading a Sheet with googlesheets4 The googlesheets4 package provides a convenient interface to Google Sheet’s API [1]. We’ll use this to set up a very simple data pipeline: A means to collect data at some user-facing point, and load that data for analysis. 48.2 Public sheets Back in c02-michelson you actually used googlesheets4 to load the speed of light data: ## Note: No need to edit this chunk! url_michelson &lt;- &quot;https://docs.google.com/spreadsheets/d/1av_SXn4j0-4Rk0mQFik3LLr-uf0YdA06i3ugE6n-Zdo/edit?usp=sharing&quot; ## Put googlesheets4 in &quot;deauthorized mode&quot; gs4_deauth() ## Get sheet metadata ss_michelson &lt;- gs4_get(url_michelson) ## Load the sheet as a dataframe df_michelson &lt;- read_sheet(ss_michelson) %&gt;% select(Date, Distinctness, Temp, Velocity) %&gt;% mutate(Distinctness = as_factor(Distinctness)) ## ✔ Reading from &quot;michelson1879&quot;. ## ✔ Range &#39;Sheet1&#39;. df_michelson %&gt;% glimpse ## Rows: 100 ## Columns: 4 ## $ Date &lt;dttm&gt; 1879-06-05, 1879-06-07, 1879-06-07, 1879-06-07, 1879-06-… ## $ Distinctness &lt;fct&gt; 3, 2, 2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2, 2, 1, 3, 3, 2, … ## $ Temp &lt;dbl&gt; 76, 72, 72, 72, 72, 72, 83, 83, 83, 83, 83, 90, 90, 71, 7… ## $ Velocity &lt;dbl&gt; 299850, 299740, 299900, 300070, 299930, 299850, 299950, 2… I made this sheet public so that anyone can access it. The line gs4_deauth() tells the googlesheets4 package not to ask for login information; this way you can easily load this public sheet, even without having a Google account. But what if we want to load one of our own private data sheets? 48.3 Private sheets In order to load a private data sheet, you’ll need to authorize googlesheets4 to use your Google account. The following line should open a browser window that will ask for your permissions. ## NOTE: No need to edit; run to authorize R to use your google account gs4_auth() Now that you’ve authorized your account, let’s create a very simple data-collection pipeline. 48.4 Setting up a Form + Sheet One convenient feature of Google Sheets is that it nicely integrates with Google Forms: We can create a form (a survey) and link it to a sheet. Let’s do that! 48.4.1 q1 Create your own form. Go to Google Forms and create a new form. Add at least one question. 48.4.2 q2 Navigate to the Responses tab and click Create Spreadsheet. Select Create a new spreadsheet and accept the default name. Create spreadsheet linked to form 48.4.3 q3 Copy the URL for your new sheet and copy it below. Run the following chunk to load your (probably empy) sheet. ## NOTE: I&#39;m not going to put a URL here, as one of my personal sheets ## won&#39;t work for you.... url_custom_sheet &lt;- &quot;&quot; df_custom_sheet &lt;- read_sheet(url_custom_sheet) df_custom_sheet %&gt;% glimpse() Now as results from your survey come in, you can simply re-run this notebook to grab the most recent version of your data for local analysis. This is very simple but surprisingly powerful: I use a pipeline exactly like this for the exit tickets! 48.5 Notes [1] It’s googlesheets4 because the package is designed for V4 of Google Sheet’s API. "],["vis-small-multiples.html", "49 Vis: Small Multiples 49.1 Small Multiples 49.2 Organizing Factors", " 49 Vis: Small Multiples Purpose: A powerful idea in visualization is the small multiple. In this exercise you’ll learn how to design and create small multiple graphs. Reading: (None; there’s a bit of reading here.) “At the heart of quantitative reasoning is a single question: Compared to what?” Edward Tufte on small multiples. 49.1 Small Multiples Facets in ggplot allow us to apply the ideas of small multiples. As an example, consider the following graph: economics %&gt;% pivot_longer( names_to = &quot;variable&quot;, values_to = &quot;value&quot;, cols = c(pce, pop, psavert, uempmed, unemploy) ) %&gt;% ggplot(aes(date, value)) + geom_line() + facet_wrap(~variable, scales = &quot;free_y&quot;) The “multiples” are the different panels; above we’ve separated the different variables into their own panel. This allows us to compare trends simply by lookin across at different panels. The faceting above works well for comparing trends: It’s clear by inspection whether the various trends are increasing, decreasing, etc. The next example with the mpg data is not so effective: ## NOTE: No need to edit; study this example mpg %&gt;% ggplot(aes(displ, hwy)) + geom_point() + facet_wrap(~class) With these scatterplots it’s more difficult to “keep in our heads” the absolute positions of the other points as we look across the multiples. Instead we could add some “ghost” points: ## NOTE: No need to edit; study this example mpg %&gt;% ggplot(aes(displ, hwy)) + ## A bit of a trick; remove the facet variable to prevent faceting geom_point( data = . %&gt;% select(-class), color = &quot;grey80&quot; ) + geom_point() + facet_wrap(~class) + theme_minimal() There’s a trick to getting the visual above; removing the facet variable from an internal dataframe prevents the faceting of that layer. This combined with a second point layer gives the “ghost” point effect. The presence of these “ghost” points provides more context; they facilitate the “Compared to what?” question that Tufte puts at the center of quantitative reasoning. 49.1.1 q1 Edit the following figure to use the “ghost” point trick above. ## TODO: Edit this code to facet on `cut`, but keep &quot;ghost&quot; points to aid in ## comparison. diamonds %&gt;% ggplot(aes(carat, price)) + geom_point() diamonds %&gt;% ggplot(aes(carat, price)) + geom_point( data = . %&gt;% select(-cut), color = &quot;grey80&quot; ) + geom_point() + facet_wrap(~cut) 49.2 Organizing Factors Sometimes your observations will organize into natural categories. In this case facets are a great way to group your observations. For example, consider the following figure: mpg %&gt;% group_by(model) %&gt;% filter(row_number(desc(year)) == 1) %&gt;% ungroup() %&gt;% mutate( manufacturer = fct_reorder(manufacturer, hwy), model = fct_reorder(model, desc(hwy)) ) %&gt;% ggplot(aes(hwy, model)) + geom_point() + facet_grid(manufacturer~., scale = &quot;free_y&quot;, space = &quot;free&quot;) + theme( strip.text.y = element_text(angle = 0) ) There’s a lot going on this figure, including a number of subtle points. Let’s list them out: I filter on the latest model with the row_number call (not strictly necessary). I’m re-ordering both the manufacturer and model on hwy. However, I reverse the order of model to get a consistent “descending” pattern. I set both the scale and space arguments of the facet call; without those the spacing would be messed up (try it!). I rotate the facet labels to make them more readable. 49.2.1 q2 Create a small multiple plot like ex-mpg-manufacturer above. Keep in mind the idea of “compared to what?” when deciding which variables to place close to one another. ## TODO: Create a set of small multiples plot from these data as_tibble(iris) %&gt;% pivot_longer( names_to = &quot;part&quot;, values_to = &quot;length&quot;, cols = -Species ) ## # A tibble: 600 × 3 ## Species part length ## &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 setosa Sepal.Length 5.1 ## 2 setosa Sepal.Width 3.5 ## 3 setosa Petal.Length 1.4 ## 4 setosa Petal.Width 0.2 ## 5 setosa Sepal.Length 4.9 ## 6 setosa Sepal.Width 3 ## 7 setosa Petal.Length 1.4 ## 8 setosa Petal.Width 0.2 ## 9 setosa Sepal.Length 4.7 ## 10 setosa Sepal.Width 3.2 ## # … with 590 more rows as_tibble(iris) %&gt;% pivot_longer( names_to = &quot;part&quot;, values_to = &quot;length&quot;, cols = -Species ) %&gt;% ggplot(aes(length, Species)) + geom_point() + facet_grid(part~., scale = &quot;free_y&quot;, space = &quot;free&quot;) + theme( strip.text.y = element_text(angle = 0) ) I chose to put the measurements of the same part close together, to facilitate comparison of the common plant features across different species. "],["stats-introduction-to-hypothesis-testing.html", "50 Stats: Introduction to Hypothesis Testing 50.1 A Full Example 50.2 Pick your population 50.3 Set up your hypotheses and actions 50.4 Compute 50.5 Different Scenario, Different Hypotheses 50.6 Proportion Ideal 50.7 Hypotheses and Actions 50.8 Closing Thoughts 50.9 The big reveal 50.10 End notes", " 50 Stats: Introduction to Hypothesis Testing Purpose: Part of the payoff of statistics is to support making decisions under uncertainty. To frame these decisions we will use the framework of hypothesis testing. In this exercise you’ll learn how to set up competing hypotheses and potential actions, based on different scenarios. Reading: Statistical Inference in One Sentence (9 min) library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✔ ggplot2 3.4.0 ✔ purrr 1.0.1 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.5.0 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() library(rsample) 50.1 A Full Example You are considering buying a set of diamonds in bulk. The prospective vendor is willing to sell you 100 diamonds at $1700 per diamond. You will not get to see the specific diamonds before buying, though. To convince you, the vendor gives you a detailed list of a prior package of bulk diamonds they sold recently—they tell you this is representative of the packages they sell. This is a weird contract, but it’s intriguing. Let’s use statistics to help determine whether or not to take the deal. 50.2 Pick your population For the sake of this exercise, let’s assume that df_population is the entire set of diamonds the vendor has in stock. ## NOTE: No need to change this! df_population &lt;- diamonds %&gt;% filter(carat &lt; 1) Important Note: No peeking! While I’ve defined df_population here, you should not look at its values until the end of the exercise. While we do have access to the entirety of the population, in most real problems we’ll only have a sample. The function slice_sample() allows us to choose a random sample from a dataframe. ## NOTE: No need to change this! set.seed(101) df_sample &lt;- df_population %&gt;% slice_sample(n = 100) 50.3 Set up your hypotheses and actions Based on the contract above, our decision threshold should be related to the sale price the vendor quotes. ## NOTE: No need to change this; this will be our decision threshold price_threshold &lt;- 1700 ## NOTE: This is for exercise-design purposes: What are the true parameters? df_population %&gt;% group_by(cut) %&gt;% summarize(price = mean(price)) %&gt;% bind_rows( df_population %&gt;% summarize(price = mean(price)) %&gt;% mutate(cut = &quot;(All)&quot;) ) ## # A tibble: 6 × 2 ## cut price ## &lt;chr&gt; &lt;dbl&gt; ## 1 Fair 2092. ## 2 Good 1793. ## 3 Very Good 1732. ## 4 Premium 1598. ## 5 Ideal 1546. ## 6 (All) 1633. In order to do hypothesis testing, we need to define null and alternative hypotheses. These two hypotheses are competing theories for the state of the world Furthermore, we are aiming to use hypothesis testing to support making a decision. To that end, we’ll also define a default action (if we fail to reject the null), and an alternative action (if we find our evidence sufficiently convincing so as to change our minds). For this buying scenario, we feel that the contract is pretty weird: We’ll set up our null hypothesis to assume the vendor is trying to rip us off. In order to make this hypothesis testable, we’ll need to make it quantitative. One way make our hypothesis quantitative is to think about the mean price of diamonds in the population: If the diamonds are—on average—less expensive than the price_threshold, then on average we’ll tend to get a set of diamonds that are worth less than what we paid. This will be our null hypothesis. Consequently, our default action will be to buy no diamonds from this vendor. In standard statistics notation, this is how we denote our null and alternative hypotheses: H_0 (Null hypothesis) The mean price of all diamonds in the population is less than the threshold price_threshold. - Default action: Buy no diamonds H_A (Alternative hypothesis) The mean price of all diamonds in the population is equal to or greater than the threshold price_threshold. - Alternative action: Buy diamonds in bulk 50.4 Compute 50.4.1 q1 Based on your results, can you reject the null hypothesis H_0 for the population with a 95-percent confidence interval? ## TASK: Compute a confidence interval on the mean, use to answer the question df_sample %&gt;% summarize( price_mean = mean(price), price_sd = sd(price), price_lo = price_mean - 1.96 * price_sd / sqrt(n()), price_hi = price_mean + 1.96 * price_sd / sqrt(n()) ) %&gt;% select(price_lo, price_hi) ## # A tibble: 1 × 2 ## price_lo price_hi ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1418. 1856. price_threshold ## [1] 1700 Observations: Based on the CI above, we cannot reject the null hypothesis H_0. Since we do not reject H_0 we take our default action of buying no diamonds from the vendor. 50.5 Different Scenario, Different Hypotheses 50.6 Proportion Ideal Let’s imagine a different scenario: We have a lead on a buyer of engagement rings who is obsessed with well-cut diamonds. If we could buy at least 50 diamonds with cut Premium or Ideal (what we’ll call “high-cut”), we could easily recoup the cost of the bulk purchase. If the proportion of high-cut diamonds in the vendor’s population is greater than 50 percent, we stand a good chance of making a lot of money. Unfortunately, I haven’t taught you any techniques for estimating a CI for a proportion. However in e-stat09-bootstrap we learned a general approximation technique: the bootstrap. Let’s put that to work to estimate a confidence interval for the proportion of high-cut diamonds in the population. 50.7 Hypotheses and Actions Let’s redefine our hypotheses to match the new scenario. H_0 (Null hypothesis) The proportion of high-cut diamonds in the population is less than 50 percent. - Default action: Buy no diamonds H_A (Alternative hypothesis) The proportion of high-cut diamonds in the population is equal to or greater than 50 percent. - Alternative action: Buy diamonds in bulk Furthermore, let’s change our decision threshold from 95-percent confidence to a higher 99-percent confidence. 50.7.1 q2 Use the techniques you learned in e-stat09-bootstrap to estimate a 99-percent confidence interval for the population proportion of high-cut diamonds. Can you reject the null hypothesis? What decision do you take? Hint 1: Remember that you can use mean(X == \"value\") to compute the proportion of cases in a sample with variable X equal to \"value\". You’ll need to figure out how to combine the cases of Premium and Ideal. Hint 2 int_pctl() takes an alpha keyword argument; this is simply alpha = 1 - confidence. ## TASK: Estimate a confidence interval for the proportion of high-cut diamonds ## in the population. Look to `e-stat09-bootstrap` for starter code. set.seed(101) fit_fun &lt;- function(split) { analysis(split) %&gt;% summarize(estimate = mean((cut == &quot;Premium&quot;) | (cut == &quot;Ideal&quot;))) %&gt;% mutate(term = &quot;proportion_high&quot;) } df_resample_total_price &lt;- bootstraps(df_sample, times = 1000) %&gt;% mutate(estimates = map(splits, fit_fun)) int_pctl(df_resample_total_price, estimates, alpha = 0.01) ## # A tibble: 1 × 6 ## term .lower .estimate .upper .alpha .method ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 proportion_high 0.530 0.640 0.750 0.01 percentile Observations: Based on the CI above, we can reject the null hypothesis H_0. Since we reject H_0 we take our alternative action and buy the diamonds! 50.8 Closing Thoughts 50.9 The big reveal To close this exercise, let’s reveal whether our chosen hypotheses matched the underlying population. 50.9.1 q3 Compute the population mean price for the diamonds. Did you reject the null hypothesis? ## TASK: Compute the population mean of diamond price df_population %&gt;% summarize(price = mean(price)) ## # A tibble: 1 × 1 ## price ## &lt;dbl&gt; ## 1 1633. price_threshold ## [1] 1700 Observations: When I did q1, I did not reject the null. Note the weird wording there: did not reject the null, rathern than “accepted the null”. In this hypothesis testing framework we never actually accept the null hypothesis, we can only fail to reject the null. What this means is that we still maintain the possibility that the null is false, and all we can say for sure is that our data are not sufficient to reject the null hypothesis. In other words, when we fail to reject the null hypothesis “we’ve learned nothing.” Learning nothing isn’t a bad thing though! It’s an important part of statistics to recognize when we’ve learned nothing. 50.9.2 q4 Compute the proportion of high-cut diamonds in the population. Did you reject the null hypothesis? ## TASK: Compute the population proportion of high-cut diamonds df_population %&gt;% summarize(proportion = mean((cut == &quot;Premium&quot;) | (cut == &quot;Ideal&quot;))) ## # A tibble: 1 × 1 ## proportion ## &lt;dbl&gt; ## 1 0.667 Observations: When I did q2 I did reject the null hypothesis. It happens that this was the correct choice; the true proportion of high-cut diamonds is greater than 50-percent. 50.10 End notes Note that the underlying population is identical in the two settings above, but the “correct” decision is different. This helps illustrate that math alone cannot help you frame a reasonable hypothesis. Ultimately, you must understand the situation you are in, and the decisions you are considering. If you’ve taken a statistics course, you might be wondering why I’m talking about hypothesis testing without introducing p-values. I feel that confidence invervals more obviously communicate the uncertainty in results, in line with Andrew Gelman’s suggestion that we embrace uncertainty. The penalty we pay working with (two-sided) confidence intervals is a reduction in statistical power. "],["stats-confidence-vs-prediction-intervals.html", "51 Stats: Confidence vs Prediction Intervals 51.1 Introduction: Confidence vs Prediction Intervals 51.2 Specific Mathematical Example: Normal Distribution 51.3 Applications of CI and PI", " 51 Stats: Confidence vs Prediction Intervals Purpose: There are multiple kinds of statistical intervals, and different intervals are useful for answering different questions. In this exercise, we’ll learn about prediction intervals: How they differ from confidence intervals, and when we would use a CI versus a PI. Reading: (None, this is the reading) library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✔ ggplot2 3.4.0 ✔ purrr 1.0.1 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.5.0 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() library(modelr) library(broom) ## ## Attaching package: &#39;broom&#39; ## The following object is masked from &#39;package:modelr&#39;: ## ## bootstrap ## Helper function to compute uncertainty bounds add_uncertainties &lt;- function(data, model, prefix = &quot;pred&quot;, ...) { df_fit &lt;- stats::predict(model, data, ...) %&gt;% as_tibble() %&gt;% rename_with(~ str_c(prefix, &quot;_&quot;, .)) bind_cols(data, df_fit) } 51.1 Introduction: Confidence vs Prediction Intervals There are multiple kinds of statistical intervals: We have already discussed confidence intervals (in e-stat06-clt), now we’ll discuss prediction intervals. 51.2 Specific Mathematical Example: Normal Distribution To help distinguish between between confidence intervals (CI) and prediction intervals (PI), let’s first limit our attention to normal distributions (where the math is easy). We saw in e-stat06-clt that a confidence interval is a way to summarize our knowledge about an estimated parameter; for instance, a confidence interval \\([l, u]\\) for the sample mean \\(\\overline{X}\\) of a normal distribution at confidence level \\(C\\) would be \\[C = \\mathbb{P}\\left[l &lt; \\overline{X} &lt; u\\right] = \\mathbb{P}\\left[\\frac{l - \\mu}{\\sigma / \\sqrt{n}} &lt; Z &lt; \\frac{u - \\mu}{\\sigma / \\sqrt{n}}\\right].\\] Note the \\(\\sigma / \\sqrt{n}\\) in the denominator on the right; this is the standard error for the sample mean \\(\\overline{X}\\). A CI is a useful way to summarize our uncertainty about an estimated parameter. A different kind of interval is a prediction interval (PI). Rather than summarizing information about an estimated parameter, a PI summarizes information about future observations. The following equation defines a prediction interval for a normal distribution assuming we magically know the mean and variance: \\[P = \\mathbb{P}\\left[l &lt; X &lt; u\\right] = \\mathbb{P}\\left[\\frac{l - \\mu}{\\sigma} &lt; Z &lt; \\frac{u - \\mu}{\\sigma}\\right]\\] Observations: Note that the CI equation above has a dependence on \\(n\\); as we gather more data the interval will tend to narrow. Note that the PI equation above have no dependence on \\(n\\); when we turn the “magic” off and have to estimate mean, sd from data a dependence on \\(n\\) shows up. However, even if we had infinite data (recovering the “magic” equation above), the interval would still not collapse to zero width. 51.2.1 q1 Check your understanding; I provide code below to compute a confidence interval for the sample mean when sampling from rnorm(mean = 1, sd = 2) with n = 400. Modify the code to compute a prediction interval for the same underlying normal distribution. ## NOTE: No need to edit this setup mu &lt;- 1 # Normal mean sd &lt;- 2 # Normal variance n &lt;- 400 # Number of samples ci_lwr &lt;- mu - 1.96 * sd / sqrt(n) ci_upr &lt;- mu + 1.96 * sd / sqrt(n) pi_lwr &lt;- mu - 1.96 * sd pi_upr &lt;- mu + 1.96 * sd Use the following tests to check your work. ## NOTE: No need to change this assertthat::assert_that(abs(pi_lwr + 2.92) &lt;= 1e-6) ## [1] TRUE assertthat::assert_that(abs(pi_upr - 4.92) &lt;= 1e-6) ## [1] TRUE print(&quot;Well done!&quot;) ## [1] &quot;Well done!&quot; Our first observation about CI and PI is that PI will tend to be wider than CI! That’s because they are telling us fundamentally different things about our population. Consequently, we use CI and PI for very different applications. 51.3 Applications of CI and PI A confidence interval is most likely to be useful when we care more about aggregates—rather than the individual observations. A prediction interval is most likely to be useful when we care more about individual observations—rather than the aggregate behavior. Let’s think back to e-stat10-hyp-intro, where we were buying many diamonds. In that case we constructed confidence intervals on the mean price of diamonds and on the proportion of high-cut diamonds. Since we cared primarily about the properties of many diamonds, it made sense to use confidence interval to support our decision making. Now let’s think of a different application: Imagine we were going to purchase just one diamond. In that case we don’t care about the mean price; we care about the single price of the one diamond we’ll ultimately end up buying. In this case, we would be better off constructing a prediction interval for the price of diamonds from the population—this will give us a sense of the range of values we might encounter in our purchase. Prediction intervals are also used for other applications, such as defining a “standard reference range” for blood tests: Since doctors care about the individual patients—we want every patient to survive, not just mythical “average” patients!—it is more appropriate to use a prediction interval for this application. Let’s apply these ideas to the diamonds dataset: ## NOTE: No need to edit this setup # Create a test-validate split set.seed(101) diamonds_randomized &lt;- diamonds %&gt;% slice(sample(dim(diamonds)[1])) diamonds_train &lt;- diamonds_randomized %&gt;% slice(1:10000) diamonds_validate &lt;- diamonds_randomized %&gt;% slice(10001:20000) We’re about to blindly apply the normal-assuming formulae, but before we do that, let’s quickly inspect our data to see how normal or not they are: ## NOTE: No need to edit this chunk bind_rows( diamonds_train %&gt;% mutate(source = &quot;Train&quot;), diamonds_validate %&gt;% mutate(source = &quot;Validate&quot;) ) %&gt;% ggplot(aes(price)) + geom_histogram(bins = 100) + facet_grid(source ~ .) Take a quick look at the plot above, and make a prediction (to yourself) whether the normally-approximated CI and PI will behave well in this case. Then continue on to q2. 51.3.1 q2 Using the formulas above, estimate CI and PI using diamonds_train. Visualize the results using the chunk q2-vis below, and answer the questions under observations. df_q2 &lt;- diamonds_train %&gt;% summarize( price_mean = mean(price), price_sd = sd(price), price_n = n() ) %&gt;% mutate( ci_lwr = price_mean - 1.96 * price_sd / sqrt(n), ci_upr = price_mean + 1.96 * price_sd / sqrt(n), pi_lwr = price_mean - 1.96 * price_sd, pi_upr = price_mean + 1.96 * price_sd ) %&gt;% select(ci_lwr, ci_upr, pi_lwr, pi_upr) df_q2 ## # A tibble: 1 × 4 ## ci_lwr ci_upr pi_lwr pi_upr ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3561. 4340. -3842. 11743. Use the following code to visualize your results; answer the questions below. ## NOTE: No need to edit this chunk df_q2 %&gt;% pivot_longer( names_to = c(&quot;type&quot;, &quot;.value&quot;), names_sep = &quot;_&quot;, cols = everything() ) %&gt;% ggplot() + geom_point( data = diamonds_validate, mapping = aes(x = &quot;&quot;, y = price), position = position_jitter(width = 0.3), size = 0.2 ) + geom_errorbar(aes(x = &quot;&quot;, ymin = lwr, ymax = upr, color = type)) + guides(color = FALSE) + facet_grid(~ type) ## Warning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use &quot;none&quot; instead as ## of ggplot2 3.3.4. Observations: Visually the CI and PI seem decent. The CI seems to be located in the “middle” of the data. The PI covers a wide fraction of the data. However, its lower bound goes negative, which is undesirable. I would check the CI against the population mean (if available) or a validation mean. I would check if the PI contains an appropriate fraction of prices, either from the population (if available), or from validation data. Both the CI and PI above assume a normal distribution and perfectly-known parameters mean, sd. The assumption of perfectly-known parameters is probably ok here (since we have a lot of data), but based on EDA we’ve done before, the assumption of normality is quite poor. 51.3.2 q3 Test whether your CI and PI are constructed correctly: Remember the definitions of what CI and PI are meant to accomplish, and check how closely your intervals agree with the validation data. ## TODO: Devise a test to see if your CI and PI are correctly reflecting ## the diamonds population; use diamonds_validation in your testing ## Testing the CI bind_cols( df_q2 %&gt;% select(ci_lwr, ci_upr), diamonds_validate %&gt;% summarize(price_mean = mean(price)) ) %&gt;% select(ci_lwr, price_mean, ci_upr) ## # A tibble: 1 × 3 ## ci_lwr price_mean ci_upr ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3561. 3917. 4340. ## Testing the PI left_join( diamonds_validate, df_q2 %&gt;% select(pi_lwr, pi_upr), by = character() ) %&gt;% summarize(P_empirical = mean(pi_lwr &lt;= price &amp; price &lt;= pi_upr)) ## # A tibble: 1 × 1 ## P_empirical ## &lt;dbl&gt; ## 1 0.935 Observations: My CI does include the population mean. My PI includes ~0.94 of the validation prices, which is quite close to the 0.95 desired. "],["model-variability-quadrants.html", "52 Model: Variability Quadrants 52.1 Variability 52.2 The Cause-Source Quadrants 52.3 Real vs Induced Source 52.4 Manufacturing structural steel components 52.5 Scenarios 52.6 Analyzing Data", " 52 Model: Variability Quadrants Purpose: All real data have variability: repeated measurements of “the same” quantity tend to result in different values. To help you recognize different kinds of variability and choose a reasonable analysis procedure based on the kind of variability, you will learn about different sources of variability in this exercise. Reading: Conceptual Tools for Handling Uncertainty (a draft chapter from a textbook I’m writing) ## Note: No need to edit this chunk! library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✔ ggplot2 3.4.0 ✔ purrr 1.0.1 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.5.0 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() 52.1 Variability As we’ve seen in this course, real data exhibit variability; that is, repeated measurements of “the same” quantity that result in different values. Variability can arise due to a variety of reasons, and different kinds of variability should be analyzed in different ways. To help make this determination, we’re going to study a theoretical framework for variability. 52.2 The Cause-Source Quadrants As descrinbed in the reading, the cause-source quadrants organize variability into four distinct categories. Today, we’re going to focus on the source axis, and limit our attention to chance causes. Variability quadrants Cause is an idea from statistical quality control (manufacturing); a chance cause is modeled as random, while an assignable cause is thought to be traceable and preventable. Source is an idea from statistics education theory; this concept is explained further below. 52.3 Real vs Induced Source The idea of source can only be understood in the distinction between a scopus and a measurement: The scopus is the quantity that we are seeking to study, while the measurement is a possibly-corrupted version of our scopus. The key insight is that variability can occur both in the scopus value, and in the measurement. Variability quadrants As a simple example: based on our current understanding of physics, the speed of light c is a constant value. Therefore, any variability we see in measurements of c are understood to be induced variability; real variability in c is not considered to be possible. Conversely, our current understanding of physics is that quantum phenomena are fundamentally unpredictable, and can only be described in a statistical sense. This means that quantum phenomena exhibit real variability. Other physical quantities exhibit both real and induced variability. Since the concept of source relies on a choice of scopus, the only way we can make progress with this concept is to consider a specific scenario in detail. 52.4 Manufacturing structural steel components The Context: A manufacturer is producing cast steel parts for a landing gear. The part in question takes a heavy load, and if it fails it will disable the aircraft on the ground. These parts will be manufactured in bulk; approximately 500 will be made and installed in commercial aircraft that will operate for decades. The Scopus: The strength of each steel component—as-manufactured—will ultimately determine whether each aircraft is safe. As we learned in c08-structures, a structure is safe if its applied stress is less than its strength. Therefore, a smaller material strength is a more conservative value for design purposes. 52.5 Scenarios 52.5.1 q1 Imagine the manufacturer selects one part and performs multiple non-destructive tensile tests on that single part, under similar conditions. The measured elasticity from each test is slightly different. Is this variability real or induced? Induced The properties of the component are essentially set at manufacturing time; if multiple measurements on the same part return different values, then the variability is most likely induced by the measurement process. 52.5.2 q2 Imagine the manufacturer selects multiple parts and—for each part—performs multiple non-destructive tensile tests, all under similar conditions. The measured elasticity values for each part are averaged to provide a more reliable estimate for each part. Upon comparing the parts, each averaged value is fairly different. Is this variability real or induced? Real The properties of the component are essentially set at manufacturing time; but no manufacturing process can create items with identical properties. Particularly if variability remains after induced variability has been controlled and eliminated (as described in the prompt), then the remaining variability is real. 52.5.3 q3 Now the manufacturer selects multiple parts and performs a destructive tensile test to characterize the strength of each part, with tests carried out under similar conditions. The measured strength values exhibit a fair amount of variability. Is this variability real or induced? Without more information, it is impossible to say. It is likely a combination of real and induced sources. Real variability can arise from the manufacturing process, and induced variability can arise from the measurement. Since the measurement is destructive, we cannot use multiple measurements to control the induced variability. Note that it would generally be conservative to treat all of the variability in a strength as real; this would lead to parts that are heavier but safer than they need to be. 52.6 Analyzing Data The following generates data with both noise and deviation set.seed(101) df_meas &lt;- map_dfr( 1:30, function(i) { Y_deviation &lt;- rlnorm(n = 1, meanlog = 2) Y_noise &lt;- rnorm(n = 5, sd = 1) tibble(Y = Y_deviation + Y_noise) %&gt;% mutate(id_sample = i, id_meas = row_number()) } ) id_sample - represents an individual part id_meas - represents an individual measurement, with multiple carried out on each part Y - an individual measurement, identified by id_sample and id_meas If we make a simple histogram, we can see that the measured value Y is highly variable: df_meas %&gt;% ggplot(aes(Y)) + geom_histogram(bins = 30) However, these data exhibit multiple sources of variability. The following questions will help you learn how to analyze data in light of this mixed variability. 52.6.1 q4 Inspect the following graph. Answer the questions under observations below. ## NOTE: No need to edit; run and inspect df_meas %&gt;% ggplot(aes(id_sample, Y)) + geom_point( data = . %&gt;% group_by(id_sample) %&gt;% summarize(Y = mean(Y)), color = &quot;red&quot;, size = 1 ) + geom_point(size = 0.2) + theme_minimal() Observations - Based on the visual, the variability due to deviation is obviously much larger than the variability due to noise: There is considerably more scatter between the red dots (each sample’s measurement mean), than there is scatter around each red dot. We can make this quantitative with an analysis of variance: fit_meas &lt;- df_meas %&gt;% lm(formula = Y ~ id_sample + id_meas) anova(fit_meas) ## Analysis of Variance Table ## ## Response: Y ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## id_sample 1 26.0 26.040 0.1609 0.6889 ## id_meas 1 0.9 0.869 0.0054 0.9417 ## Residuals 147 23792.3 161.852 A random effects model would be more appropriate for these data, but the fixed-effects ANOVA above gives a rough quantitative comparison. 52.6.2 q5 Imagine Y represents the measured strength of the cast steel. Would it be safe to simply average all of the values and use that as the material strength for design? No, it would be foolish to take the average of all these strength measurements and use that value for design. In fact, in aircraft design, this approach would be illegal according to Federal Airworthiness Regulations. 52.6.3 q6 Compute the 0.1 quantile of the Y measurements. Would this be a conservative value to use as a material strength for design? ## TODO: Compute the 0.01 quantile of the `Y` values; complete the code below # For comparison, here&#39;s the mean of the data Y_mean &lt;- df_meas %&gt;% summarize(Y_mean = mean(Y)) %&gt;% pull(Y_mean) Y_lo &lt;- df_meas %&gt;% summarize(Y_lo = quantile(Y, p = 0.1)) %&gt;% pull(Y_lo) # Compare the values Y_mean ## [1] 12.30886 Y_lo ## 10% ## 2.011725 Use the following to check your work. ## NO NEED TO EDIT; use this to check your work assertthat::assert_that(abs(as.numeric(Y_lo) - 2.0117) &lt; 1e-3) ## [1] TRUE print(&quot;Nice!&quot;) ## [1] &quot;Nice!&quot; Observations Y_lo is considerably smaller than Y_mean. Yes, the 10% quantile would be a conservative material strength value. This would still not be in compliance with FAA regulations for material properties, but it is certainly better than using the (sample) mean. 52.6.4 q7 The following code reduces the variability due to noise before computing the quantile. Run the code below, and answer the questions under Observations below. ## NOTE: No need to edit; run and answer the questions below Y_lo_improved &lt;- df_meas %&gt;% ## Take average within each sample&#39;s measurements group_by(id_sample) %&gt;% summarize(Y_sample = mean(Y)) %&gt;% ## Take quantile over all the samples summarize(Y_lo = quantile(Y_sample, p = 0.1)) %&gt;% pull(Y_lo) Y_lo_improved ## 10% ## 2.566182 Y_lo ## 10% ## 2.011725 Observations The new value Y_lo_improved is less conservative, but it is also more efficient. This value reduces the effects of noise, which focuses on the design-relevant deviation present in the material property of interest. To compute Y_lo_improved, we needed multiple observations id_meas on each sample id_sample. In reality, we cannot collect a dataset like df_meas for strength properties; this is because we can’t collect more than one strength measurement per sample! It is possible to collect repeated observations with any non-destructive measurement. For material properties, this could include the elasticity, poisson ratio, density, etc. Aside: This kind of statistical experimental design is sometimes called a nested design. "],["vis-improving-graphs.html", "53 Vis: Improving Graphs 53.1 Improve these graphs!", " 53 Vis: Improving Graphs Purpose: Creating a presentation-quality graph is an iterative exercise. There are many different ways to show the same data, some of which are more effective for communication than others. Let’s return to the ideas from “How Humans See Data” and use them to improve upon some graphs: This will give you practice iterating on visuals. Reading: How Humans See Data (Video from prior exercise, for reference) library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✔ ggplot2 3.4.0 ✔ purrr 1.0.1 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.5.0 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() 53.1 Improve these graphs! Using the ideas from the reading (video), state some issues with the following graphs. Remember the visual hierarchy: Position along a common scale Position on identical but nonaligned scales Length Angle; Slope (With slope not too close to 0, \\(\\pi/2\\), or \\(\\pi\\).) Area Volume; Density; Color saturation Color hue 53.1.1 q1 Use concepts from the reading to improve the following graph. Make sure your graph shows all the same variables, no more and no fewer. ## NOTE: No need to edit; run and inspect mpg %&gt;% ggplot(aes(manufacturer, cty)) + geom_boxplot() + coord_flip() Create your improved graph here ## TODO: Create an improved version of the graph above ## NOTE: This is just one possibility mpg %&gt;% ggplot(aes(fct_reorder(manufacturer, cty), cty)) + geom_boxplot() + coord_flip() 53.1.2 q2 Use concepts from the reading to improve the following graph. Make sure your graph shows all the same variables, no more and no fewer. ## NOTE: No need to edit; run and inspect as_tibble(mtcars) %&gt;% mutate(model = rownames(mtcars)) %&gt;% ggplot(aes(x = &quot;&quot;, y = &quot;&quot;, size = mpg)) + geom_point() + facet_wrap(~model) Create your improved graph here ## TODO: Create an improved version of the graph above ## NOTE: This is just one possibility as_tibble(mtcars) %&gt;% mutate( model = rownames(mtcars), model = fct_reorder(model, mpg) ) %&gt;% ggplot(aes(x = model, y = mpg)) + geom_col() + coord_flip() 53.1.3 q3 Use concepts from the reading to improve the following graph. Make sure your graph shows all the same variables, no more and no fewer. ## NOTE: No need to edit; run and inspect diamonds %&gt;% ggplot(aes(clarity, fill = cut)) + geom_bar() Create your improved graph here ## TODO: Create an improved version of the graph above ## NOTE: This is just one possibility diamonds %&gt;% count(cut, clarity) %&gt;% ggplot(aes(clarity, n, color = cut, group = cut)) + geom_line() 53.1.4 q4 Use concepts from the reading to improve the following graph. Make sure your graph shows all the same variables, no more and no fewer. ## NOTE: No need to edit; run and inspect diamonds %&gt;% ggplot(aes(x = &quot;&quot;, fill = cut)) + geom_bar() + coord_polar(&quot;y&quot;) + labs(x = &quot;&quot;) Create your improved graph here ## TODO: Create an improved version of the graph above ## NOTE: This is just one possibility diamonds %&gt;% ggplot(aes(cut)) + geom_bar() "],["model-introduction-to-modeling.html", "54 Model: Introduction to Modeling 54.1 A simple model 54.2 Fitting a model 54.3 Assessing a model 54.4 Model Diagnostics 54.5 Improving a model 54.6 Quantifying uncertainty 54.7 Summary 54.8 Preview", " 54 Model: Introduction to Modeling Purpose: Modeling is a key tool for data science: We will use models to understand the relationships between variables and to make predictions. Building models is subtle and difficult. To that end, this will be a high-level tour through the key parts of building and assessing a model. In this exercise, you’ll learn what a model is, how to fit a model, how to assess a fitted model, some ways to improve a model, and how to quantify how trustworthy a model is. Reading: (None, this exercise is the reading.) library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✔ ggplot2 3.4.0 ✔ purrr 1.0.1 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.5.0 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() library(modelr) library(broom) ## ## Attaching package: &#39;broom&#39; ## The following object is masked from &#39;package:modelr&#39;: ## ## bootstrap ## NOTE: No need to edit this chunk set.seed(101) ## Select training data df_train &lt;- diamonds %&gt;% slice(1:1e4) ## Select test data df_test &lt;- diamonds %&gt;% slice((1e4 + 1):2e4) 54.1 A simple model In what follows, we’ll try to fit a linear, one-dimensional model for the price of a diamond. It will be linear in the sense that it will be a linear function of its inputs; i.e. for an input \\(x\\) we’ll limit ourselves to scaling the input \\(\\beta \\times x\\). It will also be one-dimensional in the sense that we will only consider one input; namely, the carat of the diamond. Thus, my model for predicted price \\(\\hat{f}\\) will be \\[\\hat{f} = \\beta_0 + \\beta_{\\text{carat}} (\\text{carat}) + \\epsilon,\\] where \\(\\epsilon\\) is an additive error term, which we’ll model as a random variable. Remember that \\(\\hat{f}\\) notation indicates an estimate for the quantity \\(f\\). To start modeling, I’ll choose parameters for my model by selecting values for the slope and intercept. ## Set model parameter values [theta] slope &lt;- 1000 / 0.5 # Eyeball: $1000 / (1/2) carat intercept &lt;- 0 ## Represent model as an `abline` df_train %&gt;% ggplot(aes(carat, price)) + geom_point() + geom_abline( slope = slope, intercept = intercept, linetype = 2, color = &quot;salmon&quot; ) That doesn’t look very good; the line tends to miss the higher-carat values. I manually adjust the slope up by a factor of two: ## Set model parameter values [theta] slope &lt;- 2000 / 0.5 # Adjusted by factor of 2 intercept &lt;- 0 ## Represent model as an `abline` df_train %&gt;% ggplot(aes(carat, price)) + geom_point() + geom_abline( slope = slope, intercept = intercept, linetype = 2, color = &quot;salmon&quot; ) This manual approach to fitting a model—choosing parameter values—is labor-intensive and silly. Fortunately, there’s a better way. We can optimize the parameter values by minimizing a chosen metric. First, let’s visualize the quantities we will seek to minimize: ## Set model parameter values [theta] slope &lt;- 2000 / 0.5 intercept &lt;- 0 ## Compute predicted values df_train %&gt;% mutate(price_pred = slope * carat + intercept) %&gt;% ## Visualize *residuals* as vertical bars ggplot(aes(carat, price)) + geom_point() + geom_segment( aes(xend = carat, yend = price_pred), color = &quot;salmon&quot; ) + geom_line( aes(y = price_pred), linetype = 2, color = &quot;salmon&quot; ) This plot shows the residuals of the model, that is \\[\\text{Residual}_i(\\theta) = \\hat{f}_i(\\theta) - f_i,\\] where \\(f_i\\) is the i-th observed output value (price), \\(\\hat{f}_i(\\theta)\\) is the i-th prediction from the model (price_pred), and \\(\\theta\\) is the set of parameter values for the model. For instance, the linear, one-dimensional model above has as parameters theta = c(slope, intercept). We can use these residuals to define an error metric and fit a model. 54.2 Fitting a model Define the mean squared error (MSE) via \\[\\text{MSE}(\\theta) = \\frac{1}{n} \\sum_{i=1}^n \\text{Residual}_i(\\theta)^2 = \\frac{1}{n} \\sum_{i=1}^n (\\hat{f}_i(\\theta) - f_i)^2.\\] This is a summary of the total error of the model. While we could carry out this optimization by hand, the R routine lm() (which stands for linear model) automates this procedure. We simply give it data over which to fit the model, and a formula defining which inputs and output to consider. ## Fit model fit_carat &lt;- df_train %&gt;% lm( data = ., # Data for fit formula = price ~ carat # Formula for fit ) fit_carat %&gt;% tidy() ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -84.6 21.0 -4.04 0.0000546 ## 2 carat 4129. 23.9 173. 0 The tidy() function takes a fit and returns the model’s parameters; here we can see the estimate values for the coefficients, as well as some statistical information (which we’ll discuss in a future exercise). The formula argument uses R’s formula notation, where Y ~ X means “fit a linear model with Y as the value to predict, and with X as an input.” The formula price ~ carat translates to the linear model \\[\\widehat{\\text{price}} = \\beta_0 + \\beta_{\\text{carat}} (\\text{carat}) + \\epsilon.\\] This slightly-mysterious formula notation price ~ carat is convenient for defining many kinds of models, as we’ll see in the following task. 54.2.1 q1 Fit a basic model. Copy the code above to fit a model of the form \\[\\widehat{\\text{price}} = \\beta_0 + \\beta_{\\text{carat}} (\\text{carat}) + \\beta_{\\text{cut}} (\\text{cut}) + \\epsilon.\\] Answer the questions below to investigate how this model form handles the variable cut. fit_q1 &lt;- df_train %&gt;% lm( data = ., formula = price ~ carat + cut ) fit_q1 %&gt;% tidy() ## # A tibble: 6 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -294. 22.1 -13.3 3.39e-40 ## 2 carat 4280. 24.0 178. 0 ## 3 cut.L 349. 17.7 19.7 4.10e-85 ## 4 cut.Q -135. 15.8 -8.56 1.26e-17 ## 5 cut.C 208. 14.3 14.5 2.94e-47 ## 6 cut^4 82.2 12.1 6.78 1.26e-11 Observations: carat is a continuous variable, while cut is an (ordinal) factor; it only takes fixed non-numerical values. We can’t reasonably multiply cut by a constant as it is not a number. The term for carat is just one numerical value (a slope), while there are multiple terms for cut. These are lm()s way of encoding the cut factor as a numerical value: Note that there are 5 levels for cut and 4 terms representing cut. Aside: Handling factors in modeling is handled automatically by lm() by introducing dummy variables. Conceptually, what the linear model does is fit a single contant value for each level of the factor. This gives us a different prediction for each factor level, as the next example shows. ## NOTE: No need to edit; just run and inspect fit_cut &lt;- lm( data = df_train, formula = price ~ cut ) df_train %&gt;% add_predictions(fit_cut, var = &quot;price_pred&quot;) %&gt;% ggplot(aes(cut)) + geom_errorbar(aes(ymin = price_pred, ymax = price_pred), color = &quot;salmon&quot;) + geom_point(aes(y = price)) 54.3 Assessing a model Next, let’s visually inspect the results of model fit_carat using the function modelr::add_predictions(): ## Compute predicted values df_train %&gt;% add_predictions( model = fit_carat, var = &quot;price_pred&quot; ) %&gt;% ggplot(aes(carat, price)) + geom_point() + geom_line( aes(y = price_pred), linetype = 2, color = &quot;salmon&quot; ) Frankly, these model predictions don’t look very good! We know that diamond prices probably depend on the “4 C’s”; maybe your model using more predictors will be more effective? 54.3.1 q2 Repeat the code above from chunk vis-carat to produce a similar visual with your model fit_q1. This visual is unlikely to be effective, note in your observations why that might be. df_train %&gt;% add_predictions( model = fit_q1, var = &quot;price_pred&quot; ) %&gt;% ggplot(aes(carat, price)) + geom_point() + geom_line( aes(y = price_pred), linetype = 2, color = &quot;salmon&quot; ) Observations: A diamond can have a different value of cut at a fixed value of carat; this means our model can take different values at a fixed value of carat. This leads to the “squiggles” we see above. Visualizing the results against a single variable quickly breaks down when we have more than one predictor! Let’s learn some other tools for assessing model accuracy. 54.4 Model Diagnostics The plot above allows us to visually assess the model performance, but sometimes we’ll want a quick numerical summary of model accuracy, particularly when comparing multiple models for the same data. The functions modelr::mse and modelr::rsquare are two error metrics we can use to summarize accuracy: ## Compute metrics mse(fit_carat, df_train) ## [1] 309104.4 rsquare(fit_carat, df_train) ## [1] 0.7491572 mse is the mean squared error. Lower values are more accurate. The mse has no formal upper bound, so we can only compare mse values between models. The mse also has the square-units of the quantity we’re predicting; for instance our model’s mse has units of \\(\\$^2\\). rsquare, also known as the coefficient of determination, lies between [0, 1]. Higher values are more accurate. The rsquare has bounded values, so we can think about it in absolute terms: a model with rsquare == 1 is essentially perfect, and values closer to 1 are better. 54.4.1 q3 Compute the mse and rsquare for your model fit_q1, and compare the values against those for fit_carat. Is your model more accurate? mse(fit_q1, df_train) ## [1] 290675.2 rsquare(fit_q1, df_train) ## [1] 0.7641128 Observations: The model fit_q1 is slightly more accurate than fit_carat, at least on df_train Aside: What’s an acceptable r-squared value? That really depends on the application. For some physics-related problems \\(R \\approx 0.9\\) might be considered unacceptably low, while for some human-behavior related problems \\(R \\approx 0.7\\) might be considered quite good! While it’s difficult to visualize model results against multiple variables, we can always compare predicted vs actual values. If the model fit were perfect, then the predicted \\(\\hat{f}\\) and actual \\(f\\) values would like along a straight line with slope one. ## NOTE: No need to change this ## predicted vs actual df_train %&gt;% add_predictions( model = fit_carat, var = &quot;price_pred&quot; ) %&gt;% ggplot(aes(price, price_pred)) + geom_abline(slope = 1, intercept = 0, color = &quot;grey50&quot;, size = 2) + geom_point() ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. This fit looks quite poor—there is a great deal of scatter of actual values away from the predicted values. What’s more, the scatter doesn’t look random; there seem to be some consistent patterns (e.g. “stripes”) in the plot that suggest there may be additional patterns we could incorporate in our model, if we added more variables. Let’s try that! 54.5 Improving a model The plot above suggests there may be some patterns we’re not accounting for in our model: Let’s build another model to use that intuition. 54.5.1 q4 Fit an updated model. Fit a model fit_4c of the form: \\[\\widehat{\\text{price}} = \\beta_0 + \\beta_{\\text{carat}} (\\text{carat}) + \\beta_{\\text{cut}} (\\text{cut}) + \\beta_{\\text{color}} (\\text{color}) + \\beta_{\\text{clarity}} (\\text{clarity}) + \\epsilon.\\] Compute the mse and rsquare of your new model, and compare against the previous models. fit_4c &lt;- df_train %&gt;% lm( data = ., formula = price ~ carat + cut + color + clarity ) rsquare(fit_q1, df_train) ## [1] 0.7641128 rsquare(fit_4c, df_train) ## [1] 0.897396 Observations: I find that adding all four C’s improves the model accuracy quite a bit. Generally, adding more variables tends to improve model accuracy. However, this is not always the case. In a future exercise we’ll learn more about selecting meaningful variables in the context of modeling. Note that even when we use all 4 C’s, we still do not have a perfect model. Generally, any model we fit will have some inaccuracy. If we plan to use our model to make decisions, it’s important to have some sense of how much we can trust our predictions. Metrics like model error are a coarse description of general model accuracy, but we can get much more useful information for individual predictions by quantifying uncertainty. 54.6 Quantifying uncertainty We’ve talked about confidence intervals before for estimates like the sample mean. Let’s take a (brief) look now at prediction intervals (PI). The code below approximates prediction intervals based on the fit_carat model. ## NOTE: No need to edit this chunk ## Helper function to compute uncertainty bounds add_uncertainties &lt;- function(data, model, prefix = &quot;pred&quot;, ...) { df_fit &lt;- stats::predict(model, data, ...) %&gt;% as_tibble() %&gt;% rename_with(~ str_c(prefix, &quot;_&quot;, .)) bind_cols(data, df_fit) } ## Generate predictions with uncertainties df_pred_uq &lt;- df_train %&gt;% add_uncertainties( model = fit_carat, prefix = &quot;pred&quot;, interval = &quot;prediction&quot;, level = 0.95 ) df_pred_uq %&gt;% glimpse() ## Rows: 10,000 ## Columns: 13 ## $ carat &lt;dbl&gt; 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, 0.23, 0… ## $ cut &lt;ord&gt; Ideal, Premium, Good, Premium, Good, Very Good, Very Good, Ve… ## $ color &lt;ord&gt; E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J, J, J, I… ## $ clarity &lt;ord&gt; SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, SI1, VS1,… ## $ depth &lt;dbl&gt; 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, 59.4, 6… ## $ table &lt;dbl&gt; 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54, 62, 5… ## $ price &lt;int&gt; 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339, 340, 3… ## $ x &lt;dbl&gt; 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, 4.00, 4… ## $ y &lt;dbl&gt; 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, 4.05, 4… ## $ z &lt;dbl&gt; 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, 2.39, 2… ## $ pred_fit &lt;dbl&gt; 865.1024, 782.5204, 865.1024, 1112.8483, 1195.4303, 906.3934,… ## $ pred_lwr &lt;dbl&gt; -225.25853, -307.86569, -225.25853, 22.55812, 105.16206, -183… ## $ pred_upr &lt;dbl&gt; 1955.463, 1872.906, 1955.463, 2203.139, 2285.699, 1996.742, 1… The helper function add_uncertainties() added the columns pred_fit (the predicted price), as well as two new columns: pred_lwr and pred_upr. These are the bounds of a prediction interval (PI), an interview meant to capture not a future sample statistic, but rather a future observation. The following visualization illustrates the computed prediction intervals: I visualize the prediction intervals with geom_errorbar. Note that we get a PI for each observation; every dot gets an interval. Since we have access to the true values price, we can assess whether the true observed values fall within the model prediction intervals; this happens when the diagonal falls within the interval on a predicted-vs-actual plot. ## NOTE: No need to edit this chunk # Visualize df_pred_uq %&gt;% filter(price &lt; 1000) %&gt;% ggplot(aes(price)) + geom_abline(slope = 1, intercept = 0, size = 2, color = &quot;grey50&quot;) + geom_errorbar( data = . %&gt;% filter(pred_lwr &lt;= price &amp; price &lt;= pred_upr), aes(ymin = pred_lwr, ymax = pred_upr), width = 0, size = 0.5, alpha = 1 / 2, color = &quot;darkturquoise&quot; ) + geom_errorbar( data = . %&gt;% filter(price &lt; pred_lwr | pred_upr &lt; price), aes(ymin = pred_lwr, ymax = pred_upr), width = 0, size = 0.5, color = &quot;salmon&quot; ) + geom_point(aes(y = pred_fit), size = 0.1) + theme_minimal() Ideally these prediction intervals should include a desired fraction of observed values; let’s compute the empirical coverage to see if this matches our desired level = 0.95. ## NOTE: No need to edit this chunk # Compute empirical coverage df_pred_uq %&gt;% summarize(coverage = mean(pred_lwr &lt;= price &amp; price &lt;= pred_upr)) ## # A tibble: 1 × 1 ## coverage ## &lt;dbl&gt; ## 1 0.959 The empirical coverage is quite close to our desired level. 54.6.1 q6 Use the helper function add_uncertainties() to add prediction intervals to df_train based on the model fit_4c. df_q6 &lt;- df_train %&gt;% add_uncertainties( model = fit_4c, prefix = &quot;pred&quot;, interval = &quot;prediction&quot;, level = 0.95 ) ## NOTE: No need to edit code below # Compute empirical coverage df_q6 %&gt;% summarize(coverage = mean(pred_lwr &lt;= price &amp; price &lt;= pred_upr)) ## # A tibble: 1 × 1 ## coverage ## &lt;dbl&gt; ## 1 0.956 # Visualize df_q6 %&gt;% ggplot(aes(price)) + geom_abline(slope = 1, intercept = 0, size = 2, color = &quot;grey50&quot;) + geom_errorbar( data = . %&gt;% filter(pred_lwr &lt;= price &amp; price &lt;= pred_upr), aes(ymin = pred_lwr, ymax = pred_upr), width = 0, size = 0.1, alpha = 1 / 5, color = &quot;darkturquoise&quot; ) + geom_errorbar( data = . %&gt;% filter(price &lt; pred_lwr | pred_upr &lt; price), aes(ymin = pred_lwr, ymax = pred_upr), width = 0, size = 0.1, color = &quot;salmon&quot; ) + geom_point(aes(y = pred_fit), size = 0.1) + theme_minimal() We will discuss prediction intervals further in a future exercise. For now, know that they give us a sense of how much we should trust our model predictions. 54.7 Summary To summarize this reading, here are the steps to fitting and using a model: Choose a model form, e.g. if only considering linear models, we may consider price ~ carat vs price ~ carat + cut. Fit the model with data; this is done by optimizing a user-chosen metric, such as the mse. Assess the model with metrics (mse, rsquare) and plots (predicted-vs-actual). Improve the model if needed, e.g. by adding more predictors. Quantify the trustworthiness of the model, e.g. with prediction intervals. Use the model to do useful work! We’ll cover this in future exercises. 54.8 Preview Notice that in this exercise, we only used df_test, but I also defined a tibble df_train. What happens when we fit the model on df_train, but assess it on df_test? ## NOTE: No need to edit this chunk rsquare(fit_4c, df_train) ## [1] 0.897396 rsquare(fit_4c, df_test) ## [1] 0.7860757 Note that rsquare on the training data df_train is much higher than rsquare on the test data df_test. This indicates that the assessment of model accuracy is overly optimistic when assessing the model with df_train. We will explore this idea more in e-stat12-models-train-test. "],["data-cleaning.html", "55 Data: Cleaning 55.1 Problem 1: No column names 55.2 Problem 2: Incorrect types 55.3 Problem 3: Uninformative values 55.4 Prepare the Data for Modeling 55.5 In summary", " 55 Data: Cleaning Purpose: Most of the data you’ll find in the wild is messy; you’ll need to clean those data before you can do useful work. In this case study, you’ll learn some more tricks for cleaning data. We’ll use these data for a future exercise on modeling, so we’ll build on the work you do here today. Reading: (None, this exercise is the reading.) library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✔ ggplot2 3.4.0 ✔ purrr 1.0.1 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.5.0 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() Background: This exercise’s data comes from the UCI Machine Learning Database; specifically their Heart Disease Data Set. These data consist of clinical measurements on patients, and are intended to help predict heart disease. ## NOTE: No need to edit; run and inspect url_disease &lt;- &quot;http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data&quot; filename_disease &lt;- &quot;./data/uci_heart_disease.csv&quot; ## Download the data locally curl::curl_download( url_disease, destfile = filename_disease ) This is a messy dataset; one we’ll have to clean if we want to make sense of it. Let’s load the data and document the ways in which it’s messy: ## NOTE: No need to edit; run and inspect read_csv(filename_disease) %&gt;% glimpse() ## New names: ## Rows: 302 Columns: 14 ## ── Column specification ## ──────────────────────────────────────────────────────── Delimiter: &quot;,&quot; chr ## (2): 0.0...12, 6.0 dbl (12): 63.0, 1.0...2, 1.0...3, 145.0, 233.0, 1.0...6, ## 2.0, 150.0, 0.0...9... ## ℹ Use `spec()` to retrieve the full column specification for this data. ℹ ## Specify the column types or set `show_col_types = FALSE` to quiet this message. ## • `1.0` -&gt; `1.0...2` ## • `1.0` -&gt; `1.0...3` ## • `1.0` -&gt; `1.0...6` ## • `0.0` -&gt; `0.0...9` ## • `0.0` -&gt; `0.0...12` ## Rows: 302 ## Columns: 14 ## $ `63.0` &lt;dbl&gt; 67, 67, 37, 41, 56, 62, 57, 63, 53, 57, 56, 56, 44, 52, 57,… ## $ `1.0...2` &lt;dbl&gt; 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,… ## $ `1.0...3` &lt;dbl&gt; 4, 4, 3, 2, 2, 4, 4, 4, 4, 4, 2, 3, 2, 3, 3, 2, 4, 3, 2, 1,… ## $ `145.0` &lt;dbl&gt; 160, 120, 130, 130, 120, 140, 120, 130, 140, 140, 140, 130,… ## $ `233.0` &lt;dbl&gt; 286, 229, 250, 204, 236, 268, 354, 254, 203, 192, 294, 256,… ## $ `1.0...6` &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,… ## $ `2.0` &lt;dbl&gt; 2, 2, 0, 2, 0, 2, 0, 2, 2, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 2,… ## $ `150.0` &lt;dbl&gt; 108, 129, 187, 172, 178, 160, 163, 147, 155, 148, 153, 142,… ## $ `0.0...9` &lt;dbl&gt; 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,… ## $ `2.3` &lt;dbl&gt; 1.5, 2.6, 3.5, 1.4, 0.8, 3.6, 0.6, 1.4, 3.1, 0.4, 1.3, 0.6,… ## $ `3.0` &lt;dbl&gt; 2, 2, 3, 1, 1, 3, 1, 2, 3, 2, 2, 2, 1, 1, 1, 3, 1, 1, 1, 2,… ## $ `0.0...12` &lt;chr&gt; &quot;3.0&quot;, &quot;2.0&quot;, &quot;0.0&quot;, &quot;0.0&quot;, &quot;0.0&quot;, &quot;2.0&quot;, &quot;0.0&quot;, &quot;1.0&quot;, &quot;0.… ## $ `6.0` &lt;chr&gt; &quot;3.0&quot;, &quot;7.0&quot;, &quot;3.0&quot;, &quot;3.0&quot;, &quot;3.0&quot;, &quot;3.0&quot;, &quot;3.0&quot;, &quot;7.0&quot;, &quot;7.… ## $ `0` &lt;dbl&gt; 2, 1, 0, 0, 0, 3, 0, 2, 1, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0,… Observations: The CSV comes without column names! read_csv() got confused and assigned the first row of data as names. Some of the numerical columns were incorrectly assigned character type. Some of the columns are coded as binary values 0, 1, but they really represent variables like sex %in% c(\"male\", \"female\"). Let’s tackle these problems one at a time: 55.1 Problem 1: No column names We’ll have a hard time making sense of these data without column names. Let’s fix that. 55.1.1 q1 Obtain the data. Following the dataset documentation, transcribe the correct column names and assign them as a character vector. You will use this to give the dataset sensible column names when you load it in q2. Hint 1: The relevant section from the dataset documentation is quoted here: Only 14 attributes used: 1. #3 (age) 2. #4 (sex) 3. #9 (cp) 4. #10 (trestbps) 5. #12 (chol) 6. #16 (fbs) 7. #19 (restecg) 8. #32 (thalach) 9. #38 (exang) 10. #40 (oldpeak) 11. #41 (slope) 12. #44 (ca) 13. #51 (thal) 14. #58 (num) (the predicted attribute) Hint 2: A “copy-paste-edit” is probably the most effective approach here! ## TODO: Assign the column names to col_names; make sure they are strings col_names &lt;- c( &quot;age&quot;, &quot;sex&quot;, &quot;cp&quot;, &quot;trestbps&quot;, &quot;chol&quot;, &quot;fbs&quot;, &quot;restecg&quot;, &quot;thalach&quot;, &quot;exang&quot;, &quot;oldpeak&quot;, &quot;slope&quot;, &quot;ca&quot;, &quot;thal&quot;, &quot;num&quot; ) Use the following to check your code. ## NOTE: No need to change this assertthat::assert_that(col_names[1] == &quot;age&quot;) ## [1] TRUE assertthat::assert_that(col_names[2] == &quot;sex&quot;) ## [1] TRUE assertthat::assert_that(col_names[3] == &quot;cp&quot;) ## [1] TRUE assertthat::assert_that(col_names[4] == &quot;trestbps&quot;) ## [1] TRUE assertthat::assert_that(col_names[5] == &quot;chol&quot;) ## [1] TRUE assertthat::assert_that(col_names[6] == &quot;fbs&quot;) ## [1] TRUE assertthat::assert_that(col_names[7] == &quot;restecg&quot;) ## [1] TRUE assertthat::assert_that(col_names[8] == &quot;thalach&quot;) ## [1] TRUE assertthat::assert_that(col_names[9] == &quot;exang&quot;) ## [1] TRUE assertthat::assert_that(col_names[10] == &quot;oldpeak&quot;) ## [1] TRUE assertthat::assert_that(col_names[11] == &quot;slope&quot;) ## [1] TRUE assertthat::assert_that(col_names[12] == &quot;ca&quot;) ## [1] TRUE assertthat::assert_that(col_names[13] == &quot;thal&quot;) ## [1] TRUE assertthat::assert_that(col_names[14] == &quot;num&quot;) ## [1] TRUE print(&quot;Well done!&quot;) ## [1] &quot;Well done!&quot; 55.2 Problem 2: Incorrect types We saw above that read_csv() incorrectly guessed some of the column types. Let’s fix that by manually specifying each column’s type. 55.2.1 q2 Call read_csv() with the col_names and col_types arguments. Use the column names you assigned above, and set all column types to col_number(). Hint: Remember that you can always read the documentation to learn how to use a new argument! ## TODO: Use the col_names and col_types arguments to give the data the ## correct column names, and to set their types to col_number() df_q2 &lt;- read_csv( filename_disease, col_names = col_names, col_types = cols( &quot;age&quot; = col_number(), &quot;sex&quot; = col_number(), &quot;cp&quot; = col_number(), &quot;trestbps&quot; = col_number(), &quot;chol&quot; = col_number(), &quot;fbs&quot; = col_number(), &quot;restecg&quot; = col_number(), &quot;thalach&quot; = col_number(), &quot;exang&quot; = col_number(), &quot;oldpeak&quot; = col_number(), &quot;slope&quot; = col_number(), &quot;ca&quot; = col_number(), &quot;thal&quot; = col_number(), &quot;num&quot; = col_number() ) ) ## Warning: One or more parsing issues, call `problems()` on your data frame for details, ## e.g.: ## dat &lt;- vroom(...) ## problems(dat) df_q2 %&gt;% glimpse() ## Rows: 303 ## Columns: 14 ## $ age &lt;dbl&gt; 63, 67, 67, 37, 41, 56, 62, 57, 63, 53, 57, 56, 56, 44, 52, 5… ## $ sex &lt;dbl&gt; 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1… ## $ cp &lt;dbl&gt; 1, 4, 4, 3, 2, 2, 4, 4, 4, 4, 4, 2, 3, 2, 3, 3, 2, 4, 3, 2, 1… ## $ trestbps &lt;dbl&gt; 145, 160, 120, 130, 130, 120, 140, 120, 130, 140, 140, 140, 1… ## $ chol &lt;dbl&gt; 233, 286, 229, 250, 204, 236, 268, 354, 254, 203, 192, 294, 2… ## $ fbs &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0… ## $ restecg &lt;dbl&gt; 2, 2, 2, 0, 2, 0, 2, 0, 2, 2, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 2… ## $ thalach &lt;dbl&gt; 150, 108, 129, 187, 172, 178, 160, 163, 147, 155, 148, 153, 1… ## $ exang &lt;dbl&gt; 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1… ## $ oldpeak &lt;dbl&gt; 2.3, 1.5, 2.6, 3.5, 1.4, 0.8, 3.6, 0.6, 1.4, 3.1, 0.4, 1.3, 0… ## $ slope &lt;dbl&gt; 3, 2, 2, 3, 1, 1, 3, 1, 2, 3, 2, 2, 2, 1, 1, 1, 3, 1, 1, 1, 2… ## $ ca &lt;dbl&gt; 0, 3, 2, 0, 0, 0, 2, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0… ## $ thal &lt;dbl&gt; 6, 3, 7, 3, 3, 3, 3, 3, 7, 7, 6, 3, 6, 7, 7, 3, 7, 3, 3, 3, 3… ## $ num &lt;dbl&gt; 0, 2, 1, 0, 0, 0, 3, 0, 2, 1, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0… Use the following to check your code. ## NOTE: No need to change this assertthat::assert_that(assertthat::are_equal(names(df_q2), col_names)) ## [1] TRUE assertthat::assert_that(all(map_chr(df_q2, class) == &quot;numeric&quot;)) ## [1] TRUE print(&quot;Nice!&quot;) ## [1] &quot;Nice!&quot; 55.3 Problem 3: Uninformative values The numeric codes given for some of the variables are uninformative; let’s replace those with more human-readable values. Rather than go and modify our raw data, we will instead recode the variables in our loaded dataset. It is bad practice to modify your raw data! Modifying your data in code provides traceable documentation for the edits you made; this is a key part of doing reproducible science. It takes more work, but your results will be more trustworthy if you do things the right way! 55.3.1 q3 Create conversion functions to recode factor values as human-readable strings. I have provided one function (convert_sex) as an example. Note: “In the wild” you would be responsible for devising your own sensible level names. However, I’m going to provide specific codes such that I can write unittests to check your answers: Variable Levels sex 1 = \"male\", 0 = \"female\" fbs 1 = TRUE, 0 = FALSE restecg 0 = \"normal\", 1 = \"ST-T wave abnormality\", 2 = \"Estes' criteria\" exang 1 = TRUE, 0 = FALSE slope 1 = \"upsloping\", 2 = \"flat\", 3 = \"downsloping\" thal 3 = \"normal\", 6 = \"fixed defect\", 7 = \"reversible defect\" ## NOTE: This is an example conversion convert_sex &lt;- function(x) { case_when( x == 1 ~ &quot;male&quot;, x == 0 ~ &quot;female&quot;, TRUE ~ NA_character_ ) } convert_cp &lt;- function(x) { case_when( x == 1 ~ &quot;typical angina&quot;, x == 2 ~ &quot;atypical angina&quot;, x == 3 ~ &quot;non-anginal pain&quot;, x == 4 ~ &quot;asymptomatic&quot;, TRUE ~ NA_character_ ) } convert_fbs &lt;- function(x) { if_else(x == 1, TRUE, FALSE) } convert_restecv &lt;- function(x) { case_when( x == 0 ~ &quot;normal&quot;, x == 1 ~ &quot;ST-T wave abnormality&quot;, x == 2 ~ &quot;Estes&#39; criteria&quot;, TRUE ~ NA_character_ ) } convert_exang &lt;- function(x) { if_else(x == 1, TRUE, FALSE) } convert_slope &lt;- function(x) { case_when( x == 1 ~ &quot;upsloping&quot;, x == 2 ~ &quot;flat&quot;, x == 3 ~ &quot;downsloping&quot;, TRUE ~ NA_character_ ) } convert_thal &lt;- function(x) { case_when( x == 3 ~ &quot;normal&quot;, x == 6 ~ &quot;fixed defect&quot;, x == 7 ~ &quot;reversible defect&quot;, TRUE ~ NA_character_ ) } Use the following to check your code. ## NOTE: No need to change this assertthat::assert_that(assertthat::are_equal( convert_cp(c(1, 2, 3, 4)), c(&quot;typical angina&quot;, &quot;atypical angina&quot;, &quot;non-anginal pain&quot;, &quot;asymptomatic&quot;) )) ## [1] TRUE assertthat::assert_that(assertthat::are_equal( convert_fbs(c(1, 0)), c(TRUE, FALSE) )) ## [1] TRUE assertthat::assert_that(assertthat::are_equal( convert_restecv(c(0, 1, 2)), c(&quot;normal&quot;, &quot;ST-T wave abnormality&quot;, &quot;Estes&#39; criteria&quot;) )) ## [1] TRUE assertthat::assert_that(assertthat::are_equal( convert_exang(c(1, 0)), c(TRUE, FALSE) )) ## [1] TRUE assertthat::assert_that(assertthat::are_equal( convert_slope(c(1, 2, 3)), c(&quot;upsloping&quot;, &quot;flat&quot;, &quot;downsloping&quot;) )) ## [1] TRUE assertthat::assert_that(assertthat::are_equal( convert_thal(c(3, 6, 7)), c(&quot;normal&quot;, &quot;fixed defect&quot;, &quot;reversible defect&quot;) )) ## [1] TRUE print(&quot;Excellent!&quot;) ## [1] &quot;Excellent!&quot; 55.3.2 q4 Use your convert_ functions from q3 to mutate the columns and recode the variables. df_q4 &lt;- df_q2 %&gt;% mutate( sex = convert_sex(sex), cp = convert_cp(cp), fbs = convert_fbs(fbs), restecg = convert_restecv(restecg), exang = convert_exang(exang), slope = convert_slope(slope), thal = convert_thal(thal) ) df_q4 ## # A tibble: 303 × 14 ## age sex cp trest…¹ chol fbs restecg thalach exang oldpeak slope ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 63 male typical… 145 233 TRUE Estes&#39;… 150 FALSE 2.3 down… ## 2 67 male asympto… 160 286 FALSE Estes&#39;… 108 TRUE 1.5 flat ## 3 67 male asympto… 120 229 FALSE Estes&#39;… 129 TRUE 2.6 flat ## 4 37 male non-ang… 130 250 FALSE normal 187 FALSE 3.5 down… ## 5 41 female atypica… 130 204 FALSE Estes&#39;… 172 FALSE 1.4 upsl… ## 6 56 male atypica… 120 236 FALSE normal 178 FALSE 0.8 upsl… ## 7 62 female asympto… 140 268 FALSE Estes&#39;… 160 FALSE 3.6 down… ## 8 57 female asympto… 120 354 FALSE normal 163 TRUE 0.6 upsl… ## 9 63 male asympto… 130 254 FALSE Estes&#39;… 147 FALSE 1.4 flat ## 10 53 male asympto… 140 203 TRUE Estes&#39;… 155 TRUE 3.1 down… ## # … with 293 more rows, 3 more variables: ca &lt;dbl&gt;, thal &lt;chr&gt;, num &lt;dbl&gt;, and ## # abbreviated variable name ¹​trestbps 55.4 Prepare the Data for Modeling Now we have a clean dataset we can use for EDA and modeling—great! Before we finish this exercise, let’s do some standard checks to understand these data: 55.4.1 q5 Perform your first checks on df_q4. Answer the questions below. Hint: You may need to do some “deeper checks” to answer some of the questions below. df_q4 %&gt;% summary() ## age sex cp trestbps ## Min. :29.00 Length:303 Length:303 Min. : 94.0 ## 1st Qu.:48.00 Class :character Class :character 1st Qu.:120.0 ## Median :56.00 Mode :character Mode :character Median :130.0 ## Mean :54.44 Mean :131.7 ## 3rd Qu.:61.00 3rd Qu.:140.0 ## Max. :77.00 Max. :200.0 ## ## chol fbs restecg thalach ## Min. :126.0 Mode :logical Length:303 Min. : 71.0 ## 1st Qu.:211.0 FALSE:258 Class :character 1st Qu.:133.5 ## Median :241.0 TRUE :45 Mode :character Median :153.0 ## Mean :246.7 Mean :149.6 ## 3rd Qu.:275.0 3rd Qu.:166.0 ## Max. :564.0 Max. :202.0 ## ## exang oldpeak slope ca ## Mode :logical Min. :0.00 Length:303 Min. :0.0000 ## FALSE:204 1st Qu.:0.00 Class :character 1st Qu.:0.0000 ## TRUE :99 Median :0.80 Mode :character Median :0.0000 ## Mean :1.04 Mean :0.6722 ## 3rd Qu.:1.60 3rd Qu.:1.0000 ## Max. :6.20 Max. :3.0000 ## NA&#39;s :4 ## thal num ## Length:303 Min. :0.0000 ## Class :character 1st Qu.:0.0000 ## Mode :character Median :0.0000 ## Mean :0.9373 ## 3rd Qu.:2.0000 ## Max. :4.0000 ## Observations: Variables: - Numerical: age, trestbps, chol, thalach, oldpeak, ca, num - Factors: sex, cp, restecg, slope, thal, heart_disease - Logical: fbs, exang, heart_disease Missingness: map( df_q4, ~ sum(is.na(.)) ) ## $age ## [1] 0 ## ## $sex ## [1] 0 ## ## $cp ## [1] 0 ## ## $trestbps ## [1] 0 ## ## $chol ## [1] 0 ## ## $fbs ## [1] 0 ## ## $restecg ## [1] 0 ## ## $thalach ## [1] 0 ## ## $exang ## [1] 0 ## ## $oldpeak ## [1] 0 ## ## $slope ## [1] 0 ## ## $ca ## [1] 4 ## ## $thal ## [1] 2 ## ## $num ## [1] 0 From this, we can see that most variables have no missing values, but ca has 4 and thal has 2. Missingness pattern: df_q4 %&gt;% filter(is.na(ca) | is.na(thal)) %&gt;% select(ca, thal, everything()) ## # A tibble: 6 × 14 ## ca thal age sex cp trest…¹ chol fbs restecg thalach exang ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 0 &lt;NA&gt; 53 fema… non-… 128 216 FALSE Estes&#39;… 115 FALSE ## 2 NA normal 52 male non-… 138 223 FALSE normal 169 FALSE ## 3 NA reversible … 43 male asym… 132 247 TRUE Estes&#39;… 143 TRUE ## 4 0 &lt;NA&gt; 52 male asym… 128 204 TRUE normal 156 TRUE ## 5 NA reversible … 58 male atyp… 125 220 FALSE normal 144 FALSE ## 6 NA normal 38 male non-… 138 175 FALSE normal 173 FALSE ## # … with 3 more variables: oldpeak &lt;dbl&gt;, slope &lt;chr&gt;, num &lt;dbl&gt;, and ## # abbreviated variable name ¹​trestbps There are six rows with missing values. If we were just doing EDA, we could stop here. However we’re going to use these data for modeling in a future exercise. Most models can’t deal with NA values, so we must choose how to handle rows with NA’s. In cases where only a few observations are missing values, we can simply filter out those rows. 55.4.2 q6 Filter out the rows with missing values. df_q6 &lt;- df_q4 %&gt;% filter(!is.na(ca), !is.na(thal)) df_q6 ## # A tibble: 297 × 14 ## age sex cp trest…¹ chol fbs restecg thalach exang oldpeak slope ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 63 male typical… 145 233 TRUE Estes&#39;… 150 FALSE 2.3 down… ## 2 67 male asympto… 160 286 FALSE Estes&#39;… 108 TRUE 1.5 flat ## 3 67 male asympto… 120 229 FALSE Estes&#39;… 129 TRUE 2.6 flat ## 4 37 male non-ang… 130 250 FALSE normal 187 FALSE 3.5 down… ## 5 41 female atypica… 130 204 FALSE Estes&#39;… 172 FALSE 1.4 upsl… ## 6 56 male atypica… 120 236 FALSE normal 178 FALSE 0.8 upsl… ## 7 62 female asympto… 140 268 FALSE Estes&#39;… 160 FALSE 3.6 down… ## 8 57 female asympto… 120 354 FALSE normal 163 TRUE 0.6 upsl… ## 9 63 male asympto… 130 254 FALSE Estes&#39;… 147 FALSE 1.4 flat ## 10 53 male asympto… 140 203 TRUE Estes&#39;… 155 TRUE 3.1 down… ## # … with 287 more rows, 3 more variables: ca &lt;dbl&gt;, thal &lt;chr&gt;, num &lt;dbl&gt;, and ## # abbreviated variable name ¹​trestbps Use the following to check your code. ## NOTE: No need to change this assertthat::assert_that( dim( df_q6 %&gt;% filter(rowSums(across(everything(), is.na)) &gt; 0) )[1] == 0 ) ## [1] TRUE print(&quot;Well done!&quot;) ## [1] &quot;Well done!&quot; 55.5 In summary We cleaned the dataset by giving it sensible names and recoding factors with human-readable values. We filtered out rows with missing values (NA’s) because we intend to use these data for modeling. "],["model-model-selection-and-the-test-validate-framework.html", "56 Model: Model Selection and the Test-Validate Framework 56.1 Illustrative Case: Polynomial Regression 56.2 Underfitting 56.3 Overfitting 56.4 A Solution: Validation Data 56.5 Intermediate Summary 56.6 More Challenging Case: Modeling Diamond Prices", " 56 Model: Model Selection and the Test-Validate Framework Purpose: When designing a model, we need to make choices about the model form. However, since we are optimizing the model to fit our data, we need to be careful not to bias our assessments and make poor modeling choices. We can use a training and validation split of our data to help make these choices. To understand these issues, we’ll discuss underfitting, overfitting, and the test-validate framework. Reading: Training, validation, and test sets (Optional) library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✔ ggplot2 3.4.0 ✔ purrr 1.0.1 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.5.0 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() library(modelr) library(broom) ## ## Attaching package: &#39;broom&#39; ## The following object is masked from &#39;package:modelr&#39;: ## ## bootstrap We’ll look at two cases: First a simple problem studying polynomials, then a more realistic case using the diamonds dataset. 56.1 Illustrative Case: Polynomial Regression To illustrate the ideas behind the test-validate framework, let’s study a very simple problem: Fitting a polynomial. The following code sets up this example. ## NOTE: No need to edit this chunk set.seed(101) # Ground-truth function we seek to approximate fcn_true &lt;- function(x) {12 * (x - 0.5)^3 - 2 * x + 1} # Generate data n_samples &lt;- 100 df_truth &lt;- tibble(x = seq(0, 1, length.out = n_samples)) %&gt;% mutate( y = fcn_true(x), # True values y_meas = y + 0.05 * rnorm(n_samples) # Measured with noise ) # Select training data df_measurements &lt;- df_truth %&gt;% slice_sample(n = 20) %&gt;% select(x, y_meas) # Visualize df_truth %&gt;% ggplot(aes(x, y)) + geom_line() + geom_point( data = df_measurements, mapping = aes(y = y_meas, color = &quot;Measurements&quot;) ) In what follows, we will behave as though we only have access to df_measurements—this is to model a “real” case where we have limited data. We will attempt to fit a polynomial to the data; remember that a polynomial of degree \\(d\\) is a function of the form \\[f_{\\text{polynomial}}(x) = \\sum_{i=0}^d \\beta_i x^i,\\] where the \\(\\beta_i\\) are coefficients, and \\(x^0 = 1\\) is a constant. 56.2 Underfitting The following code fits a polynomial of degree 2 to the available data df_measurements. 56.2.1 q1 Run the following code and inspect the (visual) results. Describe whether the model (Predicted) captures the “trends” in the measured data (black dots). ## NOTE: No need to edit this code; run and inspect # Fit a polynomial of degree = 2 fit_d2 &lt;- df_measurements %&gt;% lm( data = ., formula = y_meas ~ poly(x, degree = 2) ) # Visualize the results df_truth %&gt;% add_predictions(fit_d2, var = &quot;y_pred&quot;) %&gt;% ggplot(aes(x)) + geom_line(aes(y = y, color = &quot;True&quot;)) + geom_line(aes(y = y_pred, color = &quot;Predicted&quot;)) + geom_point(data = df_measurements, aes(y = y_meas)) + scale_color_discrete(name = &quot;&quot;) + theme_minimal() Observations: The Predicted values do not capture the trend in the measured points, nor in the True function. This phenomenon is called underfitting: This is when the model is not “flexible” enough to capture trends observed in the data. We can increase the flexibility of the model by increasing the polynomial order, which we’ll do below. 56.3 Overfitting Let’s increase the polynomial order and re-fit the data to try to solve the underfitting problem. 56.3.1 q2 Copy the code from above to fit a degree = 17 polynomial to the measurements. ## TASK: Fit a high-degree polynomial to df_measurements fit_over &lt;- df_measurements %&gt;% lm(data = ., formula = y_meas ~ poly(x, degree = 17)) ## NOTE: No need to modify code below y_limits &lt;- c( df_truth %&gt;% pull(y) %&gt;% min(), df_truth %&gt;% pull(y) %&gt;% max() ) df_truth %&gt;% add_predictions(fit_over, var = &quot;y_pred&quot;) %&gt;% ggplot(aes(x)) + geom_line(aes(y = y, color = &quot;True&quot;)) + geom_line(aes(y = y_pred, color = &quot;Predicted&quot;)) + geom_point(data = df_measurements, aes(y = y_meas)) + scale_color_discrete(name = &quot;&quot;) + coord_cartesian(ylim = y_limits) + theme_minimal() Observations: The predictions are perfect at the measured points. The predictions are terrible outside the measured points. The phenomenon we see with the high-degree case above is called overfitting. Overfitting tends to occur when our model is “too flexible”; this excess flexibility allows the model to fit to extraneous patterns, such as measurement noise or data artifacts due to sampling. So we have a “Goldilocks” problem: We need the model to be flexible enough to fit patterns in the data. (Avoid underfitting) We need the model to be not too flexible so as not to fit to noise. (Avoid overfitting) Quantities such as polynomial order that control model flexibility are called hyperparameters; essentially, these are parameters that are not set during the optimization we discussed in e-stat11-models-intro. We might choose to set hyperparameter values based on minimizing the model error. However, if we try to set the hyperparameters based on the training error, we’re going to make some bad choices. The next task gives us a hint why. 56.3.2 q3 Compute the mse for the 2nd and high-degree polynomial models on df_measurements. Which model has the lower error? Which hyperparameter value (polynomial degree) would you choose, based solely on these numbers? Hint: We learned how to do this in e-stat11-models-intro. # TASK: Compute the mse for fit_d2 and fit_over on df_measurements mse(fit_d2, df_measurements) ## [1] 0.03025928 mse(fit_over, df_measurements) ## [1] 0.001280951 Observations: fit_over has lower error on df_measurements. Based solely on these results, we would be inclined to choose high-degree polynomial model. However, this would be a poor decision, as we have a highly biased measure of model error. We would be better served by studying the error on a validation set. 56.4 A Solution: Validation Data A solution to the problem above is to reserve a set of validation data to tune the hyperparameters of our model. Note that this requires us to split our data into different sets: training data and validation data. The following code makes that split on df_measurements. ## NOTE: No need to edit this chunk set.seed(101) # Select &quot;training&quot; data from our available measurements df_train &lt;- df_measurements %&gt;% slice_sample(n = 10) # Use the remaining data as &quot;validation&quot; data df_validate &lt;- anti_join( df_measurements, df_train, by = &quot;x&quot; ) # Visualize the split df_truth %&gt;% ggplot(aes(x, y)) + geom_line() + geom_point( data = df_train %&gt;% mutate(source = &quot;Train&quot;), mapping = aes(y = y_meas, color = source) ) + geom_point( data = df_validate %&gt;% mutate(source = &quot;Validate&quot;), mapping = aes(y = y_meas, color = source) ) + scale_color_discrete(name = &quot;Data&quot;) Idea: Fit the model on the training data df_train. Assess the model on validation data df_validate. Use the assessment on validation data to choose the polynomial order. The following code sweeps through different values of polynomial order, fits a polynomial, and computes the associated error on both the Train and Validate sets. ## NOTE: No need to change this code df_sweep &lt;- map_dfr( seq(1, 9, by = 1), function(order) { # Fit a temporary model fit_tmp &lt;- lm( data = df_train, formula = y_meas ~ poly(x, order) ) # Compute error on the Train and Validate sets tibble( error_Train = mse(fit_tmp, df_train), error_Validate = mse(fit_tmp, df_validate), order = order ) } ) %&gt;% pivot_longer( names_to = c(&quot;.value&quot;, &quot;source&quot;), names_sep = &quot;_&quot;, cols = matches(&quot;error&quot;) ) In the next task, you will compare the resulting error metrics. 56.4.1 q4 Inspect the results of the degree sweep, and answer the questions below. ## NOTE: No need to edit; inspect and write your observations df_sweep %&gt;% ggplot(aes(order, error, color = source)) + geom_line() + scale_y_log10() + scale_x_continuous(breaks = seq(1, 10, by = 1)) + scale_color_discrete(name = &quot;Method&quot;) + coord_cartesian(ylim = c(1e-3, 1)) + theme(legend.position = &quot;bottom&quot;) + labs( x = &quot;Polynomial Order&quot;, y = &quot;Mean Squared Error&quot; ) Observations Training error is minimized at polynomial order 8, or possibly higher. Validation error is minimized at polynomial order 3. Selecting the polynomial order via the validation error leads to the correct choice. 56.5 Intermediate Summary We’ve seen a few ideas: A model that is not flexible enough will tend to underfit a dataset. A model that is too flexible will tend to overfit a dataset. The training error is an optimistic measure of accuracy, it is not an appropriate metric for setting hyperparameter values. To set hyperparameter values, we are better off “holding out” a validation set from our data, and using the validation error to make model decisions. 56.6 More Challenging Case: Modeling Diamond Prices Above we made our model more flexible by changing the polynomial order. For instance, a 2nd-order polynomial model would be \\[\\hat{y}_2 = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\epsilon,\\] while a 5th-order polynomial model would be \\[\\hat{y}_5 = \\beta_0 + \\beta_1 x + \\beta_2 x^2+ \\beta_3 x^3+ \\beta_4 x^4 + \\beta_5 x^5 + \\epsilon.\\] In effect, we are adding another predictor of the form \\(\\beta_i x^i\\) every time we increase the polynomial order. Increasing polynomial order is just one way we increase model flexibility; another way is to add additional variables to the model. For instance, in the diamonds dataset we have a number of variables that we could use as predictors: diamonds %&gt;% select(-price) %&gt;% names() ## [1] &quot;carat&quot; &quot;cut&quot; &quot;color&quot; &quot;clarity&quot; &quot;depth&quot; &quot;table&quot; &quot;x&quot; ## [8] &quot;y&quot; &quot;z&quot; Let’s put the train-validation idea to work! Below I set up training and validation sets of the diamonds data, and train a very silly model that blindly uses all the variables available as predictors. The challenge: Can you beat this model? ## NOTE: No need to edit this setup # Create a test-validate split set.seed(101) diamonds_randomized &lt;- diamonds %&gt;% slice(sample(dim(diamonds)[1])) diamonds_train &lt;- diamonds_randomized %&gt;% slice(1:10000) diamonds_validate &lt;- diamonds_randomized %&gt;% slice(10001:20000) # Try to beat this naive model that uses all variables! fit_full &lt;- lm( data = diamonds_train, formula = price ~ . # The `.` notation here means &quot;use all variables&quot; ) 56.6.1 q5 Build your own model! Choose which predictors to include by modifying the formula argument below. Use the validation data to help guide your choice. Answer the questions below. Hint: We’ve done EDA on diamonds before. Use your knowledge from that past EDA to choose variables you think will be informative for predicting the price. ## NOTE: This is just one possible model! fit_q5 &lt;- lm( data = diamonds_train, formula = price ~ carat + cut + color + clarity ) # Compare the two models on the validation set mse(fit_q5, diamonds_validate) ## [1] 1306804 mse(fit_full, diamonds_validate) ## [1] 1568726 Observations: carat by itself does a decent job predicting price. Based on EDA we’ve done before, it appears that carat is the major decider in diamond price. cut, color, and clarity help, but do not have the same predictive power (by themselves) as carat. Based on EDA we’ve done before, we know that cut, color, and clarity are important for price, but not quite as important as carat. x, y, and z alone have predictive power similar to carat alone. They probably correlate well with the weight, as they measure the dimensions of the diamond. x, y, and z together have very poor predictive power depth and table do not have the same predictive power as carat. The best combination of predictors I found was carat + cut + color + clarity. Aside: The process of choosing predictors—sometimes called features—is called feature selection. One last thing: Note above that I first randomized the diamonds before selecting training and validation data. This is really important! Let’s see what happens if we don’t randomize the data before splitting: 56.6.2 q6 Visualize a histogram for the prices of diamonds_train_bad and diamonds_validate_bad. Answer the questions below. ## NOTE: No need to edit this part diamonds_train_bad &lt;- diamonds %&gt;% slice(1:10000) diamonds_validate_bad &lt;- diamonds %&gt;% slice(10001:20000) ## TODO: Visualize a histogram of prices for both `bad` sets. bind_rows( diamonds_train_bad %&gt;% mutate(source = &quot;Train&quot;), diamonds_validate_bad %&gt;% mutate(source = &quot;Validate&quot;) ) %&gt;% ggplot(aes(price)) + geom_histogram(bins = 100) + facet_grid(source ~ .) Observations: diamonds_test_bad and diamonds_validate_bad have very little overlap! It seems the diamonds datset has some ordering along price, which greatly affects our split. If we were to train and then validate, we would be training on lower-price diamonds and predicting on higher-price diamonds. This might actually be appropriate if we’re trying to extrapolate from low to high. But if we are trying to get a representative estimate of error for training, this would be an inappropriate split. "],["data-liberating-data-with-webplotdigitizer.html", "57 Data: Liberating data with WebPlotDigitizer 57.1 Setup 57.2 Extract 57.3 Use the extracted data", " 57 Data: Liberating data with WebPlotDigitizer Purpose: Sometimes data are messy—we know how to deal with that. Other times data are “locked up” in a format we can’t easily analyze, such as in an image. In this exercise you’ll learn how to liberate data from a plot using WebPlotDigitizer. Reading: (None, this exercise is the reading.) Optional Reading: WebPlotDigitizer tutorial video ~ 19 minutes. (I recommend you give this a watch if you want some inspiration on other use cases: There are a lot of very clever ways to use this tool!) library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✔ ggplot2 3.4.0 ✔ purrr 1.0.1 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.5.0 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() Background: WebPlotDigitizer is one of those tools that is insanely useful, but no one ever teaches. I didn’t learn about this until six years into graduate school. You’re going to learn some very practical skills in this exercise! Note: I originally extracted these data from an Economist article on American meat prices and production in 2020. 57.1 Setup 57.1.1 q1 Get WebPlotDigitizer. Go to the WebPlotDigitizer website and download the desktop version (matching your operating system). Note: On Mac OS X you may have to open Security &amp; Privacy in order to launch WebPlotDigitizer on your machine. 57.2 Extract 57.2.1 q2 Extract the data from the following image: Beef production This image shows the percent change in US beef production as reported in this Economist article. We’ll go through extraction step-by-step: Click the Load Image(s) button, and select ./images/beef_production.png. 2. Choose the 2D (X-Y) Plot type. 3. Make sure to read these instructions! 4. Place the four control points; it doesn’t matter what precise values you pick, just that you know the X values for the first two, and the Y values for the second two. Note: Once you’ve placed a single point, you can use the arrow keys on your keyboard to make micro adjustments to the point; this means you don’t have to be super-accurate with your mouse. Use this to your advantage! 5. Calibrate the axes by entering the X and Y values you placed. Note that you can give decimals, dates, times, or exponents. 6. Now that you have a set of axes, you can extract the data. This plot is fairly high-contrast, so we can use the Automatic Extraction tools. Click on the Box setting, and select the foreground color to match the color of the data curve (in this case, black). Load image Once you’ve selected the box tool, draw a rectangle over an area containing the data. Note that if you cover the labels, the algorithm will try to extract those too! 8. Click the Run button; you should see red dots covering the data curve. Load image Now you can save the data to a file; make sure the dataset is selected (highlighted in orange) and click the View Data button. 10. Click the Download .CSV button and give the file a sensible name. Congrats! You just liberated data from a plot! 57.2.2 q3 Extract the data from the following plot. This will give you price data to compare against the production data. Beef price 57.3 Use the extracted data 57.3.1 q4 Load the price and production datasets you extracted. Join and plot price vs production; what kind of relationship do you see? ## NOTE: Your filenames may vary! df_price &lt;- read_csv( &quot;./data/beef_price.csv&quot;, col_names = c(&quot;date&quot;, &quot;price_percent&quot;) ) ## Rows: 232 Columns: 2 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (1): price_percent ## date (1): date ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. df_production &lt;- read_csv( &quot;./data/beef_production.csv&quot;, col_names = c(&quot;date&quot;, &quot;production_percent&quot;) ) ## Rows: 227 Columns: 2 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (1): production_percent ## date (1): date ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ## NOTE: I&#39;m relying on WebPlotDigitizer to produce dates in order to ## make this join work. This will probably fail if you have numbers ## rather than dates. df_both &lt;- inner_join( df_price, df_production, by = &quot;date&quot; ) df_both %&gt;% ggplot(aes(production_percent, price_percent, color = date)) + geom_point() Observations: In the middle of the pandemic beef production dropped quickly without a large change in price. After production dropped by 20% beef price began to spike. As the pandemic continued in the US, beef production increased slightly, but price continued to rise. "],["model-warnings-when-interpreting-linear-models.html", "58 Model: Warnings when interpreting linear models 58.1 1st Warning: Models Are a Function of the Population 58.2 2nd Warning: Model Coefficients are a Function of All Chosen Predictors 58.3 Main Punchline", " 58 Model: Warnings when interpreting linear models Purpose: When fitting a model, we might like to use that model to interpret how predictors affect some outcome of interest. This is a useful thing to do, but interpreting models is also very challenging. This exercise will give you a couple warnings about interpreting models. Reading: (None, this is the reading) library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✔ ggplot2 3.4.0 ✔ purrr 1.0.1 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.5.0 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() library(modelr) library(broom) ## ## Attaching package: &#39;broom&#39; ## The following object is masked from &#39;package:modelr&#39;: ## ## bootstrap For this exercise, we’ll use the familiar diamonds dataset. ## NOTE: No need to edit this setup # Create a test-validate split set.seed(101) diamonds_randomized &lt;- diamonds %&gt;% slice(sample(dim(diamonds)[1])) diamonds_train &lt;- diamonds_randomized %&gt;% slice(1:10000) 58.1 1st Warning: Models Are a Function of the Population Remember that any time we’re doing statistics, we must first define the population. That means when we’re fitting models, we need to pay attention to the data we feed the model for training. Let’s start with a curious observation; look at the effect of cut on price at low and high carat values: ## NOTE: No need to edit this chunk diamonds_train %&gt;% mutate( grouping = if_else(carat &lt; 1.0, &quot;Lower carat&quot;, &quot;Upper carat&quot;) ) %&gt;% ggplot(aes(cut, price)) + geom_boxplot() + scale_y_log10() + facet_grid(~ grouping) The trend in cut is what we’d expect at upper values (carat &gt; 1), but reversed at lower values (carat &lt;= 1)! Let’s see how this affects model predictions. 58.1.1 q1 Compare two models. Fit two models on diamonds_train, one for carat &lt;= 1 and one for carat &gt; 1. Use only cut as the predictor. First, make a prediction about how the predictions for the two models will compare, and then inspect the model results below. fit_lower &lt;- diamonds_train %&gt;% filter(carat &lt;= 1) %&gt;% lm(formula = price ~ cut) fit_upper &lt;- diamonds_train %&gt;% filter(carat &gt; 1) %&gt;% lm(formula = price ~ cut) ## NOTE: No need to modify this code tibble(cut = c(&quot;Fair&quot;, &quot;Good&quot;, &quot;Very Good&quot;, &quot;Premium&quot;, &quot;Ideal&quot;)) %&gt;% mutate( cut = fct_relevel(cut, &quot;Fair&quot;, &quot;Good&quot;, &quot;Very Good&quot;, &quot;Premium&quot;, &quot;Ideal&quot;) ) %&gt;% add_predictions(fit_lower, var = &quot;price_pred-lower&quot;) %&gt;% add_predictions(fit_upper, var = &quot;price_pred-upper&quot;) %&gt;% pivot_longer( names_to = c(&quot;.value&quot;, &quot;model&quot;), names_sep = &quot;-&quot;, cols = matches(&quot;price&quot;) ) %&gt;% ggplot(aes(cut, price_pred, color = model)) + geom_line(aes(group = model)) + geom_point() + scale_y_log10() Observations: I expected the lower model to have decreasing price with increasing cut, and the upper model to have the reverse trend. Yup! The model behavior matched my expectations. Why is this happening? Let’s investigate! 58.1.2 q2 Change the model Repeat the same exercise above, but instead of price ~ cut fit carat ~ cut. Interpret the model results: Can the behavior we see below help explain the behavior above? fit_carat_lower &lt;- diamonds_train %&gt;% filter(carat &lt;= 1) %&gt;% lm(formula = carat ~ cut) fit_carat_upper &lt;- diamonds_train %&gt;% filter(carat &gt; 1) %&gt;% lm(formula = carat ~ cut) ## NOTE: No need to change this code tibble(cut = c(&quot;Fair&quot;, &quot;Good&quot;, &quot;Very Good&quot;, &quot;Premium&quot;, &quot;Ideal&quot;)) %&gt;% mutate( cut = fct_relevel(cut, &quot;Fair&quot;, &quot;Good&quot;, &quot;Very Good&quot;, &quot;Premium&quot;, &quot;Ideal&quot;) ) %&gt;% add_predictions(fit_carat_lower, var = &quot;carat_pred-lower&quot;) %&gt;% add_predictions(fit_carat_upper, var = &quot;carat_pred-upper&quot;) %&gt;% pivot_longer( names_to = c(&quot;.value&quot;, &quot;model&quot;), names_sep = &quot;-&quot;, cols = matches(&quot;carat&quot;) ) %&gt;% ggplot(aes(cut, carat_pred, color = model)) + geom_line(aes(group = model)) + geom_point() + scale_y_log10() Observations: We see that the lower model has carat decreasing with increasing cut, while the upper model gives carat a relatively flat relationship with cut. This trend could account for the behavior we saw above: For the lower model, the variable cut could be used as a proxy for carat, which would lead to a negative model trend. We can try to fix these issues by adding more predictors. But that leads to our second warning…. 58.2 2nd Warning: Model Coefficients are a Function of All Chosen Predictors Our models are not just a function of the population, but also of the specific set of predictors we choose for the model. That may seem like an obvious statement, but the effects are profound: Adding a new predictor x2 can change the model’s behavior according to another predictor, say x1. This could change an effect enough to reverse the sign of a predictor! The following task will demonstrate this effect. 58.2.1 q3 Fit two models, one with both carat and cut, and another with cut only. Fit only to the low-carat diamonds (carat &lt;= 1). Use the provided code to compare the model behavior with cut, and answer the questions under observations below. fit_carat_cut &lt;- diamonds_train %&gt;% filter(carat &lt;= 1) %&gt;% lm(formula = price ~ carat + cut) fit_cut_only &lt;- diamonds_train %&gt;% filter(carat &lt;= 1) %&gt;% lm(formula = price ~ cut) ## NOTE: No need to change this code tibble( cut = c(&quot;Fair&quot;, &quot;Good&quot;, &quot;Very Good&quot;, &quot;Premium&quot;, &quot;Ideal&quot;), carat = c(0.4) ) %&gt;% mutate( cut = fct_relevel(cut, &quot;Fair&quot;, &quot;Good&quot;, &quot;Very Good&quot;, &quot;Premium&quot;, &quot;Ideal&quot;) ) %&gt;% add_predictions(fit_carat_cut, var = &quot;price_pred-carat_cut&quot;) %&gt;% add_predictions(fit_cut_only, var = &quot;price_pred-cut_only&quot;) %&gt;% pivot_longer( names_to = c(&quot;.value&quot;, &quot;model&quot;), names_sep = &quot;-&quot;, cols = matches(&quot;price&quot;) ) %&gt;% ggplot(aes(cut, price_pred, color = model)) + geom_line(aes(group = model)) + geom_point() + scale_y_log10() Observations: cut has a negative effect on price for the cut_only model. cut has a positive effect on price for the carat_cut model. We saw above that cut predicts carat at low carat values; when we don’t include carat in the model, the cut variable acts as a surrogate for carat. When we do include carat in the model, the model uses carat to predict the price, and can more correctly account for the behavior of `cut. 58.3 Main Punchline When fitting a model, we might be tempted to interpret the model parameters. Sometimes this can be helpful, but as we’ve seen above the model behavior is a complex function of the population, the available data, and the specific predictors we choose for the model. When making predictions this is not so much of an issue. But when trying to interpret a model, we need to exercise caution. A more formal treatment of these ideas is to think about confounding variables. The more general statistical exercise of assigning causal behavior to different variables is called causal inference. These topics are slippery, and largely outside the scope of this course. If you’d like to learn more, I highly recommend taking more formal courses in statistics! "],["data-liberating-data-with-tabula.html", "59 Data: Liberating data with Tabula 59.1 Setup 59.2 Liberating Data", " 59 Data: Liberating data with Tabula Purpose: Sometimes data are messy—we know how to deal with that. Other times data are “locked up” in a format we can’t easily analyze, such as in a PDF. In this exercise you’ll learn how to liberate data from a PDF table using Tabula. Reading: (None, this exercise is the reading.) library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✔ ggplot2 3.4.0 ✔ purrr 1.0.1 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.5.0 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() Background: Tabula is a piece of software developed for journalists carrying out investigative reporting. It was developed with support from organizations like ProPublica and The New York Times. This tool is meant to help investigators parse unwieldy PDFs and liberate useful information. 59.1 Setup 59.1.1 q1 Install Tabula. Download and install Tabula; the webpage has installation instructions. Note: Tabula’s interface is through a locally-hosted server; it should automatically open a browser window for Tabula. If it does not, then open http://localhost:8080/ after you’ve launched Tabula. 59.2 Liberating Data 59.2.1 q2.1 Obtain the data. Download FY2019 independent financial audit report (PDF) from the Needham, MA financial reports page. 59.2.2 q2.2 Try it the hard way. Try copy-pasting from the FY2019 report the table Government-Wide Financial Analysis into a text document or your favorite spreadsheet editor. This is unlikely to produce the desired result. (Please don’t spend any time trying to format this copied data—you’re about to learn a better way!) Tabula is a tool that will help us liberate the data; basically, it’s a copy-paste for PDF tables that actually works. 59.2.3 q3 Extract from the FY2019 report the Government-Wide Financial Analysis table. We’ll do this in steps: Click the browse button to select your downloaded FY2019 report and click Import. Tabula browse Wait for the file to finish processing; this takes about 2 minutes on my laptop. Tabula browse Once Tabula has imported the file, your view will switch to a view of the PDF. Tabula browse Scroll to the Government-Wide Financial Analysis table; click and drag to select the data. Click Preview &amp; Export Extracted Data. Tabula browse You will arrive at a preview of the extracted data. You may find that Tabula has merged some of the columns; if this happens click the Revise selection(s) button to go back and adjust your selection. Tabula browse Once you have a preview that matches the columns above, select the CSV filetype and click the Export button. Download the file to your data folder and give it a sensible filename. 59.2.4 q4 Load and clean the data. Load and clean the table you extracted above. Use the column names category and [government|business|total]_[2019|2018]. Do not tidy (pivot) the data yet, but make sure the appropriate columns are numeric. Note: In accounting practice, numbers in parentheses are understood to be negative, e.g. (1000) = -1000. df_2019_raw &lt;- read_csv( &quot;./data/needham_fs19.csv&quot;, skip = 1, col_names = c( &quot;category&quot;, &quot;X2&quot;, &quot;governmental_2019&quot;, &quot;X4&quot;, &quot;governmental_2018&quot;, &quot;X6&quot;, &quot;business_2019&quot;, &quot;X8&quot;, &quot;business_2018&quot;, &quot;X10&quot;, &quot;total_2019&quot;, &quot;X12&quot;, &quot;total_2018&quot; ) ) df_2019 &lt;- df_2019_raw %&gt;% select(-contains(&quot;X&quot;)) %&gt;% ## across() allows us to apply the same mutation to multiple ## columns; remove all internal spaces from numbers mutate(across(-category, ~str_remove_all(., &quot;\\\\s&quot;))) %&gt;% ## Handle numbers enclosed by parentheses; make them negative ## and remove all parentheses for the number parser mutate(across( -category, ~if_else( str_detect(., &quot;\\\\(&quot;), str_c(&quot;-&quot;, str_remove_all(., &quot;[\\\\(\\\\)]&quot;)), str_remove_all(., &quot;[\\\\(\\\\)]&quot;) ) )) %&gt;% ## Use the number parser to handle conversions mutate(across(-category, parse_number)) %&gt;% ## Fix a couple chopped lines mutate( category = if_else( category == &quot;resources&quot;, &quot;Total assets and deferred outflow of resources&quot;, category ) ) %&gt;% mutate( category = str_replace( category, &quot;resources, and net position&quot;, &quot;Total liabilities, deferred inflow of resources, and net position&quot; ) ) %&gt;% filter(!is.na(governmental_2019)) df_2019 %&gt;% glimpse() Use the following to check your work: ## NOTE: No need to edit; check a couple problematic values assertthat::assert_that( ( df_2019 %&gt;% filter(category == &quot;Deferred outflow of resources&quot;) %&gt;% pull(business_2019) ) == 1160 ) assertthat::assert_that( ( df_2019 %&gt;% filter(category == &quot;Unrestricted&quot;) %&gt;% pull(governmental_2019) ) == -62396 ) print(&quot;Excellent!&quot;) Where Tabula really shines is in cases where you need to process many documents; if you find yourself needing to process a whole folder of PDF’s, consider using Tabula. "],["model-logistic-regression.html", "60 Model: Logistic Regression 60.1 Setup 60.2 Logistic Regression 60.3 A Worked Example 60.4 Doing Logistic Regression", " 60 Model: Logistic Regression Purpose: So far we’ve talked about models to predict continuous values. However, we can also use models to make predictions about binary outcomes—classification. Classifiers are useful for a variety of uses, but they introduce a fair bit more complexity than simple linear models. In this exercise you’ll learn about logistic regression: a variation on linear regression that is useful for classification. Reading: StatQuest: Logistic Regression (Required, just watch up to 10:47 and dont’ worry about the p-value stuff). library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✔ ggplot2 3.4.0 ✔ purrr 1.0.1 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.5.0 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() library(modelr) library(broom) ## ## Attaching package: &#39;broom&#39; ## The following object is masked from &#39;package:modelr&#39;: ## ## bootstrap Note: This exercise is heavily inspired by Josh Starmer’s logistic regression example. Background: This exercise’s data comes from the UCI Machine Learning Database; specifically their Heart Disease Data Set. These data consist of clinical measurements on patients, and are intended to help study heart disease. 60.1 Setup Note: The following chunk contains a lot of stuff, but you already did this in e-data13-cleaning! ## NOTE: No need to edit; you did all this in a previous exercise! url_disease &lt;- &quot;http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data&quot; filename_disease &lt;- &quot;./data/uci_heart_disease.csv&quot; ## Download the data locally curl::curl_download( url_disease, destfile = filename_disease ) ## Wrangle the data col_names &lt;- c( &quot;age&quot;, &quot;sex&quot;, &quot;cp&quot;, &quot;trestbps&quot;, &quot;chol&quot;, &quot;fbs&quot;, &quot;restecg&quot;, &quot;thalach&quot;, &quot;exang&quot;, &quot;oldpeak&quot;, &quot;slope&quot;, &quot;ca&quot;, &quot;thal&quot;, &quot;num&quot; ) ## Recoding functions convert_sex &lt;- function(x) { case_when( x == 1 ~ &quot;male&quot;, x == 0 ~ &quot;female&quot;, TRUE ~ NA_character_ ) } convert_cp &lt;- function(x) { case_when( x == 1 ~ &quot;typical angina&quot;, x == 2 ~ &quot;atypical angina&quot;, x == 3 ~ &quot;non-anginal pain&quot;, x == 4 ~ &quot;asymptomatic&quot;, TRUE ~ NA_character_ ) } convert_fbs &lt;- function(x) { if_else(x == 1, TRUE, FALSE) } convert_restecv &lt;- function(x) { case_when( x == 0 ~ &quot;normal&quot;, x == 1 ~ &quot;ST-T wave abnormality&quot;, x == 2 ~ &quot;Estes&#39; criteria&quot;, TRUE ~ NA_character_ ) } convert_exang &lt;- function(x) { if_else(x == 1, TRUE, FALSE) } convert_slope &lt;- function(x) { case_when( x == 1 ~ &quot;upsloping&quot;, x == 2 ~ &quot;flat&quot;, x == 3 ~ &quot;downsloping&quot;, TRUE ~ NA_character_ ) } convert_thal &lt;- function(x) { case_when( x == 3 ~ &quot;normal&quot;, x == 6 ~ &quot;fixed defect&quot;, x == 7 ~ &quot;reversible defect&quot;, TRUE ~ NA_character_ ) } ## Load and wrangle df_heart_disease &lt;- read_csv( filename_disease, col_names = col_names, col_types = cols( &quot;age&quot; = col_number(), &quot;sex&quot; = col_number(), &quot;cp&quot; = col_number(), &quot;trestbps&quot; = col_number(), &quot;chol&quot; = col_number(), &quot;fbs&quot; = col_number(), &quot;restecg&quot; = col_number(), &quot;thalach&quot; = col_number(), &quot;exang&quot; = col_number(), &quot;oldpeak&quot; = col_number(), &quot;slope&quot; = col_number(), &quot;ca&quot; = col_number(), &quot;thal&quot; = col_number(), &quot;num&quot; = col_number() ) ) %&gt;% mutate( sex = convert_sex(sex), cp = convert_cp(cp), fbs = convert_fbs(fbs), restecg = convert_restecv(restecg), exang = convert_exang(exang), slope = convert_slope(slope), thal = convert_thal(thal) ) ## Warning: One or more parsing issues, call `problems()` on your data frame for details, ## e.g.: ## dat &lt;- vroom(...) ## problems(dat) df_heart_disease ## # A tibble: 303 × 14 ## age sex cp trest…¹ chol fbs restecg thalach exang oldpeak slope ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 63 male typical… 145 233 TRUE Estes&#39;… 150 FALSE 2.3 down… ## 2 67 male asympto… 160 286 FALSE Estes&#39;… 108 TRUE 1.5 flat ## 3 67 male asympto… 120 229 FALSE Estes&#39;… 129 TRUE 2.6 flat ## 4 37 male non-ang… 130 250 FALSE normal 187 FALSE 3.5 down… ## 5 41 female atypica… 130 204 FALSE Estes&#39;… 172 FALSE 1.4 upsl… ## 6 56 male atypica… 120 236 FALSE normal 178 FALSE 0.8 upsl… ## 7 62 female asympto… 140 268 FALSE Estes&#39;… 160 FALSE 3.6 down… ## 8 57 female asympto… 120 354 FALSE normal 163 TRUE 0.6 upsl… ## 9 63 male asympto… 130 254 FALSE Estes&#39;… 147 FALSE 1.4 flat ## 10 53 male asympto… 140 203 TRUE Estes&#39;… 155 TRUE 3.1 down… ## # … with 293 more rows, 3 more variables: ca &lt;dbl&gt;, thal &lt;chr&gt;, num &lt;dbl&gt;, and ## # abbreviated variable name ¹​trestbps The data above are clean, but we still need to prepare them for modeling. Remember from e-data13-cleaning that we had to filter out rows with NA values. Additionally, we’re going to convert num (a numerical factor) into a binary outcome indicating the presence of heart disease: ## NOTE: No need to edit; preparing the data for modeling df_data &lt;- df_heart_disease %&gt;% rowid_to_column() %&gt;% ## Filter rows with NA&#39;s (you did this in e-data13-cleaning) filter(!is.na(ca), !is.na(thal)) %&gt;% ## Create binary outcome for heart disease mutate(heart_disease = num &gt; 0) The last step of data setup is up to you! 60.1.1 q1 Perform a train-validate split of df_data. Make sure to shuffle the data when splitting, and ensure that df_train and df_validate together contain the entire dataset. n_train &lt;- 200 df_train &lt;- df_data %&gt;% slice_sample(n = n_train) df_validate &lt;- anti_join( df_data, df_train, by = &quot;rowid&quot; ) Use the following to check your code. ## NOTE: No need to change this # Correct size assertthat::assert_that( dim(bind_rows(df_train, df_validate))[1] == dim(df_data)[1] ) ## [1] TRUE # All rowid&#39;s appear exactly once assertthat::assert_that(all( bind_rows(df_train, df_validate) %&gt;% count(rowid) %&gt;% pull(n) == 1 )) ## [1] TRUE # Data shuffled assertthat::assert_that( !all( bind_rows(df_train, df_validate) %&gt;% pull(rowid) == df_data %&gt;% pull(rowid) ) ) ## [1] TRUE print(&quot;Well done!&quot;) ## [1] &quot;Well done!&quot; 60.2 Logistic Regression As the required video introduced, logistic regression bears some resemblance to linear regression. However, rather than predicting continuous outcomes (such as the price of a diamond), we will instead predict a binary outcome (in the present exercise: whether or not a given patient has heart disease). In order to “fit a line” to this kind of binary data, we make a careful choice about what to model: Rather than model the binary outcome directly, we instead model the probability (a continuous value) that a given observation falls into one category or the other. We can then categorize observations based on predicted probabilities with some user-specified threshold (which we’ll talk more about in a future exercise). There’s one more trick we need to make this scheme work: Probabilities lie between \\(p \\in [0, 1]\\), but the response of linear regression can be any real value between \\(x \\in (-\\infty, +\\infty)\\). To deal with this, we use the logit function to “warp space” and transform between the interval \\(p \\in [0, 1]\\) and the whole real line \\(x \\in (-\\infty, +\\infty)\\). ## We&#39;ll need the logit and inverse-logit functions to &quot;warp space&quot; logit &lt;- function(p) { odds_ratio &lt;- p / (1 - p) log(odds_ratio) } inv.logit &lt;- function(x) { exp(x) / (1 + exp(x)) } The result of the logit function is a log-odds ratio, which is just a different way of expressing a probability. This is what it looks like to map from probabilities p to log-odds ratios: tibble(p = seq(0, 1, length.out = 100)) %&gt;% mutate(x = logit(p)) %&gt;% ggplot(aes(p, x)) + geom_vline(xintercept = 0, linetype = 2) + geom_vline(xintercept = 1, linetype = 2) + geom_line() + labs(x = &quot;Probability&quot;, y = &quot;Logit Value (log-odds ratio)&quot;) And this is what it looks like to carry out the inverse mapping from log-odds ratios to probabilities: tibble(p = seq(0, 1, length.out = 100)) %&gt;% mutate(x = logit(p)) %&gt;% ggplot(aes(x, p)) + geom_hline(yintercept = 0, linetype = 2) + geom_hline(yintercept = 1, linetype = 2) + geom_line() + labs(y = &quot;Probability&quot;, x = &quot;Logit Value (log-odds ratio)&quot;) This curve (the inverse-logit) is the one we’ll stretch and shift in order to fit a logistic regression. 60.3 A Worked Example The following code chunk fits a logistic regression model to your training data, predicts classification probabilities on the validation data, and visualizes the results so we can assess the model. You’ll practice carrying out these steps soon: First let’s practice interpreting a logistic regression model’s outputs. 60.3.1 q2 Run the following code and study the results. Answer the questions under observations below. ## NOTE: No need to edit; just run and answer the questions below ## Fit a basic logistic regression model: biological-sex only fit_basic &lt;- glm( formula = heart_disease ~ sex, data = df_train, family = &quot;binomial&quot; ) ## Predict the heart disease probabilities on the validation data df_basic &lt;- df_validate %&gt;% add_predictions(fit_basic, var = &quot;log_odds_ratio&quot;) %&gt;% arrange(log_odds_ratio) %&gt;% rowid_to_column(var = &quot;order&quot;) %&gt;% ## Remember that logistic regression fits the log_odds_ratio; ## convert this to a probability with inv.logit() mutate(pr_heart_disease = inv.logit(log_odds_ratio)) ## Plot the predicted probabilities and actual classes df_basic %&gt;% ggplot(aes(order, pr_heart_disease, color = heart_disease)) + geom_hline(yintercept = 0.5, linetype = 2) + geom_point() + facet_grid(~ sex, scales = &quot;free_x&quot;) + coord_cartesian(ylim = c(0, 1)) + theme(legend.position = &quot;bottom&quot;) + labs( x = &quot;Rank-ordering of Predicted Probability&quot;, y = &quot;Predicted Probability of Heart Disease&quot; ) Observations: With a threshold at 0.5 the model would perform poorly: it appears we would miss miss a large number of people with heart disease. This model only considers the binary variable sex; thus the model only predicts two probability values, one for female and one for male. In the next modeling exercise we’ll discuss how to quantitatively assess the results of a classifier. For the moment, know that our objective is usually to maximize the rates of true positives (TP) and true negatives (TN). In our example, true positives are when we correctly identify the presence of heart disease, and true negatives are when we correctly flag the absence of heart disease. Note that we can make errors in “either direction”: a false positive (FP) or a false negative (FN), depending on the underlying true class. ## NOTE: No need to edit; run and inspect pr_threshold &lt;- 0.5 df_basic %&gt;% mutate( true_positive = (pr_heart_disease &gt; pr_threshold) &amp; heart_disease, false_positive = (pr_heart_disease &gt; pr_threshold) &amp; !heart_disease, true_negative = (pr_heart_disease &lt;= pr_threshold) &amp; !heart_disease, false_negative = (pr_heart_disease &lt;= pr_threshold) &amp; heart_disease ) %&gt;% summarize( TP = sum(true_positive), FP = sum(false_positive), TN = sum(true_negative), FN = sum(false_negative) ) ## # A tibble: 1 × 4 ## TP FP TN FN ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 36 28 23 10 These numbers don’t mean a whole lot on their own; we’ll use them to compare performance across models. Next you’ll practice using R functions to carry out logistic regression for classification, and build a model to compare against this basic one. 60.4 Doing Logistic Regression 60.4.1 q3 Using the code from q2 as a pattern, fit a logistic regression model to df_train. fit_q3 &lt;- glm( formula = heart_disease ~ . - num, data = df_train, family = &quot;binomial&quot; ) Use the following to check your work. ## NOTE: No need to change this # Correct size assertthat::assert_that(dim( df_validate %&gt;% add_predictions(fit_q3) )[1] &gt; 0) ## [1] TRUE print(&quot;Nice!&quot;) ## [1] &quot;Nice!&quot; 60.4.2 q4 Recall that logistic regression predicts log-odds-ratio values; add these predictions to df_validate and convert them to probabilities pr_heart_disease. df_q4 &lt;- df_validate %&gt;% add_predictions(fit_q3, var = &quot;log_odds_ratio&quot;) %&gt;% mutate(pr_heart_disease = inv.logit(log_odds_ratio)) ## Plot the predicted probabilities and actual classes df_q4 %&gt;% arrange(pr_heart_disease) %&gt;% rowid_to_column(var = &quot;order&quot;) %&gt;% ggplot(aes(order, pr_heart_disease, color = heart_disease)) + geom_point() + coord_cartesian(ylim = c(0, 1)) + theme(legend.position = &quot;bottom&quot;) + labs( x = &quot;Rank-ordering of Predicted Probability&quot;, y = &quot;Predicted Probability of Heart Disease&quot; ) Use the following to check your code. ## NOTE: No need to change this # Correct size assertthat::assert_that(all( df_q4 %&gt;% mutate(check = (0 &lt;= pr_heart_disease) &amp; (pr_heart_disease &lt;= 1)) %&gt;% pull(check) )) ## [1] TRUE print(&quot;Excellent!&quot;) ## [1] &quot;Excellent!&quot; 60.4.3 q5 Inspect your graph from q4 and choose a threshold for classification. Compare your count of true positives (TP) and true negatives (TN) to the model above. ## NOTE: This is a somewhat subjective choice; we&#39;ll learn some principles ## in the next modeling exercise. pr_threshold &lt;- 0.75 ## NOTE: No need to edit this; just inspect the results df_q4 %&gt;% mutate( true_positive = (pr_heart_disease &gt; pr_threshold) &amp; heart_disease, false_positive = (pr_heart_disease &gt; pr_threshold) &amp; !heart_disease, true_negative = (pr_heart_disease &lt;= pr_threshold) &amp; !heart_disease, false_negative = (pr_heart_disease &lt;= pr_threshold) &amp; heart_disease ) %&gt;% summarize( TP = sum(true_positive), FP = sum(false_positive), TN = sum(true_negative), FN = sum(false_negative) ) ## # A tibble: 1 × 4 ## TP FP TN FN ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 29 8 43 17 Observations: My model ended up having fewer true positives. My model ended up with many more true negatives. "],["stats-randomization.html", "61 Stats: Randomization 61.1 An Example: Fertilizer and Crop Yield 61.2 Visualize the Scenario 61.3 Confound - Increased yield due to proximity to the river! 61.4 Randomization to the rescue! 61.5 Why does randomization work? 61.6 Conclusion 61.7 References", " 61 Stats: Randomization Purpose: You’ve probably heard that a “control” is important for doing science. If you’re lucky, you may have also heard about randomization. These two ideas are the backbone of sound data collection. In this exercise, you’ll learn the basics about how to plan data collection. This is probably the most important lesson in this entire class, so I hope you do this exercise very carefully! Reading: (None, this is the reading) library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✔ ggplot2 3.4.0 ✔ purrr 1.0.1 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.5.0 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## NOTE: Don&#39;t edit this; this sets up the example simulate_yield &lt;- function(v) { ## Check assertions if (length(v) != 6) { stop(&quot;Design must be a vector of length 6&quot;) } if (length(setdiff(v, c(&quot;T&quot;, &quot;C&quot;)) != 0)) { stop(&quot;Design must be a vector with &#39;T&#39; and &#39;C&#39; characters only&quot;) } if (length(setdiff(c(&quot;T&quot;, &quot;C&quot;), v)) &gt; 0) { stop(&quot;Design must contain at least one &#39;T&#39; and at least one &#39;C&#39;&quot;) } ## Simulate data tibble(condition = v) %&gt;% mutate( condition = fct_relevel(condition, &quot;T&quot;, &quot;C&quot;), plot = row_number(), yield = if_else(condition == &quot;T&quot;, 1, 0) + plot / 3 + rnorm(n(), mean = 1, sd = 0.5) ) } 61.1 An Example: Fertilizer and Crop Yield It’s difficult to explain the ideas behind data collection without talking about data to collect, so let’s consider a specific example: Imagine we’re testing a fertilizer, and we want to know how much it affects the yield of a specific crop. We have access to a farm, which we section off into six plots. In order to determine the effect the fertilizer has, we need to add fertilizer to some plots, and leave other plots without fertilizer (to serve as a comparison). In scientific jargon, these choices are referred to as the “treatment” and “control”. The treatment will have the effect we wish to study, while the control serves as a baseline for a meaningful (quantitative) comparison. The code below selects a simple arrangement of treatment and control plots. ## Define the sequence of treatment (T) and control (C) plots experimental_design &lt;- c(&quot;T&quot;, &quot;T&quot;, &quot;T&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;) In statistics, the word “design” refers to the “design of data collection”. The purposeful planning of data collection is called statistical design of experiments. 61.2 Visualize the Scenario The following code visualizes the scenario: how experimental conditions are arranged spatially on our test farm. tibble( condition = experimental_design, plot = 1:length(experimental_design) ) %&gt;% ggplot(aes(plot, 1)) + geom_label( aes(label = condition, fill = condition), size = 10 ) + scale_x_continuous(breaks = 1:6) + scale_y_continuous(breaks = NULL) + theme_minimal() + theme(legend.position = &quot;none&quot;) + labs( x = &quot;Plot of Land&quot;, y = NULL ) Now let’s simulate the results of the experiment! 61.2.1 q1 Simulate the results of this experimental design, and answer the questions under Observations below. ## TODO: Do not edit; run the following code, answer the questions below ## For reproducibility, set the seed set.seed(101) ## Simulate the experimental yields experimental_design %&gt;% simulate_yield() %&gt;% ## Analyze the data group_by(condition) %&gt;% summarize( yield_mean = mean(yield), yield_sd = sd(yield) ) ## # A tibble: 2 × 3 ## condition yield_mean yield_sd ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 T 2.59 0.391 ## 2 C 2.95 0.584 Observations The treatment is to add fertilizer. Based on the description above, I would expect the treatment to have greater yield. However, in this case, the control has greater yield by ~0.3. This is the reverse of what we would expect! The mean difference has the wrong sign! 61.3 Confound - Increased yield due to proximity to the river! What I didn’t tell you about the experimental setup is that there’s a river on the right-hand-side of the plots: tibble( condition = experimental_design, plot = 1:length(experimental_design) ) %&gt;% ggplot(aes(plot, 1)) + geom_label( aes(label = condition, fill = condition), size = 10 ) + geom_vline( xintercept = 7, color = &quot;cornflowerblue&quot;, size = 8 ) + annotate( &quot;text&quot;, x = 6.7, y = 1.25, label = &quot;River&quot;, hjust = 1 ) + scale_x_continuous(breaks = 1:6) + scale_y_continuous(breaks = NULL, limits = c(0.5, 1.5)) + theme_minimal() + theme(legend.position = &quot;none&quot;) + labs( x = &quot;Plot of Land&quot;, y = NULL ) ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. While fertilizer leads to an increase in crop yield, additional water also leads to a higher crop yield. These are the only plots we have available for planting, and it’s too expensive to move the river, so we’ll have to figure out how to place the plots to deal with this experimental reality. Terminology: When there are other factors present in an experiment affecting our outcome of interest, we call those factors confounds. When we don’t know that a confound exists, it is sometimes called a lurking variable (Joiner 1979). 61.3.1 q2 Try defining a different order of the plots to overcome the confound (river). ## TODO: Define your own experimental design ## An &quot;obvious&quot; first attempt would be to simply switch the order, but this ## will tend to over-estimate the effect of the treatment your_design &lt;- c(&quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;T&quot;, &quot;T&quot;, &quot;T&quot;) ## NOTE: No need to edit; use this to simulate your design your_design %&gt;% simulate_yield() %&gt;% group_by(condition) %&gt;% summarize( yield_mean = mean(yield), yield_sd = sd(yield) ) ## # A tibble: 2 × 3 ## condition yield_mean yield_sd ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 T 3.58 0.354 ## 2 C 1.90 0.481 Observations In my case, I found the treatment to have a higher yield than the control. In my case, I found the treatment to have a higher yield by about two units; this is a drastic overestimate of the effect. In my case, the effects I see are due to both the treatment and the river; this leads to a strong overestimate of the effect of the treatment on the yield. 61.4 Randomization to the rescue! To recap: We’re trying to accurately estimate the effect of the treatment over the control. However, there is a river that tends to increase the yield of plots nearby. The only thing we can affect in our data collection is where to place the treatment and control plots. You might be tempted to try to do something “smart” to cancel out the effects of the river (such as alternate the order of T and C). While that might work for this specific example, in real experiments there are often many different confounds with all sorts of complicated effects on the quantity we seek to study. What we need is a general-purpose way to do statistical design of experiments that can deal with any kind of confound. To that end, the gold-standard for dealing with confounds is to randomize our data collection. The sample() function will randomize the order of a vector, as the following code shows. ## NOTE: No need to edit; run and inspect ## Start with a simple arrangement experimental_design %&gt;% ## Randomize the order sample() ## [1] &quot;C&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; &quot;C&quot; &quot;C&quot; If we randomize the order of treatment and control plots, then we transform the effects of the river (and any other confounds) into a random effect. 61.4.1 q3 Randomize the run order to produce your design. Answer the questions under Observations below. ## TODO: Complete the code below ## Set the seed for reproducibility set.seed(101) ## Simulate the experimental results experimental_design %&gt;% ## Randomize the plot order sample() %&gt;% simulate_yield() %&gt;% group_by(condition) %&gt;% summarize( yield_mean = mean(yield), yield_sd = sd(yield) ) ## # A tibble: 2 × 3 ## condition yield_mean yield_sd ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 T 3.22 0.679 ## 2 C 2.63 0.818 Observations In this case, we find the treatment has a higher yield than the control. Since we randomized the run order, the confound cannot have a consistent effect on the outcome (on average). In my case, I found the difference to be about 0.6 units; this is smaller than the true treatment effect, but less optimistic than before. 61.5 Why does randomization work? Let’s visually compare a naive (sequential) design with a randomized design, and draw a line to represent the effects of the river on crop yield. set.seed(101) bind_rows( experimental_design %&gt;% simulate_yield() %&gt;% mutate(design = &quot;Naive&quot;), experimental_design %&gt;% sample() %&gt;% simulate_yield() %&gt;% mutate(design = &quot;Randomized&quot;), ) %&gt;% ggplot(aes(plot, yield)) + geom_line(aes(y = plot / 3), color = &quot;cornflowerblue&quot;) + geom_segment( aes(y = plot / 3, yend = yield, xend = plot), arrow = arrow(length = unit(0.05, &quot;npc&quot;)), color = &quot;grey70&quot; ) + geom_point(aes(color = condition)) + facet_grid(design~.) + theme_minimal() In the naive case, we’ve placed all our treatments at locations where the river has a low effect, and our controls at locations where the river has a high effect. This results in a consistent effect that reverses the perceived difference between treatment and control. However, when we randomize, a mix of high and low river effects enter into both the treatment and control conditions. So long as there is an average difference between the treatment and control, we can detect it with sufficiently many samples from a well-designed study. It’s not randomization alone that’s saving us from confounds: The river will boost production for all plots, so we’ll always see a yield higher than what we’d get with the treatment or control alone. Studying the difference between treatment and control cancels out any constant difference, while randomization scrambles the effect of the river. This is why we combine randomization with a treatment-to-control comparison: Randomization allows us to transform confounds into a random effect Comparing a treatment and control allows us to isolate the effect of the treatment Using both ideas together—randomization with a control—is the foundation of sound experimental design. A similar idea—random assignment—is used in medical science to determine the effects of new drugs and other medical interventions. 61.6 Conclusion Here are the key takeaways from this lesson: “More data” is not enough for sound science; the right data is what you need to understand an effect. Getting the right data is a matter of carefully planning your data collection; designing an experiment. Confounds can confuse our analysis of data, and lead us to make incorrect conclusions. No amount of fancy math can overcome poorly-collected data. Randomization, paired with a treatment-to-control comparison, is our best tool to deal with confounds. 61.7 References Joiner, B., “Lurking Variables: Some Examples” (1979) link "],["model-assessing-classification-with-roc.html", "62 Model: Assessing Classification with ROC 62.1 Setup 62.2 Assessing a Classifier 62.3 Positives and negatives 62.4 Classification Rates and Decision Thresholds 62.5 The Receiver Operating Characteristic (ROC) Curve 62.6 Practice Assessing Classifiers 62.7 Selecting a Threshold", " 62 Model: Assessing Classification with ROC Purpose: With regression models, we used model metrics in order to assess and select a model (e.g. choose which features we should use). In order to do the same with classification models, we need some quantitative measure of accuracy. However, assessing the “accuracy” of a classifier is far more complicated. To do this, we’ll need to understand the receiver operating characteristic. Reading: StatQuest: ROC and AUC… clearly explained! (Required, ~17 minutes) library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✔ ggplot2 3.4.0 ✔ purrr 1.0.1 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.5.0 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() library(modelr) library(broom) ## ## Attaching package: &#39;broom&#39; ## The following object is masked from &#39;package:modelr&#39;: ## ## bootstrap ## We&#39;ll need the logit and inverse-logit functions to &quot;warp space&quot; logit &lt;- function(p) { odds_ratio &lt;- p / (1 - p) log(odds_ratio) } inv.logit &lt;- function(x) { exp(x) / (1 + exp(x)) } 62.1 Setup Note: The following chunk contains a lot of stuff, but you already did this in e-model04-logistic! ## NOTE: No need to edit; you did all this in a previous exercise! url_disease &lt;- &quot;http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data&quot; filename_disease &lt;- &quot;./data/uci_heart_disease.csv&quot; ## Download the data locally curl::curl_download( url_disease, destfile = filename_disease ) ## Wrangle the data col_names &lt;- c( &quot;age&quot;, &quot;sex&quot;, &quot;cp&quot;, &quot;trestbps&quot;, &quot;chol&quot;, &quot;fbs&quot;, &quot;restecg&quot;, &quot;thalach&quot;, &quot;exang&quot;, &quot;oldpeak&quot;, &quot;slope&quot;, &quot;ca&quot;, &quot;thal&quot;, &quot;num&quot; ) ## Recoding functions convert_sex &lt;- function(x) { case_when( x == 1 ~ &quot;male&quot;, x == 0 ~ &quot;female&quot;, TRUE ~ NA_character_ ) } convert_cp &lt;- function(x) { case_when( x == 1 ~ &quot;typical angina&quot;, x == 2 ~ &quot;atypical angina&quot;, x == 3 ~ &quot;non-anginal pain&quot;, x == 4 ~ &quot;asymptomatic&quot;, TRUE ~ NA_character_ ) } convert_fbs &lt;- function(x) { if_else(x == 1, TRUE, FALSE) } convert_restecv &lt;- function(x) { case_when( x == 0 ~ &quot;normal&quot;, x == 1 ~ &quot;ST-T wave abnormality&quot;, x == 2 ~ &quot;Estes&#39; criteria&quot;, TRUE ~ NA_character_ ) } convert_exang &lt;- function(x) { if_else(x == 1, TRUE, FALSE) } convert_slope &lt;- function(x) { case_when( x == 1 ~ &quot;upsloping&quot;, x == 2 ~ &quot;flat&quot;, x == 3 ~ &quot;downsloping&quot;, TRUE ~ NA_character_ ) } convert_thal &lt;- function(x) { case_when( x == 3 ~ &quot;normal&quot;, x == 6 ~ &quot;fixed defect&quot;, x == 7 ~ &quot;reversible defect&quot;, TRUE ~ NA_character_ ) } ## Load and wrangle df_data &lt;- read_csv( filename_disease, col_names = col_names, col_types = cols( &quot;age&quot; = col_number(), &quot;sex&quot; = col_number(), &quot;cp&quot; = col_number(), &quot;trestbps&quot; = col_number(), &quot;chol&quot; = col_number(), &quot;fbs&quot; = col_number(), &quot;restecg&quot; = col_number(), &quot;thalach&quot; = col_number(), &quot;exang&quot; = col_number(), &quot;oldpeak&quot; = col_number(), &quot;slope&quot; = col_number(), &quot;ca&quot; = col_number(), &quot;thal&quot; = col_number(), &quot;num&quot; = col_number() ) ) %&gt;% mutate( sex = convert_sex(sex), cp = convert_cp(cp), fbs = convert_fbs(fbs), restecg = convert_restecv(restecg), exang = convert_exang(exang), slope = convert_slope(slope), thal = convert_thal(thal) ) %&gt;% rowid_to_column() %&gt;% ## Filter rows with NA&#39;s (you did this in e-data13-cleaning) filter(!is.na(ca), !is.na(thal)) %&gt;% ## Create binary outcome for heart disease mutate(heart_disease = num &gt; 0) ## Warning: One or more parsing issues, call `problems()` on your data frame for details, ## e.g.: ## dat &lt;- vroom(...) ## problems(dat) set.seed(101) df_train &lt;- df_data %&gt;% slice_sample(n = 200) df_validate &lt;- anti_join( df_data, df_train, by = &quot;rowid&quot; ) 62.2 Assessing a Classifier What makes for a “good” or a “bad” classifier? When studying continuous models, we studied a variety of diagnostic plots and error metrics to assess model accuracy. However, since we’re now dealing with a discrete response, our metrics are going to look very different. In order to assess a classifier, we’re going to need to build up some concepts. To learn these concepts, let’s return to the basic model from the previous modeling exercise: ## NOTE: No need to edit ## Fit a basic logistic regression model: biological-sex only fit_basic &lt;- glm( formula = heart_disease ~ sex, data = df_train, family = &quot;binomial&quot; ) ## Predict the heart disease probabilities on the validation data df_basic &lt;- df_validate %&gt;% add_predictions(fit_basic, var = &quot;log_odds_ratio&quot;) %&gt;% arrange(log_odds_ratio) %&gt;% rowid_to_column(var = &quot;order&quot;) %&gt;% mutate(pr_heart_disease = inv.logit(log_odds_ratio)) 62.3 Positives and negatives With a binary (two-class) classifier, there are only 4 possible outcomes of a single prediction. We can summarize all four in a table:                | Predicted True | Predicted False | Actually True | True Positive | False Negative | Actually False | False Positive | True Negative | Note: A table with the total counts of [TP, FP, FN, TN] is called a confusion matrix. There are two ways in which we can be correct: True Positive: We correctly identified a positive case; e.g. we correctly identified that a given patient has heart disease. True Negative: We correctly identified a negative case; e.g. we correctly identified that a given patient does not have heart disease. And there are two ways in which we can be incorrect: False Positive: We predicted a case to be positive, but in reality it was negative; e.g. we predicted that a given patient has heart disease, but in reality they do not have the disease. False Negative: We predicted a case to be negative, but in reality it was positive; e.g. we predicted that a given patient does not have heard disease, but in reality they do have the disease. Note that we might have different concerns about false positives and negatives. For instance in the heart disease case, we might be more concerned with flagging all possible cases of heart disease, particularly if follow-up examination can diagnose heart disease with greater precision. In that case, we might want to avoid false negatives but accept more false positives. We can make quantitative judgments about these classification tradeoffs by controlling classification rates with a decision threshold. 62.4 Classification Rates and Decision Thresholds We can summarize the tradeoffs a classifier makes in terms of classification rates. First, let’s introduce some shorthand: TP | Total count of True Positives | FP | Total count of False Positives | TN | Total count of True Negatives | FN | Total count of False Negatives | Two important rates are the true positive rate and false positive rate, defined as: True Positive Rate (TPR): The ratio of true positives to all positives, that is: TPR = TP / P = TP / (TP + FN) We generally want to maximize the TPR. In the heart disease example, this is the number of patients with heart disease that we correctly diagnose; a higher TPR in this setting means we can follow-up with and treat more individuals. False Positive Rate (FPR): The ratio of false positives to all negatives, that is: FPR = FP / N = FP / (FP + TN) We generally want to minimize the FPR. In the heart disease example, this is the number of patients without heart disease that we falsely diagnose with the disease; a higher FPR in this setting means we will waste valuable time and resources following up with healthy individuals. We can control the TPR and FPR by choosing our decision threshold for our classifier. Remember that in the previous exercise e-model04-logistic we set an arbitrary threshold of pr_heart_disease &gt; 0.5 for detection: We can instead pick a pr_threshold to make our classifier more or less sensitive, which will adjust the TPR and FPR. The next task will illustrate this idea. 62.4.1 q1 Compute the true positive rate (TPR) and false positive rate (FPR) using the model fitted above, calculating on the validation data. Hint 1: Remember that you can use summarize(n = sum(boolean)) to count the number of TRUE values in a variable boolean. Feel free to compute intermediate boolean values with things like mutate(boolean = (x &lt; 0) &amp; flag) before your summarize. Hint 2: We did part of this in the previous modeling exercise! pr_threshold &lt;- 0.5 df_q1 &lt;- df_basic %&gt;% mutate( true_positive = (pr_heart_disease &gt; pr_threshold) &amp; heart_disease, false_positive = (pr_heart_disease &gt; pr_threshold) &amp; !heart_disease, true_negative = (pr_heart_disease &lt;= pr_threshold) &amp; !heart_disease, false_negative = (pr_heart_disease &lt;= pr_threshold) &amp; heart_disease ) %&gt;% summarize( TP = sum(true_positive), FP = sum(false_positive), TN = sum(true_negative), FN = sum(false_negative) ) %&gt;% mutate( TPR = TP / (TP + FN), FPR = FP / (FP + TN) ) df_q1 ## # A tibble: 1 × 6 ## TP FP TN FN TPR FPR ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 26 37 24 10 0.722 0.607 Use the following test to check your work. ## NOTE: No need to edit; use this to check your work assertthat::assert_that( all_equal( df_q1 %&gt;% select(TPR, FPR), df_validate %&gt;% add_predictions(fit_basic, var = &quot;l_heart_disease&quot;) %&gt;% mutate(pr_heart_disease = inv.logit(l_heart_disease)) %&gt;% summarize( TP = sum((pr_heart_disease &gt; pr_threshold) &amp; heart_disease), FP = sum((pr_heart_disease &gt; pr_threshold) &amp; !heart_disease), TN = sum((pr_heart_disease &lt;= pr_threshold) &amp; !heart_disease), FN = sum((pr_heart_disease &lt;= pr_threshold) &amp; heart_disease) ) %&gt;% mutate( TPR = TP / (TP + FN), FPR = FP / (FP + TN) ) %&gt;% select(TPR, FPR) ) ) ## [1] TRUE print(&quot;Excellent!&quot;) ## [1] &quot;Excellent!&quot; 62.5 The Receiver Operating Characteristic (ROC) Curve As the required video mentioned, we can summarize TPR and FPR at different threshold values pr_threshold with the receiver operating characteristic curve (ROC curve). This plot gives us an overview of the tradeoffs we can achieve with our classification model. The ROC curve shows TPR against FPR. Remember that we want to maximize TPR and minimize FPR*; therefore, the ideal point for the curve to reach is the top-left point in the graph. A very poor classifier would run along the diagonal—this would be equivalent to randomly guessing the class of each observation. An ROC curve below the diagonal is worse than random guessing! To compute an ROC curve, we could construct a confusion matrix at a variety of thresholds, compute the TPR and FPR for each, and repeat. However, there’s a small bit of “shortcut code” we could use to do the same thing. The following chunk illustrates how to compute an ROC curve. 62.5.1 q2 Inspect the following ROC curve for the basic classifier and assess its performance. Is this an effective classifier? How do you know? ## NOTE: No need to edit; run and inspect df_basic %&gt;% ## Begin: Shortcut code for computing an ROC arrange(desc(pr_heart_disease)) %&gt;% summarize( true_positive_rate = cumsum(heart_disease) / sum(heart_disease), false_positive_rate = cumsum(!heart_disease) / sum(!heart_disease) ) %&gt;% ## End: Shortcut code for computing an ROC ggplot(aes(false_positive_rate, true_positive_rate)) + geom_abline(intercept = 0, slope = 1, linetype = 2) + geom_step() + coord_cartesian(xlim = c(0, 1), ylim = c(0, 1)) + theme_minimal() Observations: This is a highly ineffective classifier; the ROC curve is very near the diagonal, indicating the classifier is not much better (for some thresholds—worse than) random guessing. 62.6 Practice Assessing Classifiers Let’s get some practice reading ROC curves. 62.6.1 q3 Inspect the following ROC curve. Is this an effective classifier? What explains this model’s performance? Is this model valid for prediction? ## NOTE: No need to edit fit_cheating &lt;- glm( formula = heart_disease ~ num, data = df_train, family = &quot;binomial&quot; ) ## Warning: glm.fit: algorithm did not converge ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred df_cheating &lt;- df_validate %&gt;% add_predictions(fit_cheating, var = &quot;log_odds_ratio&quot;) %&gt;% arrange(log_odds_ratio) %&gt;% rowid_to_column(var = &quot;order&quot;) %&gt;% mutate(pr_heart_disease = inv.logit(log_odds_ratio)) df_cheating %&gt;% ## Begin: Shortcut code for computing an ROC arrange(desc(pr_heart_disease)) %&gt;% summarize( true_positive_rate = cumsum(heart_disease) / sum(heart_disease), false_positive_rate = cumsum(!heart_disease) / sum(!heart_disease) ) %&gt;% ## End: Shortcut code for computing an ROC ggplot(aes(false_positive_rate, true_positive_rate)) + geom_abline(intercept = 0, slope = 1, linetype = 2) + geom_step() + coord_cartesian(xlim = c(0, 1), ylim = c(0, 1)) + theme_minimal() Observations: This is an optimal classifier; we can achieve TPR = 1 with FPR = 0. In fact, it’s suspiciously good…. This model is using the outcome to predict the outcome! Remember that heart_disease = num &gt; 0; this is not a valid way to predict the presence of heart disease. Next you’ll fit your own model and assess its performance. 62.6.2 q4 Fit a model to the training data, and predict class probabilities on the validation data. Compare your model’s performance to that of fit_baseline (fitted below). fit_q4 &lt;- glm( formula = heart_disease ~ age + cp + trestbps, data = df_train, family = &quot;binomial&quot; ) df_q4 &lt;- df_validate %&gt;% add_predictions(fit_q4, var = &quot;log_odds_ratio&quot;) %&gt;% arrange(log_odds_ratio) %&gt;% rowid_to_column(var = &quot;order&quot;) %&gt;% mutate(pr_heart_disease = inv.logit(log_odds_ratio)) ## Here&#39;s another model for comparison fit_baseline &lt;- glm( formula = heart_disease ~ sex + cp + trestbps, data = df_train, family = &quot;binomial&quot; ) df_baseline &lt;- df_validate %&gt;% add_predictions(fit_baseline, var = &quot;log_odds_ratio&quot;) %&gt;% arrange(log_odds_ratio) %&gt;% rowid_to_column(var = &quot;order&quot;) %&gt;% mutate(pr_heart_disease = inv.logit(log_odds_ratio)) ## NOTE: No need to edit bind_rows( df_q4 %&gt;% arrange(desc(pr_heart_disease)) %&gt;% summarize( true_positive_rate = cumsum(heart_disease) / sum(heart_disease), false_positive_rate = cumsum(!heart_disease) / sum(!heart_disease) ) %&gt;% mutate(model = &quot;Personal&quot;), df_baseline %&gt;% arrange(desc(pr_heart_disease)) %&gt;% summarize( true_positive_rate = cumsum(heart_disease) / sum(heart_disease), false_positive_rate = cumsum(!heart_disease) / sum(!heart_disease) ) %&gt;% mutate(model = &quot;Baseline&quot;) ) %&gt;% ggplot(aes(false_positive_rate, true_positive_rate, color = model)) + geom_abline(intercept = 0, slope = 1, linetype = 2) + geom_step() + coord_cartesian(xlim = c(0, 1), ylim = c(0, 1)) + theme_minimal() + theme(legend.position = &quot;bottom&quot;) Observations: My model fit_q4 is comparable in performance to fit_baseline; it outperforms (in TPR for fixed FPR) in some places, and underperforms in others. As one sweeps from low to high FPR, the TPR increases—at first quickly, then it tapers off to increase slowly. Both FPR and TPR equal zero at the beginning, and both limit to one. The “tradeoff” is that we can have an arbitrarily high TPR, but we “pay” for this through an increase in the FPR. 62.7 Selecting a Threshold The ROC summarizes performance characteristics for a variety of thresholds pr_threshold, but to actually deploy a classifier and make decisions, we have to pick a specific threshold value. Picking a threshold is not just an exercise in mathematics; we need to inform this decision with our intended use-case. The following chunk plots potential pr_threshold against achieved TPR values for your model. Use this image to pick a classifier threshold. 62.7.1 q5 Pick a target TPR value for your heart disease predictor; what is a reasonable value for TPR, and why did yo pick that value? What values of pr_threshold meet or exceed that target TPR? What specific value for pr_threshold do you choose, and why? ## NOTE: No need to edit this; use these data to pick a threshold df_thresholds &lt;- df_q4 %&gt;% ## Begin: Shortcut code for computing an ROC arrange(desc(pr_heart_disease)) %&gt;% summarize( pr_heart_disease = pr_heart_disease, true_positive_rate = cumsum(heart_disease) / sum(heart_disease), false_positive_rate = cumsum(!heart_disease) / sum(!heart_disease) ) ## End: Shortcut code for computing an ROC ## TODO: Pick a threshold using df_thresholds above df_thresholds %&gt;% filter(true_positive_rate &gt;= 0.9) %&gt;% filter(false_positive_rate == min(false_positive_rate)) ## # A tibble: 2 × 3 ## pr_heart_disease true_positive_rate false_positive_rate ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.252 0.917 0.492 ## 2 0.249 0.944 0.492 pr_threshold &lt;- 0.249 tpr_achieved &lt;- 0.944 fpr_achieved &lt;- 0.492 ## NOTE: No need to edit; use this visual to help your decision df_thresholds %&gt;% ggplot(aes(true_positive_rate, pr_heart_disease)) + geom_vline(xintercept = tpr_achieved, linetype = 2) + geom_hline(yintercept = pr_threshold, linetype = 2) + geom_step() + labs( x = &quot;True Positive Rate&quot;, y = &quot;Pr Threshold&quot; ) Observations: I pick TPR &gt; 0.9, as I want to catch the vast majority of patients with the disease. Filtering df_thresholds shows that pr_threshold &gt;= 0.252 achieves my desired TPR. To pick a specific value for pr_threshold, I also try to minimize the FPR. For my case, there is a range of values of pr_threshold that can minimize the FPR; therefore I take the more permissive end of the interval. Ultimately I picked pr_threshold = 0.249, which gave TPR = 0.944, FPR = 0.492. This will lead to a lot of false positives, but we will have a very sensitive detector. "],["vis-control-charts.html", "63 Vis: Control Charts 63.1 Control Chart: Example 63.2 Control chart steps 63.3 Data Preparation 63.4 Generate Control Limits 63.5 Visualize and Interpret 63.6 Control Chart: Application", " 63 Vis: Control Charts Purpose: Remember in c02-michelson (q4) when you studied a control chart? Now that we’ve learned about confidence intervals, we can more formally study control charts. These are a key tool in statistical process control, which is how manufacturers rigorously track and control the quality of manufactured goods. Control charts help a process manager track when something has gone wrong in a manufacturing line, and are used to determine when a process is running smoothly. Reading: Example use of a control chart, based on NIST mass calibration data. (Optional) Prerequisites: c02-michelson, e-stat06-clt library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✔ ggplot2 3.4.0 ✔ purrr 1.0.1 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.5.0 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() 63.1 Control Chart: Example Below is an example of a control chart: A control chart is used to help detect when something out-of-the-ordinary occurred. Essentially, it is a tool to help us determine when something non-random happened so we can plan a follow-up study and prevent that non-random event from happening in the future. To do that detection work, we look for patterns. Note that two kinds of patterns have been highlighted below: an outlier that lies outside the control limits, and a “run” of batch means that all lie off-center (to one side of the “grand mean,” the solid line). control chart 63.2 Control chart steps To construct and use a control chart, follow these steps: Group individual observations into consecutive batches, say 4 to 10 observations. If parts are manufactured in batches, then use those groups. Take the mean of each batch, compute the “grand mean” based on all of the data, and establish “control limits” based on a confidence interval for the batch means (where \\(n\\) is your batch size). Plot each batch mean sequentially, and visually indicate the control limits and grand mean on your plot. Compare each batch mean against the control limits and grand mean. Look for patterns to suggest batches where something out-of-the-ordinary happened. Some examples include: Batches that fall outside the control limits Consecutive batches that lie above or below the mean Persistent up-and-down patterns If there are no coherent patterns and if only an expected number of batch means fall outside the control limits, then there is no evidence for non-random behavior. A process free of any obvious non-random behavior is said to be under statistical control, or to be a stable process. If you do detect something out-of-the-ordinary, use the batch index to go investigate those cases in greater detail. A control chart helps you detect when something went wrong—it does not tell you what went wrong. Like any form of EDA, it is also a good idea to experiment with different batch sizes. 63.3 Data Preparation To illustrate the control chart concept, let’s first generate some data that is completely random. set.seed(101) df_data &lt;- tibble(X = rnorm(n = 1000)) Following Step 1, we need to assign batch identifiers to group the data. 63.3.1 q1 Use integer division %/% and the row_number() helper to group consecutive observations into groups of 4 with a common identifier id. Hint: Since R is a one-based index language, you will need to adjust the output of row_number() before performing the integer division %/%. df_q1 &lt;- df_data %&gt;% mutate(id = (row_number() - 1) %/% 4) df_q1 ## # A tibble: 1,000 × 2 ## X id ## &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.326 0 ## 2 0.552 0 ## 3 -0.675 0 ## 4 0.214 0 ## 5 0.311 1 ## 6 1.17 1 ## 7 0.619 1 ## 8 -0.113 1 ## 9 0.917 2 ## 10 -0.223 2 ## # … with 990 more rows Use the following to check your work. ## NOTE: No need to change this assertthat::assert_that( df_q1 %&gt;% filter(row_number() &lt;= 4) %&gt;% summarize(sd = sd(id)) %&gt;% pull(sd) == 0 ) ## [1] TRUE assertthat::assert_that( df_q1 %&gt;% filter(row_number() &gt;= 997) %&gt;% summarize(sd = sd(id)) %&gt;% pull(sd) == 0 ) ## [1] TRUE print(&quot;Nice!&quot;) ## [1] &quot;Nice!&quot; 63.4 Generate Control Limits Next, we’ll use our knowledge about confidence intervals and the CLT to set the control limits, based on our batch size. 63.4.1 q2 Use a central limit theorem (CLT) approximation to set 3 sigma confidence interval limits on the group mean you computed above. Note: A 3 sigma bound corresponds to a coverage probability of 1 - pnorm(-3) * 2; approximately \\(99.7%\\). Hint: Think carefully about how many samples will be in each group, not in your dataset in total. X_grand &lt;- df_data %&gt;% summarize(X_mean = mean(X)) %&gt;% pull(X_mean) X_sd &lt;- df_data %&gt;% summarize(X_sd = sd(X)) %&gt;% pull(X_sd) X_g4_lo &lt;- X_grand - 3 * X_sd / sqrt(4) X_g4_up &lt;- X_grand + 3 * X_sd / sqrt(4) X_g4_lo ## [1] -1.473529 X_grand ## [1] -0.03486206 X_g4_up ## [1] 1.403805 Use the following to check your work. ## NOTE: No need to change this assertthat::assert_that(abs(X_g4_lo + 3 / sqrt(4)) &lt; 0.1) ## [1] TRUE assertthat::assert_that(abs(X_grand) &lt; 0.05) ## [1] TRUE assertthat::assert_that(abs(X_g4_up - 3 / sqrt(4)) &lt; 0.1) ## [1] TRUE print(&quot;Excellent!&quot;) ## [1] &quot;Excellent!&quot; 63.5 Visualize and Interpret 63.5.1 q3 Inspect the following control chart, and answer the questions under observe below. ## NOTE: No need to edit; run and inspect df_q1 %&gt;% group_by(id) %&gt;% summarize(X_batch = mean(X)) %&gt;% ggplot(aes(id, X_batch)) + geom_hline(yintercept = X_g4_lo, linetype = &quot;dashed&quot;) + geom_hline(yintercept = X_grand) + geom_hline(yintercept = X_g4_up, linetype = &quot;dashed&quot;) + geom_point() + theme_minimal() + labs( x = &quot;Batch Index&quot;, y = &quot;Batch Mean&quot; ) Observations: - I would expect about 1 - qnorm(-3) * 2—\\(99.7%\\) of the points to lie inside the control bounds. Given that we have 250 points, I’d expect about one to lie outside. - One point lies outside the control limits, as we would expect if the points were completely random. - I don’t see any coherent pattern in the points. This makes sense, as these points are random (by construction). 63.6 Control Chart: Application Next you will construct a control chart for a real dataset. The following code downloads and parses a dataset from the NIST website studying proof masses. These are comparative measurements between “exact” 1 kilogram masses, carried out by one of the world’s most-rigorous measurement societies. ## NO NEED TO EDIT; the following will download and read the data url &lt;- &quot;https://www.itl.nist.gov/div898/handbook/datasets/MASS.DAT&quot; filename &lt;- &quot;./data/nist-mass.dat&quot; download.file(url, filename) df_mass &lt;- read_table( filename, skip = 25, col_names = c( &quot;date&quot;, &quot;standard_id&quot;, &quot;Y&quot;, &quot;balance_id&quot;, &quot;residual_sd&quot;, &quot;design_id&quot; ) ) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## date = col_double(), ## standard_id = col_double(), ## Y = col_double(), ## balance_id = col_double(), ## residual_sd = col_double(), ## design_id = col_double() ## ) df_mass ## # A tibble: 217 × 6 ## date standard_id Y balance_id residual_sd design_id ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 75.9 41 -19.5 12 0.0217 41 ## 2 75.9 41 -19.5 12 0.0118 41 ## 3 76.0 41 -19.5 12 0.0232 41 ## 4 76.1 41 -19.5 12 0.021 41 ## 5 76.6 41 -19.5 12 0.0265 41 ## 6 76.7 41 -19.5 12 0.0317 41 ## 7 77.2 41 -19.5 12 0.0194 41 ## 8 77.3 41 -19.5 12 0.0316 41 ## 9 77.6 41 -19.5 12 0.0274 41 ## 10 77.7 41 -19.5 12 0.0361 41 ## # … with 207 more rows Note that Y denotes a kind of comparison between multiple “exact” 1 kilogram masses (with Y measured in micrograms), while date denotes the (fractional) years since 1900. First, try plotting and interpreting the data without a control chart. 63.6.1 q4 Plot the measured values Y against their date of measurement date. Answer the questions under Observations below. ## TASK: Plot Y vs date Observations: - There is considerable variation! Measurements of any kind are not exactly repeatable; variability is unavoidable. - There seem to be two “upward” trends; one in the late 1970’s, and another from the mid 80’s onward. - The “density” of measurements is not consistent; it seems that many measurements are taken around the same time, with much more “sparse” measurements in between. - Something odd seems to have happened after 1985; the measurements seem to shift upward. Next, prepare the data for a control chart of the NIST data. 63.6.2 q5 Generate control chart data for a batch size of 10. n_group &lt;- 10 df_q4 &lt;- df_mass %&gt;% mutate(group_id = (row_number() - 1) %/% n_group) %&gt;% group_by(group_id) %&gt;% summarize( Y_mean = mean(Y), date = min(date) ) Y_mean &lt;- df_mass %&gt;% summarize(Y_mean = mean(Y)) %&gt;% pull(Y_mean) Y_sd &lt;- df_mass %&gt;% summarize(Y_sd = sd(Y)) %&gt;% pull(Y_sd) Y_g_lo &lt;- Y_mean - 3 * Y_sd / sqrt(n_group) Y_g_up &lt;- Y_mean + 3 * Y_sd / sqrt(n_group) Y_g_lo ## [1] -19.49853 Y_mean ## [1] -19.46452 Y_g_up ## [1] -19.43051 Use the following to check your work. ## NOTE: No need to change this assertthat::assert_that(abs(Y_g_lo + 19.49853) &lt; 1e-4) ## [1] TRUE assertthat::assert_that(abs(Y_mean + 19.46452) &lt; 1e-4) ## [1] TRUE assertthat::assert_that(abs(Y_g_up + 19.43051) &lt; 1e-4) ## [1] TRUE print(&quot;Excellent!&quot;) ## [1] &quot;Excellent!&quot; Next, plot the control chart data and inspect. 63.6.3 q6 Run the following chunk and answer the questions under Observations below. ## NOTE: No need to edit; run and inspect df_q4 %&gt;% ggplot(aes(date, Y_mean)) + geom_hline(yintercept = Y_g_lo, linetype = &quot;dashed&quot;) + geom_hline(yintercept = Y_mean) + geom_hline(yintercept = Y_g_up, linetype = &quot;dashed&quot;) + geom_point() + theme_minimal() + labs( x = &quot;Batch Year&quot;, y = &quot;Measurement (micrograms)&quot; ) Observations: - There is considerable variation! Measurements of any kind are not exactly repeatable; variability is unavoidable. - Still the case in this control chart! - There seem to be two “upward” trends; one in the late 1970’s, and another from the mid 80’s onward. - This is much more prominent in the control chart - The “density” of measurements is not consistent; it seems that many measurements are taken around the same time, with much more “sparse” measurements in between. - This is a bit harder to see in the control chart; we have to keep in mind that each batch represents multiple measurements. - Something odd seems to have happened after 1985; the measurements seem to shift upward. - More visually obvious in the control chart. We can also make quantitative statements with the control chart: There are three points violating the control limits in the late 80’s. There is also one batch violating the control limit before 1985. From 1980 to 1985, many of the batches lay below the grand mean; something seems off here. Remember that a control chart is only a detection tool; to say more, you would need to go investigate the data collection process. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
